<!DOCTYPE html>






  


<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="2aen8txJ9makMaGgtEKHpowVyKdkJYiySE3ojNCZAwU" />





  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.2.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.2.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="重点参考：  https://github.com/CheckChe0803/BigData-Interview                                                          Hadoop   Hive   Spark   Flink   HBase   Kafka   Zookeeper         文档   文档   文档   文档">
<meta name="keywords" content="BigData">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据相关">
<meta property="og:url" content="https://damaoguo.github.io/2019/11/24/大数据相关/index.html">
<meta property="og:site_name" content="MaoGuo&#39;s Blog">
<meta property="og:description" content="重点参考：  https://github.com/CheckChe0803/BigData-Interview                                                          Hadoop   Hive   Spark   Flink   HBase   Kafka   Zookeeper         文档   文档   文档   文档">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hadoop.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hive.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hbase.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/kafka.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/zookeeper.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hdfs1architecture.gif">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hdfs-ha.png">
<meta property="og:image" content="https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hdfs写流程.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hdfs读流程.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hadoop1.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hadoop2.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hive_architecture.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/how_hive_works.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark架构图.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark程序执行流程.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark-shuffle-v1.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark-shuffle-v3.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/sparkShuffleWriter.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/stageDivide.jpg">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/spark中的广播变量.png">
<meta property="og:image" content="https://github.com/JerryLead/SparkInternals/raw/master/markdown/PNGfigures/TorrentBroadcast.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink架构图.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flinkRuntime.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink故障恢复.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink中window的实现.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink中window分类.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flink基于credit的反压.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/flinkOnYarn.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hbase架构图.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hbase逻辑结构.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/kafka压缩.png">
<meta property="og:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/kafka在zk中的存储结构.png">
<meta property="og:updated_time" content="2020-01-08T02:04:42.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大数据相关">
<meta name="twitter:description" content="重点参考：  https://github.com/CheckChe0803/BigData-Interview                                                          Hadoop   Hive   Spark   Flink   HBase   Kafka   Zookeeper         文档   文档   文档   文档">
<meta name="twitter:image" content="https://damaoguo.github.io/2019/11/24/大数据相关/hadoop.jpg">






  <link rel="canonical" href="https://damaoguo.github.io/2019/11/24/大数据相关/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>大数据相关 | MaoGuo's Blog</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ea8bf9b92c205ae2c147d91b5242a6b0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/damaoguo/damaoguo.github.io" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MaoGuo's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>




<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://damaoguo.github.io/2019/11/24/大数据相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袤锅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/boy.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MaoGuo's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">大数据相关
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-11-24 16:48:37" itemprop="dateCreated datePublished" datetime="2019-11-24T16:48:37+08:00">2019-11-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-01-08 10:04:42" itemprop="dateModified" datetime="2020-01-08T10:04:42+08:00">2020-01-08</time>
              
            
          </span>

          
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

          
        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>重点参考：</p>
<ul>
<li><a href="https://github.com/CheckChe0803/BigData-Interview" target="_blank" rel="noopener">https://github.com/CheckChe0803/BigData-Interview</a></li>
</ul>
<table>
    <tr>
     <th><img width="50px" src="/2019/11/24/大数据相关/hadoop.jpg"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/hive.jpg"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/spark.jpg"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/flink.png"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/hbase.png"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/kafka.png"></th>
     <th><img width="50px" src="/2019/11/24/大数据相关/zookeeper.jpg"></th>
    </tr>
<tr>
  <td align="center">Hadoop</td>
  <td align="center">Hive</td>
  <td align="center">Spark</td>
  <td align="center">Flink</td>
  <td align="center">HBase</td>
  <td align="center">Kafka</td>
  <td align="center">Zookeeper</td>
</tr>
 <tr>
     <td align="center"><a href="https://hadoop.apache.org/docs/" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://hive.apache.org/" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://spark.apache.org/docs/" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://ci.apache.org/projects/flink/flink-docs-stable/" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://hbase.apache.org/book.html" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">文档</a></td>
  <td align="center"><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">文档</a></td>
</tr>
</table>

<a id="more"></a>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="HDFS架构？-2"><a href="#HDFS架构？-2" class="headerlink" title="HDFS架构？[2]"></a><a href="https://hadoop.apache.org/docs/" target="_blank" rel="noopener">HDFS架构？</a><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://hadoop.apache.org/docs/">[2]</span></a></sup></h3><h4 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS-1"></a><a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" target="_blank" rel="noopener">HDFS-1</a></h4><ul>
<li>架构图</li>
</ul>
<blockquote>
<p>HDFS has a master/slave architecture. An HDFS cluster consists of <font color="red">a single NameNode, a master server</font> that manages the file system namespace and regulates access to files by clients. In addition, there are<font color="red"> a number of DataNodes,</font> usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like <font color="red">opening, closing, and renaming files and directories.</font> It also <font color="red">determines the mapping of blocks to DataNodes.</font> The DataNodes are responsible for <font color="red">serving read and write requests</font> from the file system’s clients. The DataNodes also perform<font color="red"> block creation, deletion, and replication </font>upon instruction from the NameNode.</p>
</blockquote>
<p><img src="/2019/11/24/大数据相关/hdfs1architecture.gif" alt=""></p>
<p><strong><font color="red">NameNode</font></strong></p>
<p>NameNode 负责管理整个分布式系统的元数据，主要包括：</p>
<ul>
<li>目录树结构；</li>
<li>文件到数据库 Block 的映射关系；</li>
</ul>
<ul>
<li>Block 副本及其存储位置等管理数据；</li>
<li>DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。</li>
</ul>
<p>这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。</p>
<ul>
<li>fsimage：是内存命名空间元数据在外存的镜像文件；</li>
<li>editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。</li>
</ul>
<p>这两个文件相结合可以构造完整的内存数据。</p>
<font color="red">**Secondary NameNode**</font>

<p>Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。</p>
<font color="red">**DataNode**</font>

<p>负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。</p>
<h4 id="HDFS-2"><a href="#HDFS-2" class="headerlink" title="HDFS-2"></a><a href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">HDFS-2</a></h4><ul>
<li>架构图和上面一样</li>
<li><a href="">HDFS High Availability</a></li>
</ul>
<p><img src="/2019/11/24/大数据相关/hdfs-ha.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>（1）Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</td>
</tr>
<tr>
<td><strong>（2）ZKFailoverController</strong>（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</td>
</tr>
<tr>
<td><strong>（3）Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持；</td>
</tr>
<tr>
<td><strong>（4）共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</td>
</tr>
<tr>
<td><strong>（5）DataNode 节点</strong>：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="HDFS比较（主要体现在jdk版本，HA等）"><a href="#HDFS比较（主要体现在jdk版本，HA等）" class="headerlink" title="HDFS比较（主要体现在jdk版本，HA等）"></a>HDFS<a href="https://cwiki.apache.org/confluence/display/HADOOP2/Roadmap" target="_blank" rel="noopener">比较</a>（主要体现在jdk版本，HA等）</h3><div class="table-container">
<table>
<thead>
<tr>
<th>比较</th>
<th>特性</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS 1</td>
<td></td>
</tr>
<tr>
<td>HDFS 2</td>
<td>HADOOP，HDFS，YARN，MAPREDUCE</td>
</tr>
<tr>
<td>HDFS 3</td>
<td>Move to JDK8+</td>
</tr>
</tbody>
</table>
</div>
<h3 id="HDFS相关的例子？"><a href="#HDFS相关的例子？" class="headerlink" title="HDFS相关的例子？"></a>HDFS相关的例子？</h3><ul>
<li>常用命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看文件系统的基本信息和统计信息：hdfs dfsadmin -report</span><br><span class="line">建立文件夹: hadoop fs -mkdir /user/tpc-h1G</span><br><span class="line">上传文件:hadoop fs -put *.tbl /user/tpc-h1G</span><br></pre></td></tr></table></figure>
<h3 id="Yarn架构？（资源管理）"><a href="#Yarn架构？（资源管理）" class="headerlink" title="Yarn架构？（资源管理）"></a>Yarn架构？（资源管理）</h3><blockquote>
<p>The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (<em>RM</em>) and per-application ApplicationMaster (<em>AM</em>). An application is either a single job or a DAG of jobs.</p>
</blockquote>
<ul>
<li>原理<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinod  Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad Agarwal, &  Eric Baldeschwieler. (2013). Apache Hadoop YARN: yet another resource  negotiator. *Proceedings of the 4th annual Symposium on Cloud Computing*. ACM.
">[1]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vavilapalli, V. K., Murthy, A. C., Douglas, C., Agarwal, S., Konar, M., Evans, R., ... & Saha, B. (2013, October). Apache hadoop yarn: Yet another resource negotiator. In *Proceedings of the 4th annual Symposium on Cloud Computing* (p. 5). ACM.
">[3]</span></a></sup></li>
<li>Yarn架构图</li>
</ul>
<p><img src="https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt=""></p>
<h4 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h4><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：</p>
<ul>
<li>调度器 Scheduler</li>
</ul>
<p>调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 <strong>资源容器(Resource Container，也即 Container)</strong>，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。</p>
<ul>
<li>应用程序管理器 Applications Manager，ASM。</li>
</ul>
<p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h4 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h4><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h4 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h4><p>提交的每个作业都会包含一个 AM，主要功能包括：</p>
<ul>
<li><p>与 RM 协商以获取资源（用 container 表示）；</p>
</li>
<li><p>将得到的任务进一步分配给内部的任务；</p>
</li>
<li><p>与 NM 通信以启动/停止任务；</p>
</li>
<li><p>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。</p>
</li>
</ul>
<p>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。</p>
<h4 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h4><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h3 id="MapReduce过程？"><a href="#MapReduce过程？" class="headerlink" title="MapReduce过程？"></a>MapReduce过程？</h3><p>MapReduce分为两个阶段: <strong>Map</strong> 和  <strong>Ruduce</strong>.</p>
<h4 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段:"></a><strong>Map阶段:</strong></h4><ul>
<li><p><strong>input</strong>.</p>
<p>在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务</p>
</li>
<li><p><strong>map</strong></p>
</li>
</ul>
<p>就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行 </p>
<ul>
<li><strong>Partition</strong>. </li>
</ul>
<p>需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition.</p>
<ul>
<li><strong>spill</strong></li>
</ul>
<p>此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件.</p>
<ul>
<li><strong>sort</strong> </li>
</ul>
<p>在写入环形缓冲区前,对数据排序.<key,value,partition>格式排序</key,value,partition></p>
<ul>
<li><strong>combine</strong>(可选). </li>
</ul>
<p>在溢出文件之前,提前开始combine,相当于本地化的reduce操作</p>
<ul>
<li><strong>merge</strong> </li>
</ul>
<p>spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件.</p>
<h4 id="Reduce阶段"><a href="#Reduce阶段" class="headerlink" title="Reduce阶段:"></a><strong>Reduce阶段:</strong></h4><ul>
<li><strong>copy</strong>.</li>
</ul>
<p>拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作.</p>
<ul>
<li><p><strong>merge</strong></p>
<p>合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件.  数据经过 <strong>内存-&gt;磁盘 , 磁盘-&gt;磁盘</strong>的过程.</p>
</li>
<li><p><strong>output</strong></p>
</li>
</ul>
<p>merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束</p>
<ul>
<li><strong>reduce</strong></li>
</ul>
<p>开始执行reduce任务,最后结果保留在hdfs上.</p>
<h4 id="案例-4"><a href="#案例-4" class="headerlink" title="案例[4]"></a>案例<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm">[4]</span></a></sup></h4><p><strong>1.下表是一个不同年份的用电量，找出平均用电量最大的年份</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Jan</th>
<th>Feb</th>
<th>Mar</th>
<th>Apr</th>
<th>May</th>
<th>Jun</th>
<th>Jul</th>
<th>Aug</th>
<th>Sep</th>
<th>Oct</th>
<th>Nov</th>
<th>Dec</th>
<th>Avg</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>1979</td>
<td>23</td>
<td>23</td>
<td>2</td>
<td>43</td>
<td>24</td>
<td>25</td>
<td>26</td>
<td>26</td>
<td>26</td>
<td>26</td>
<td>25</td>
<td>26</td>
<td>25</td>
</tr>
<tr>
<td>1980</td>
<td>26</td>
<td>27</td>
<td>28</td>
<td>28</td>
<td>28</td>
<td>30</td>
<td>31</td>
<td>31</td>
<td>31</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>29</td>
</tr>
<tr>
<td>1981</td>
<td>31</td>
<td>32</td>
<td>32</td>
<td>32</td>
<td>33</td>
<td>34</td>
<td>35</td>
<td>36</td>
<td>36</td>
<td>34</td>
<td>34</td>
<td>34</td>
<td>34</td>
</tr>
<tr>
<td>1984</td>
<td>39</td>
<td>38</td>
<td>39</td>
<td>39</td>
<td>39</td>
<td>41</td>
<td>42</td>
<td>43</td>
<td>40</td>
<td>39</td>
<td>38</td>
<td>38</td>
<td>40</td>
</tr>
<tr>
<td>1985</td>
<td>38</td>
<td>39</td>
<td>39</td>
<td>39</td>
<td>39</td>
<td>41</td>
<td>41</td>
<td>41</td>
<td>00</td>
<td>40</td>
<td>39</td>
<td>39</td>
<td>45</td>
</tr>
</tbody>
</table>
</div>
<p>分别实现Mapper和Reducer接口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hadoop; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.*; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.*; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessUnits</span> </span>&#123;</span><br><span class="line">   <span class="comment">//Mapper class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EMapper</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> </span></span><br><span class="line"><span class="class">   <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span> ,/*<span class="title">Input</span> <span class="title">key</span> <span class="title">Type</span> */ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Input</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">Text</span>,                /*<span class="title">Output</span> <span class="title">key</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   <span class="title">IntWritable</span>&gt;        /*<span class="title">Output</span> <span class="title">value</span> <span class="title">Type</span>*/ </span></span><br><span class="line"><span class="class">   </span>&#123;</span><br><span class="line">      <span class="comment">//Map function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, </span></span></span><br><span class="line"><span class="function"><span class="params">      OutputCollector&lt;Text, IntWritable&gt; output,   </span></span></span><br><span class="line"><span class="function"><span class="params">      </span></span></span><br><span class="line"><span class="function"><span class="params">      Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">         String line = value.toString(); </span><br><span class="line">         String lasttoken = <span class="keyword">null</span>; </span><br><span class="line">         StringTokenizer s = <span class="keyword">new</span> StringTokenizer(line,<span class="string">"\t"</span>); </span><br><span class="line">         String year = s.nextToken(); </span><br><span class="line">         </span><br><span class="line">         <span class="keyword">while</span>(s.hasMoreTokens()) &#123;</span><br><span class="line">            lasttoken = s.nextToken();</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">int</span> avgprice = Integer.parseInt(lasttoken); </span><br><span class="line">         output.collect(<span class="keyword">new</span> Text(year), <span class="keyword">new</span> IntWritable(avgprice)); </span><br><span class="line">      &#125; </span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Reducer class </span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">E_EReduce</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Reducer</span>&lt; <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span> &gt; </span>&#123;</span><br><span class="line">   </span><br><span class="line">      <span class="comment">//Reduce function </span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">( Text key, Iterator &lt;IntWritable&gt; values, </span></span></span><br><span class="line"><span class="function"><span class="params">      OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">         <span class="keyword">int</span> maxavg = <span class="number">30</span>; </span><br><span class="line">         <span class="keyword">int</span> val = Integer.MIN_VALUE; </span><br><span class="line">            </span><br><span class="line">         <span class="keyword">while</span> (values.hasNext()) &#123; </span><br><span class="line">            <span class="keyword">if</span>((val = values.next().get())&gt;maxavg) &#123; </span><br><span class="line">               output.collect(key, <span class="keyword">new</span> IntWritable(val)); </span><br><span class="line">            &#125; </span><br><span class="line">         &#125;</span><br><span class="line">      &#125; </span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//Main function </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span><span class="keyword">throws</span> Exception </span>&#123; </span><br><span class="line">      JobConf conf = <span class="keyword">new</span> JobConf(ProcessUnits.class); </span><br><span class="line">      <span class="comment">//任务：获取最大用电量</span></span><br><span class="line">      conf.setJobName(<span class="string">"max_eletricityunits"</span>); </span><br><span class="line">      conf.setOutputKeyClass(Text.class);</span><br><span class="line">      conf.setOutputValueClass(IntWritable.class); </span><br><span class="line">      conf.setMapperClass(E_EMapper.class); </span><br><span class="line">      conf.setCombinerClass(E_EReduce.class); </span><br><span class="line">      conf.setReducerClass(E_EReduce.class); </span><br><span class="line">      conf.setInputFormat(TextInputFormat.class); </span><br><span class="line">      conf.setOutputFormat(TextOutputFormat.class); </span><br><span class="line">      </span><br><span class="line">      FileInputFormat.setInputPaths(conf, <span class="keyword">new</span> Path(args[<span class="number">0</span>])); </span><br><span class="line">      FileOutputFormat.setOutputPath(conf, <span class="keyword">new</span> Path(args[<span class="number">1</span>])); </span><br><span class="line">      </span><br><span class="line">      JobClient.runJob(conf); </span><br><span class="line">   &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不建立单机环境，仅导包完成调试功能：运行和调试MapReduce程序只需要有相应的Hadoop依赖包就行，可以完全当成一个普通的JAVA程序。</p>
<p><strong>2.排序：order by</strong></p>
<p><strong>3.去重：distinct</strong></p>
<p><strong>4.多表查询</strong></p>
<p><strong>5.倒排索引</strong></p>
<p>(ps:spark经典案例<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[http://www.damaoguo.site/2019/03/30/spark%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/](http://www.damaoguo.site/2019/03/30/spark经典案例/)">[8]</span></a></sup>)</p>
<h3 id="Yarn-调度MapReduce？"><a href="#Yarn-调度MapReduce？" class="headerlink" title="Yarn 调度MapReduce？"></a>Yarn 调度MapReduce？</h3><p>Yarn采用的双层调度框架，RM将资源分配给AM,AM再将资源进一步分配给Task,资源不够时会为TASK预留，直到资源充足。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="hdfs写流程？"><a href="#hdfs写流程？" class="headerlink" title="hdfs写流程？"></a>hdfs写流程？</h3><p><img src="/2019/11/24/大数据相关/hdfs写流程.png"></p>
<ol>
<li>Client 调用 DistributedFileSystem 对象的 <code>create</code> 方法，创建一个文件输出流（FSDataOutputStream）对象；</li>
<li>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</li>
<li>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</li>
<li>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</li>
<li>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</li>
<li>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 <code>close</code> 方法，完成文件写入；</li>
<li>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</li>
</ol>
<h3 id="hdfs读流程？"><a href="#hdfs读流程？" class="headerlink" title="hdfs读流程？"></a>hdfs读流程？</h3><p><img src="/2019/11/24/大数据相关/hdfs读流程.png" alt=""></p>
<ol>
<li>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</li>
<li>NameNode 返回存储的每个块的 DataNode 列表；</li>
<li>Client 将连接到列表中最近的 DataNode；</li>
<li>Client 开始从 DataNode 并行读取数据；</li>
<li>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</li>
</ol>
<h3 id="hdfs创建一个文件的流程？-类的调用过程"><a href="#hdfs创建一个文件的流程？-类的调用过程" class="headerlink" title="hdfs创建一个文件的流程？(类的调用过程)"></a>hdfs创建一个文件的流程？(类的调用过程)</h3><ul>
<li>Apache Hadoop HDFS 2.9.1 API<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://hadoop.apache.org/docs/r2.9.1/hadoop-project-dist/hadoop-hdfs/api/overview-summary.html 
">[5]</span></a></sup></li>
</ul>
<ol>
<li>客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。</li>
<li>FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。</li>
<li>进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。</li>
<li>最后，RpcServer将RPC响应返回给客户端。</li>
</ol>
<h3 id="hadoop1-x-和hadoop-2-x-的区别？"><a href="#hadoop1-x-和hadoop-2-x-的区别？" class="headerlink" title="hadoop1.x 和hadoop 2.x 的区别？"></a>hadoop1.x 和hadoop 2.x 的区别？</h3><ul>
<li><strong>资源调度方式的改变</strong></li>
</ul>
<p>在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理.</p>
<ul>
<li><strong>HA模式</strong></li>
</ul>
<p>在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode.</p>
<ul>
<li><strong>HDFS Federation</strong></li>
</ul>
<p>Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性</p>
<h4 id="hadoop1-x："><a href="#hadoop1-x：" class="headerlink" title="hadoop1.x："></a>hadoop1.x：</h4><p><img src="/2019/11/24/大数据相关/hadoop1.jpg" alt=""></p>
<h4 id="hadoop2-x："><a href="#hadoop2-x：" class="headerlink" title="hadoop2.x："></a>hadoop2.x：</h4><p><img src="/2019/11/24/大数据相关/hadoop2.jpg" alt=""></p>
<h3 id="hadoop-HA介绍？"><a href="#hadoop-HA介绍？" class="headerlink" title="hadoop HA介绍？"></a>hadoop HA介绍？</h3><ul>
<li><strong><font color="red">HDFS High Availability</font></strong><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html
">[6]</span></a></sup></li>
</ul>
<ol>
<li><strong>Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li>
<li><strong>ZKFailoverController（主备切换控制器，FC）</strong>：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</li>
<li><strong>Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持；</li>
<li><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</li>
<li><strong>DataNode 节点</strong>：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ol>
<h3 id="hadoop的常用配置文件？"><a href="#hadoop的常用配置文件？" class="headerlink" title="hadoop的常用配置文件？"></a>hadoop的常用配置文件？</h3><p><strong>在TPC-H的测评实验中，使用配置文件见github.com/maomao1994/TPC-H<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://github.com/maomao1994/TPC-H/
">[7]</span></a></sup></strong></p>
<ol>
<li><strong><font color="red">hadoop-env.sh</font></strong>: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；</li>
<li><strong><font color="red">core-site.xml</font></strong>: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；</li>
<li><strong><font color="red">hdfs-site.xml</font></strong>: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置；</li>
<li><strong><font color="red">mapred-site.xml</font></strong>：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；</li>
<li><strong><font color="red">yarn-site.xml</font></strong></li>
<li><strong><font color="red">~/hadoop/etc/hadoop/slaves</font></strong></li>
</ol>
<h3 id="小文件过多会有什么危害-如何避免？"><a href="#小文件过多会有什么危害-如何避免？" class="headerlink" title="小文件过多会有什么危害,如何避免？"></a>小文件过多会有什么危害,如何避免？</h3><p>Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存.</p>
<p>每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间.</p>
<p>显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat<k,v>实现小文件的合并</k,v></p>
<h3 id="启动hadoop集群会分别启动哪些进程-各自的作用？"><a href="#启动hadoop集群会分别启动哪些进程-各自的作用？" class="headerlink" title="启动hadoop集群会分别启动哪些进程,各自的作用？"></a>启动hadoop集群会分别启动哪些进程,各自的作用？</h3><ol>
<li><p><strong><font color="red">NameNode：</font></strong></p>
<ul>
<li>维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件</li>
<li>记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息</li>
<li>负责响应客户端的   数据块位置请求  。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取</li>
<li>接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息</li>
</ul>
</li>
<li><p><strong><font color="red">SecondaryNameNode(非HA模式)：</font></strong></p>
<ul>
<li>实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(<strong>注意 并不是NameNode的备用节点</strong>)</li>
</ul>
</li>
<li><strong><font color="red">DataNode：</font></strong><ul>
<li>根据需要存取并检索数据块</li>
<li>定期向namenode发送其存储的数据块列表</li>
</ul>
</li>
<li><strong><font color="red">ResourceManager：</font></strong><ul>
<li>负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。</li>
</ul>
</li>
<li><p><strong><font color="red">NodeManager：</font></strong></p>
<ul>
<li>运行ResourceManager分配的任务，同时将任务进度向application master报告</li>
</ul>
</li>
<li><p><strong><font color="red">JournalNode(HA下启用):</font></strong></p>
<ul>
<li>高可用情况下存放namenode的editlog文件</li>
</ul>
</li>
</ol>
<h2 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>The Apache Hive ™ data warehouse software facilitates<font color="red"> reading, writing, and managing large datasets residing in distributed storage using SQL.</font> Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>
<h3 id="HIVE架构"><a href="#HIVE架构" class="headerlink" title="HIVE架构"></a>HIVE架构</h3><p><strong><font color="red">下面是hive的架构图</font><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://www.tutorialspoint.com/hive/hive_introduction.htm
">[9]</span></a></sup></strong></p>
<p><img src="/2019/11/24/大数据相关/hive_architecture.jpg" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Unit Name</th>
<th>Operation</th>
</tr>
</thead>
<tbody>
<tr>
<td>User Interface</td>
<td>Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server).</td>
</tr>
<tr>
<td>Meta Store</td>
<td>Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping.</td>
</tr>
<tr>
<td>HiveQL Process Engine</td>
<td>HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it.</td>
</tr>
<tr>
<td>Execution Engine</td>
<td>The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce.</td>
</tr>
<tr>
<td>HDFS or HBASE</td>
<td>Hadoop distributed file system or HBASE are the data storage techniques to store data into file system.</td>
</tr>
</tbody>
</table>
</div>
<font color="red">**下面hive和hadoop的交互图<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://www.tutorialspoint.com/hive/hive_introduction.htm
">[9]</span></a></sup>**</font>

<p><img src="/2019/11/24/大数据相关/how_hive_works.jpg" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Step No.</th>
<th>Operation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>Execute Query</strong>The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.</td>
</tr>
<tr>
<td>2</td>
<td><strong>Get Plan</strong>The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.</td>
</tr>
<tr>
<td>3</td>
<td><strong>Get Metadata</strong>The compiler sends metadata request to Metastore (any database).</td>
</tr>
<tr>
<td>4</td>
<td><strong>Send Metadata</strong>Metastore sends metadata as a response to the compiler.</td>
</tr>
<tr>
<td>5</td>
<td><strong>Send Plan</strong>The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.</td>
</tr>
<tr>
<td>6</td>
<td><strong>Execute Plan</strong>The driver sends the execute plan to the execution engine.</td>
</tr>
<tr>
<td>7</td>
<td><strong>Execute Job</strong>Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.</td>
</tr>
<tr>
<td>7.1</td>
<td><strong>Metadata Ops</strong>Meanwhile in execution, the execution engine can execute metadata operations with Metastore.</td>
</tr>
<tr>
<td>8</td>
<td><strong>Fetch Result</strong>The execution engine receives the results from Data nodes.</td>
</tr>
<tr>
<td>9</td>
<td><strong>Send Results</strong>The execution engine sends those resultant values to the driver.</td>
</tr>
<tr>
<td>10</td>
<td><strong>Send Results</strong>The driver sends the results to Hive Interfaces.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="hive的数据类型"><a href="#hive的数据类型" class="headerlink" title="hive的数据类型"></a>hive的数据类型</h3><font color="red">**All the data types in Hive are classified into four types, given as follows:<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://www.tutorialspoint.com/hive/hive_introduction.htm
">[9]</span></a></sup>**</font>

<ol>
<li>Column Types</li>
<li>Literals</li>
<li>Null Values</li>
<li>Complex Types</li>
</ol>
<p>hive的数据类型细分如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>细分</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><font color="red"><strong>Column Types</strong></font></td>
<td><strong>Integral Types</strong></td>
<td>TINYINT，SMALLINT，INT，BIGINT</td>
</tr>
<tr>
<td></td>
<td><strong>String Types</strong></td>
<td>VARCHAR，CHAR</td>
</tr>
<tr>
<td></td>
<td><strong>Timestamp</strong></td>
<td>It supports traditional UNIX timestamp with optional nanosecond precision. <br>It supports java.sql.Timestamp format “YYYY-MM-DD HH:MM:SS.fffffffff” and format “yyyy-mm-dd hh:mm:ss.ffffffffff”.</td>
</tr>
<tr>
<td></td>
<td><strong>Dates</strong></td>
<td>NaN.</td>
</tr>
<tr>
<td></td>
<td><strong>Decimals</strong></td>
<td>as same as Big Decimal format of Java<br>DECIMAL(precision, scale)</td>
</tr>
<tr>
<td></td>
<td><strong>Union Types</strong></td>
<td>UNIONTYPE\<int, double,="" array\<string="">, struct\<a:int,b:string>&gt;</a:int,b:string></int,></td>
</tr>
<tr>
<td><strong><font color="red">Literals</font></strong></td>
<td><strong>Floating Point Types</strong></td>
<td>numbers with decimal points</td>
</tr>
<tr>
<td></td>
<td><strong>Decimal Type</strong></td>
<td>Decimal type data is nothing but floating point value with higher range than DOUBLE data type. The range of decimal type is approximately -10^-308 to 10^308.</td>
</tr>
<tr>
<td><strong><font color="red">Null Value</font></strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong><font color="red">Complex Types</font></strong></td>
<td><strong>Arrays</strong></td>
<td>Syntax: ARRAY\<data_type></data_type></td>
</tr>
<tr>
<td></td>
<td><strong>Maps</strong></td>
<td>Syntax: MAP\<primitive_type, data_type=""></primitive_type,></td>
</tr>
<tr>
<td></td>
<td><strong>Structs</strong></td>
<td>Syntax: STRUCT\<col_name :="" data_type="" [comment="" col_comment],="" ...=""></col_name></td>
</tr>
</tbody>
</table>
</div>
<h3 id="SQL和JDBC"><a href="#SQL和JDBC" class="headerlink" title="SQL和JDBC"></a>SQL和JDBC</h3><font color="red">**请参考教程<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://www.tutorialspoint.com/hive/hive_introduction.htm
">[9]</span></a></sup>**</font>

<h3 id="hive-内部表和外部表的区别？"><a href="#hive-内部表和外部表的区别？" class="headerlink" title="hive 内部表和外部表的区别？"></a>hive 内部表和外部表的区别？</h3><ul>
<li>建表时带有external关键字为外部表，否则为内部表</li>
<li>内部表和外部表建表时都可以自己指定location</li>
<li>删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除</li>
<li>其他用法是一样的</li>
</ul>
<h3 id="Hive中-sort-by-order-by-cluster-by-distribute-by-的区别？"><a href="#Hive中-sort-by-order-by-cluster-by-distribute-by-的区别？" class="headerlink" title="Hive中 sort by / order by / cluster by / distribute by 的区别？"></a>Hive中 <font color="red">sort by / order by / cluster by / distribute by </font>的区别？</h3><ul>
<li><p><strong>order by</strong> </p>
<pre><code>order by 是要对输出的结果进行全局排序，这就意味着**只有一个reducer**才能实现（多个reducer无法保证全局有序）但是当数据量过大的时候，效率就很低。如果在严格模式下（hive.mapred.mode=strict）,则必须配合limit使用
</code></pre></li>
<li><p><strong>sort by</strong></p>
<pre><code>sort by 不是全局排序，只是在进入到reducer之前完成排序，只保证了每个reducer中数据按照指定字段的有序性，是局部排序。配置mapred.reduce.tasks=[nums]可以对输出的数据执行归并排序。可以配合limit使用，提高性能
</code></pre></li>
<li><p><strong>distribute by</strong> </p>
<pre><code>distribute by 指的是按照指定的字段划分到不同的输出reduce文件中，和sort by一起使用时需要注意，distribute by必须放在前面
</code></pre></li>
<li><p><strong>cluster by</strong></p>
<p>cluster by 可以看做是一个特殊的distribute by+sort by，它具备二者的功能，但是只能实现倒序排序的方式,不能指定排序规则为asc 或者desc</p>
</li>
</ul>
<h3 id="hive的metastore的三种模式？"><a href="#hive的metastore的三种模式？" class="headerlink" title="hive的metastore的三种模式？"></a>hive的metastore的三种模式？</h3><ul>
<li><p><strong><font color="red">内嵌Derby方式</font></strong></p>
<p>这个是Hive默认的启动模式，一般用于单元测试，这种存储方式有一个缺点：在同一时间只能有一个进程连接使用数据库。</p>
</li>
<li><p><strong><font color="red">Local方式</font></strong></p>
<p>本地MySQL</p>
</li>
<li><p><strong><font color="red">Remote方式</font></strong></p>
<p>远程MySQL,一般常用此种方式</p>
</li>
</ul>
<h3 id="hive-中-join都有哪些？"><a href="#hive-中-join都有哪些？" class="headerlink" title="hive 中 join都有哪些？"></a>hive 中 join都有哪些？</h3><p>Hive中除了支持和传统数据库中一样的<font color="red">内关联（JOIN）、左关联（LEFT JOIN）、右关联（RIGHT JOIN）、全关联（FULL JOIN），还支持左半关联（LEFT SEMI JOIN）</font></p>
<ul>
<li><p><strong>内关联（JOIN）</strong></p>
<p>只返回能关联上的结果。</p>
</li>
<li><p><strong>左外关联（LEFT [OUTER] JOIN）</strong></p>
<p>以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。</p>
</li>
<li><p><strong>右外关联（RIGHT [OUTER] JOIN）</strong></p>
<p>和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。</p>
</li>
<li><p><strong>全外关联（FULL [OUTER] JOIN）</strong></p>
<p>以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。</p>
</li>
<li><p><strong>LEFT SEMI JOIN</strong></p>
<p>以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录</p>
</li>
<li><p><strong>笛卡尔积关联（CROSS JOIN）</strong></p>
<p>返回两个表的笛卡尔积结果，不需要指定关联键。</p>
</li>
</ul>
<h3 id="Impala-和-hive-的查询有哪些区别？"><a href="#Impala-和-hive-的查询有哪些区别？" class="headerlink" title="Impala 和 hive 的查询有哪些区别？"></a>Impala 和 hive 的查询有哪些区别？</h3><p><strong>Impala是基于Hive的大数据实时分析查询引擎</strong>，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。</p>
<p><strong><font color="red">Impala相对于Hive所使用的优化技术</font></strong></p>
<ul>
<li>1、没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。</li>
<li>2、使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。</li>
<li>3、充分利用可用的硬件指令（SSE4.2）。</li>
<li>4、更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。</li>
<li>5、通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。</li>
<li>6、最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。</li>
</ul>
<h3 id="Hive中大表join小表的优化方法？"><a href="#Hive中大表join小表的优化方法？" class="headerlink" title="Hive中大表join小表的优化方法？"></a>Hive中大表join小表的优化方法？</h3><font color="red">在小表和大表进行join时，将**小表放在前边**，效率会高，hive会将小表进行缓存</font>

<h3 id="Hive-Sql-是怎样解析成MR-job的"><a href="#Hive-Sql-是怎样解析成MR-job的" class="headerlink" title="Hive Sql 是怎样解析成MR job的?"></a>Hive Sql 是怎样解析成MR job的?</h3><p><strong>主要分为6个阶段:</strong></p>
<ol>
<li><strong><font color="red">Hive使用Antlr实现语法解析</font></strong>.根据Antlr制定的SQL语法解析规则,完成SQL语句的词法/语法解析,<font color="red">将SQL转为抽象语法树AST.</font></li>
<li><strong><font color="red">遍历AST,生成基本查询单元QueryBlock</font></strong>.QueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出.</li>
<li><strong><font color="red">遍历QueryBlock,生成OperatorTree</font></strong>.Hive最终生成的MapReduce任务，<font color="red">Map阶段和Reduce阶段均由OperatorTree组成,</font>。Operator就是在Map阶段或者Reduce阶段完成单一特定的操作。QueryBlock生成Operator Tree就是遍历上一个过程中生成的QB和QBParseInfo对象的保存语法的属性.</li>
<li><strong><font color="red">优化OperatorTree</font></strong>大部分逻辑层优化器通过变换OperatorTree，合并操作符，达到减少MapReduce Job，减少shuffle数据量的目的</li>
<li><strong><font color="red">OperatorTree生成MapReduce Job</font></strong>.遍历OperatorTree,翻译成MR任务.<ul>
<li>对输出表生成MoveTask</li>
<li>从OperatorTree的其中一个根节点向下深度优先遍历</li>
<li>ReduceSinkOperator标示Map/Reduce的界限，多个Job间的界限</li>
<li>遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask</li>
<li>生成StatTask更新元数据</li>
<li>剪断Map与Reduce间的Operator的关系</li>
</ul>
</li>
<li><strong><font color="red">优化任务.</font></strong> 使用物理优化器对MR任务进行优化,生成最终执行任务</li>
</ol>
<h3 id="Hive-UDF简单介绍？"><a href="#Hive-UDF简单介绍？" class="headerlink" title="Hive UDF简单介绍？"></a>Hive UDF简单介绍？</h3><p>在Hive中，用户可以自定义一些函数，用于扩展HiveQL的功能，而这类函数叫做UDF（用户自定义函数）。UDF分为两大类：UDAF（用户自定义聚合函数）和UDTF（用户自定义表生成函数）。</p>
<p><strong><font color="red">Hive有两个不同的接口编写UDF程序。一个是基础的UDF接口，一个是复杂的GenericUDF接口。</font></strong></p>
<ol>
<li><strong>org.apache.hadoop.hive.ql. exec.UDF</strong> 基础UDF的函数读取和返回基本类型，即Hadoop和Hive的基本类型。如，Text、IntWritable、LongWritable、DoubleWritable等。</li>
<li><strong>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</strong> 复杂的GenericUDF可以处理Map、List、Set类型。</li>
</ol>
<h3 id="SQL题-按照学生科目分组-取每个科目的TopN？"><a href="#SQL题-按照学生科目分组-取每个科目的TopN？" class="headerlink" title="SQL题: 按照学生科目分组, 取每个科目的TopN？"></a>SQL题: 按照学生科目分组, 取每个科目的TopN？</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">id,name,subject,score</span><br><span class="line">1,小明,语文,87</span><br><span class="line">2,张三,语文,27</span><br><span class="line">3,王五,语文,69</span><br><span class="line">4,李四,语文,99</span><br><span class="line">5,小明,数学,86</span><br><span class="line">6,马六,数学,33</span><br><span class="line">7,李四,数学,44</span><br><span class="line">8,小红,数学,50</span><br></pre></td></tr></table></figure>
<p><strong>按照各个科目的成绩排名 取 Top3</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a.* from</span><br><span class="line">(select id,name,subject,score,row_number() over(partition by subject order by score desc) rank from student) a</span><br><span class="line">where a.rank &lt;= 3</span><br></pre></td></tr></table></figure>
<h3 id="SQL题-获取每个用户的前1-4次的数据？"><a href="#SQL题-获取每个用户的前1-4次的数据？" class="headerlink" title="SQL题: 获取每个用户的前1/4次的数据？"></a>SQL题: 获取每个用户的前1/4次的数据？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cookieId  createTime    pv</span><br><span class="line">--------------------------</span><br><span class="line">cookie1 2015-04-10      1</span><br><span class="line">cookie1 2015-04-11      5</span><br><span class="line">cookie1 2015-04-12      7</span><br><span class="line">cookie1 2015-04-13      3</span><br><span class="line">cookie1 2015-04-14      2</span><br><span class="line">cookie1 2015-04-15      4</span><br><span class="line">cookie1 2015-04-16      4</span><br><span class="line">cookie2 2015-04-10      2</span><br><span class="line">cookie2 2015-04-11      3</span><br><span class="line">cookie2 2015-04-12      5</span><br><span class="line">cookie2 2015-04-13      6</span><br><span class="line">cookie2 2015-04-14      3</span><br><span class="line">cookie2 2015-04-15      9</span><br><span class="line">cookie2 2015-04-16      7</span><br></pre></td></tr></table></figure>
<p>获取每个用户前1/4次的访问记录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.* from </span><br><span class="line">(SELECT cookieid,createtime,pv,NTILE(4)</span><br><span class="line">OVER(PARTITION BY cookieId ORDER BY createtime) AS rn</span><br><span class="line">from table ) a</span><br><span class="line">WHERE a.rn = 1</span><br></pre></td></tr></table></figure>
<p><font color="red">NTILE(n)</font>，用于将分组数据按照顺序切分成n片，返回当前切片值</p>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="spark-的运行架构？"><a href="#spark-的运行架构？" class="headerlink" title="spark 的运行架构？"></a>spark 的运行架构？</h3><p><img src="/2019/11/24/大数据相关/spark架构图.jpg" alt=""></p>
<ul>
<li><strong><font color="red">Cluster Manager(Master)</font></strong>：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器</li>
<li><strong><font color="red">Worker节点</font></strong>：从节点，负责控制计算节点，启动Executor或者Driver。</li>
<li><strong><font color="red">Driver</font></strong>： 运行Application 的main()函数</li>
<li><strong><font color="red">Executor</font></strong>：执行器，是为某个Application运行在worker node上的一个进程</li>
</ul>
<h3 id="一个spark程序的执行流程？"><a href="#一个spark程序的执行流程？" class="headerlink" title="一个spark程序的执行流程？"></a>一个spark程序的执行流程？</h3><p><img src="/2019/11/24/大数据相关/spark程序执行流程.jpg" alt=""></p>
<ul>
<li><strong>A -&gt;</strong> 当 Driver 进程被启动之后,首先它将发送请求到Master节点上,进行Spark应用程序的注册</li>
<li><strong>B -&gt;</strong> Master在接受到Spark应用程序的注册申请之后,会发送给Worker,让其进行资源的调度和分配.</li>
<li><strong>C -&gt;</strong> Worker 在接受Master的请求之后,会为Spark应用程序启动Executor, 来分配资源</li>
<li><strong>D -&gt;</strong> Executor启动分配资源好后,就会想Driver进行反注册,这是Driver已经知道哪些Executor为他服务了</li>
<li><strong>E -&gt;</strong> 当Driver得到注册了Executor之后,就可以开始正式执行spark应用程序了. 首先第一步,就是创建初始RDD,读取数据源,再执行之后的一系列算子. HDFS文件内容被读取到多个worker节点上,形成内存中的分布式数据集,也就是初始RDD</li>
<li><strong>F -&gt;</strong> Driver就会根据 Job 任务任务中的算子形成对应的task,最后提交给 Executor, 来分配给task进行计算的线程</li>
<li><strong>G -&gt;</strong> task就会去调用对应的任务数据来计算,并task会对调用过来的RDD的partition数据执行指定的算子操作,形成新的RDD的partition,这时一个大的循环就结束了</li>
<li>后续的RDD的partition数据又通过Driver形成新的一批task提交给Executor执行,循环这个操作,直到所有的算子结束</li>
</ul>
<h3 id="spark的shuffle介绍？"><a href="#spark的shuffle介绍？" class="headerlink" title="spark的shuffle介绍？"></a>spark的shuffle介绍？</h3><p><strong>spark中的shuffle主要有3种:</strong></p>
<ul>
<li><p><strong>Hash Shuffle</strong> 2.0以后移除</p>
<p><img src="/2019/11/24/大数据相关/spark-shuffle-v1.png" alt=""></p>
<p>在map阶段(shuffle write)，每个map都会为下游stage的每个partition写一个临时文件，假如下游stage有1000个partition，那么每个map都会生成1000个临时文件，一般来说一个executor上会运行多个map task，这样下来，一个executor上会有非常多的临时文件，假如一个executor上运行M个map task，下游stage有N个partition，那么一个executor上会生成M<em>N个文件。另一方面，如果一个executor上有K个core，那么executor同时可运行K个task，这样一来，就会同时申请K</em>N个文件描述符，一旦partition数较多，势必会耗尽executor上的文件描述符，同时生成K*N个write handler也会带来大量内存的消耗。</p>
<p>在reduce阶段(shuffle read)，每个reduce task都会拉取所有map对应的那部分partition数据，那么executor会打开所有临时文件准备网络传输，这里又涉及到大量文件描述符，另外，如果reduce阶段有combiner操作，那么它会把网络中拉到的数据保存在一个<code>HashMap</code>中进行合并操作，如果数据量较大，很容易引发OOM操作。</p>
</li>
<li><p><strong>Sort Shuffle</strong> 1.1开始(sort shuffle也经历过优化升级,详细见参考文章1)</p>
<p><img src="/2019/11/24/大数据相关/spark-shuffle-v3.png" alt=""></p>
<p>在map阶段(shuffle write)，会按照partition id以及key对记录进行排序，将所有partition的数据写在同一个文件中，该文件中的记录首先是按照partition id排序一个一个分区的顺序排列，每个partition内部是按照key进行排序存放，map task运行期间会顺序写每个partition的数据，并通过一个索引文件记录每个partition的大小和偏移量。这样一来，每个map task一次只开两个文件描述符，一个写数据，一个写索引，大大减轻了Hash Shuffle大量文件描述符的问题，即使一个executor有K个core，那么最多一次性开K*2个文件描述符。</p>
<p>在reduce阶段(shuffle read)，reduce task拉取数据做combine时不再是采用<code>HashMap</code>，而是采用<code>ExternalAppendOnlyMap</code>，该数据结构在做combine时，如果内存不足，会刷写磁盘，很大程度的保证了鲁棒性，避免大数据情况下的OOM。</p>
</li>
<li><p><strong>Unsafe Shuffle</strong> 1.5开始, 1.6与Sort shuffle合并</p>
<p>从spark 1.5.0开始，spark开始了钨丝计划(Tungsten)，目的是优化内存和CPU的使用，进一步提升spark的性能。为此，引入Unsafe Shuffle，它的做法是将数据记录用二进制的方式存储，直接在序列化的二进制数据上sort而不是在java 对象上，这样一方面可以减少memory的使用和GC的开销，另一方面避免shuffle过程中频繁的序列化以及反序列化。在排序过程中，它提供cache-efficient sorter，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能.</p>
</li>
</ul>
<hr>
<p><strong>现在2.1 分为三种writer， 分为 BypassMergeSortShuffleWriter， SortShuffleWriter 和 UnsafeShuffleWriter</strong></p>
<h4 id="三种Writer的分类"><a href="#三种Writer的分类" class="headerlink" title="三种Writer的分类"></a>三种Writer的分类</h4><p><img src="/2019/11/24/大数据相关/sparkShuffleWriter.jpg" alt=""></p>
<p>上面是使用哪种 writer 的判断依据， 是否开启 mapSideCombine 这个判断，是因为有些算子会在 map 端先进行一次 combine， 减少传输数据。 因为 BypassMergeSortShuffleWriter 会临时输出Reducer个（分区数目）小文件，所以分区数必须要小于一个阀值，默认是小于200</p>
<p>UnsafeShuffleWriter需要Serializer支持relocation，Serializer支持relocation：原始数据首先被序列化处理，并且再也不需要反序列，在其对应的元数据被排序后，需要Serializer支持relocation，在指定位置读取对应数据</p>
<p><a href="http://sharkdtu.com/posts/spark-shuffle.html" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="http://spark.coolplayer.net/?p=576" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="Spark的-partitioner-都有哪些"><a href="#Spark的-partitioner-都有哪些" class="headerlink" title="Spark的 partitioner 都有哪些?"></a>Spark的 partitioner 都有哪些?</h3><p><strong>Partitioner主要有两个实现类：HashPartitioner和RangePartitioner,HashPartitioner是大部分transformation的默认实现，sortBy、sortByKey使用RangePartitioner实现，也可以自定义Partitioner.</strong></p>
<ul>
<li><p><strong>HashPartitioner</strong></p>
<p>numPartitions方法返回传入的分区数，getPartition方法使用key的hashCode值对分区数取模得到PartitionId，写入到对应的bucket中。</p>
</li>
<li><p><strong>RangePartitioner</strong></p>
<p>RangePartitioner是先根据所有partition中数据的分布情况，尽可能均匀地构造出重分区的分隔符，再将数据的key值根据分隔符进行重新分区</p>
<ul>
<li>使用reservoir Sample方法对每个Partition进行分别抽样</li>
<li>对数据量大(大于sampleSizePerPartition)的分区进行重新抽样</li>
<li>由权重信息计算出分区分隔符rangeBounds</li>
<li>由rangeBounds计算分区数和key的所属分区</li>
</ul>
</li>
</ul>
<p><a href="https://blog.csdn.net/qq_34842671/article/details/83685179" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="spark-有哪几种join？"><a href="#spark-有哪几种join？" class="headerlink" title="spark 有哪几种join？"></a>spark 有哪几种join？</h3><p><strong>Spark 中和 join 相关的算子有这几个</strong>：<code>join</code>、<code>fullOuterJoin</code>、<code>leftOuterJoin</code>、<code>rightOuterJoin</code></p>
<ul>
<li><p><strong>join</strong></p>
<p>join函数会输出两个RDD中key相同的所有项，并将它们的value联结起来，它联结的key要求在两个表中都存在，类似于SQL中的INNER JOIN。但它不满足交换律，a.join(b)与b.join(a)的结果不完全相同，值插入的顺序与调用关系有关。</p>
</li>
<li><p><strong>leftOuterJoin</strong></p>
<p>leftOuterJoin会保留对象的所有key，而用None填充在参数RDD other中缺失的值，因此调用顺序会使结果完全不同。如下面展示的结果，</p>
</li>
<li><p><strong>rightOuterJoin</strong></p>
<p>rightOuterJoin与leftOuterJoin基本一致，区别在于它的结果保留的是参数other这个RDD中所有的key。</p>
</li>
<li><p><strong>fullOuterJoin</strong></p>
<p>fullOuterJoin会保留两个RDD中所有的key，因此所有的值列都有可能出现缺失的情况，所有的值列都会转为Some对象。</p>
</li>
</ul>
<p><a href="http://www.neilron.xyz/join-in-spark/" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="RDD有哪些特点？"><a href="#RDD有哪些特点？" class="headerlink" title="RDD有哪些特点？"></a>RDD有哪些特点？</h3><ol>
<li><p><strong>A list of partitions</strong><br>RDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。</p>
</li>
<li><p><strong>A function for computing each split</strong><br>RDD的每个partition上面都会有function，也就是函数应用，其作用是实现RDD之间partition的转换。</p>
</li>
<li><p><strong>A list of dependencies on other RDDs</strong><br>RDD会记录它的依赖 ，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。</p>
</li>
<li><p><strong>Optionally,a Partitioner for Key-value RDDs</strong><br>可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面</p>
</li>
<li><p><strong>Optionally, a list of preferred locations to compute each split on</strong></p>
<p>最优的位置去计算，也就是数据的本地性。</p>
</li>
</ol>
<h3 id="讲一下宽依赖和窄依赖？"><a href="#讲一下宽依赖和窄依赖？" class="headerlink" title="讲一下宽依赖和窄依赖？"></a>讲一下宽依赖和窄依赖？</h3><p>区别宽窄依赖的核心点是 <strong>子RDD的partition与父RDD的partition是否是1对多的关系</strong>,如果是这样的关系的话,</p>
<p>说明多个父rdd的partition需要经过shuffle过程汇总到一个子rdd的partition,这样就是一次宽依赖,在DAGScheduler中会产生stage的切分.</p>
<h3 id="Spark中的算子都有哪些？"><a href="#Spark中的算子都有哪些？" class="headerlink" title="Spark中的算子都有哪些？"></a>Spark中的算子都有哪些？</h3><p>总的来说,spark分为两大类算子:</p>
<ul>
<li><p><strong>Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理</strong></p>
<p>Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算</p>
</li>
<li><p><strong>Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业</strong></p>
<p>Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark系统</p>
<hr>
</li>
</ul>
<h4 id="1-Value数据类型的Transformation算子"><a href="#1-Value数据类型的Transformation算子" class="headerlink" title="1. Value数据类型的Transformation算子"></a>1. Value数据类型的Transformation算子</h4><ul>
<li>输入分区与输出分区一对一型<ul>
<li>map算子</li>
<li>flatMap算子</li>
<li>mapPartitions算子</li>
<li>glom算子</li>
</ul>
</li>
<li>输入分区与输出分区多对一型<ul>
<li>union算子</li>
<li>cartesian算子</li>
</ul>
</li>
<li>输入分区与输出分区多对多型<ul>
<li>grouBy算子</li>
</ul>
</li>
<li>输出分区为输入分区子集型<ul>
<li>filter算子</li>
<li>distinct算子</li>
<li>subtract算子</li>
<li>sample算子</li>
<li>takeSample算子</li>
</ul>
</li>
<li>Cache型<ul>
<li>cache算子</li>
<li>persist算子</li>
</ul>
</li>
</ul>
<h4 id="2-Key-Value数据类型的Transfromation算子"><a href="#2-Key-Value数据类型的Transfromation算子" class="headerlink" title="2. Key-Value数据类型的Transfromation算子"></a>2. Key-Value数据类型的Transfromation算子</h4><ul>
<li>输入分区与输出分区一对一<ul>
<li>mapValues算子</li>
</ul>
</li>
<li>对单个RDD或两个RDD聚集<ul>
<li>combineByKey算子</li>
<li>reduceByKey算子</li>
<li>partitionBy算子</li>
<li>Cogroup算子</li>
</ul>
</li>
<li>连接<ul>
<li>join算子</li>
<li>leftOutJoin 和 rightOutJoin算子</li>
</ul>
</li>
</ul>
<h4 id="3-Action算子"><a href="#3-Action算子" class="headerlink" title="3. Action算子"></a>3. Action算子</h4><ul>
<li>无输出<ul>
<li>foreach算子</li>
</ul>
</li>
<li>HDFS算子<ul>
<li>saveAsTextFile算子</li>
<li>saveAsObjectFile算子</li>
</ul>
</li>
<li>Scala集合和数据类型<ul>
<li>collect算子</li>
<li>collectAsMap算子</li>
<li>reduceByKeyLocally算子</li>
<li>lookup算子</li>
<li>count算子</li>
<li>top算子</li>
<li>reduce算子</li>
<li>fold算子</li>
<li>aggregate算子</li>
<li>countByValue</li>
<li>countByKey</li>
</ul>
</li>
</ul>
<p><a href="https://www.cnblogs.com/kpsmile/p/10434390.html" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="RDD的缓存级别都有哪些？"><a href="#RDD的缓存级别都有哪些？" class="headerlink" title="RDD的缓存级别都有哪些？"></a>RDD的缓存级别都有哪些？</h3><p>NONE :什么类型都不是<br>DISK_ONLY：磁盘<br>DISK_ONLY_2：磁盘；双副本<br>MEMORY_ONLY： 内存；反序列化；把RDD作为反序列化的方式存储，假如RDD的内容存不下，剩余的分区在以后需要时会重新计算，不会刷到磁盘上。<br>MEMORY_ONLY_2：内存；反序列化；双副本<br>MEMORY_ONLY_SER：内存；序列化；这种序列化方式，每一个partition以字节数据存储，好处是能带来更好的空间存储，但CPU耗费高<br>MEMORY_ONLY_SER_2 : 内存；序列化；双副本<br>MEMORY_AND_DISK：内存 + 磁盘；反序列化；双副本；RDD以反序列化的方式存内存，假如RDD的内容存不下，剩余的会存到磁盘<br>MEMORY_AND_DISK_2 : 内存 + 磁盘；反序列化；双副本<br>MEMORY_AND_DISK_SER：内存 + 磁盘；序列化<br>MEMORY_AND_DISK_SER_2：内存 + 磁盘；序列化；双副本</p>
<h3 id="RDD-懒加载？"><a href="#RDD-懒加载？" class="headerlink" title="RDD 懒加载？"></a>RDD 懒加载？</h3><p>Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Acion 操作的时候才会真正触发运算,这也就是懒加载.</p>
<h3 id="spark的几种部署方式？"><a href="#spark的几种部署方式？" class="headerlink" title="spark的几种部署方式？"></a>spark的几种部署方式？</h3><p><strong>目前,除了local模式为本地调试模式以为, Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN</strong></p>
<ul>
<li><p><strong>Standalone模式</strong></p>
<p>即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的： </p>
<ul>
<li>都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）； </li>
<li>各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。 </li>
</ul>
</li>
<li><p><strong>Spark On YARN模式</strong></p>
<p><strong>spark on yarn 的支持两种模式：</strong> </p>
<ul>
<li>yarn-cluster：适用于生产环境； </li>
<li>yarn-client：适用于交互、调试，希望立即看到app的输出 </li>
</ul>
<p>yarn-cluster和yarn-client的区别在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。</p>
</li>
<li><p><strong>Spark On Mesos模式</strong></p>
<p>Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序</p>
<ul>
<li>粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</li>
<li>细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。</li>
</ul>
</li>
</ul>
<h3 id="spark-on-yarn-模式下的-cluster模式和-client模式有什么区别？"><a href="#spark-on-yarn-模式下的-cluster模式和-client模式有什么区别？" class="headerlink" title="spark on yarn 模式下的 cluster模式和 client模式有什么区别？"></a>spark on yarn 模式下的 cluster模式和 client模式有什么区别？</h3><ol>
<li>yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出.</li>
<li>yarn-cluster 和 yarn-client 模式的区别其实就是 <strong>Application Master 进程</strong>的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的container 通信来调度他们工作，也就是说 Client 不能离开。</li>
</ol>
<h3 id="spark运行原理-从提交一个jar到最后返回结果-整个过程？"><a href="#spark运行原理-从提交一个jar到最后返回结果-整个过程？" class="headerlink" title="spark运行原理,从提交一个jar到最后返回结果,整个过程？"></a>spark运行原理,从提交一个jar到最后返回结果,整个过程？</h3><ol>
<li><code>spark-submit</code> 提交代码，执行 <code>new SparkContext()</code>，在 SparkContext 里构造 <code>DAGScheduler</code> 和 <code>TaskScheduler</code>。</li>
<li>TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。</li>
<li>Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。</li>
<li>Executor 启动后，会自己反向注册到 TaskScheduler 中。 所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。</li>
<li>每执行到一个 Action，就会创建一个 Job。Job 会提交给 DAGScheduler。</li>
<li>DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。</li>
<li>TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。</li>
<li>Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。(TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition)</li>
</ol>
<h3 id="spark的stage是如何划分的？"><a href="#spark的stage是如何划分的？" class="headerlink" title="spark的stage是如何划分的？"></a>spark的stage是如何划分的？</h3><p><strong>stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage.</strong></p>
<p><img src="/2019/11/24/大数据相关/stageDivide.jpg" alt=""></p>
<h3 id="spark的rpc-spark2-0为什么放弃了akka-而用netty？"><a href="#spark的rpc-spark2-0为什么放弃了akka-而用netty？" class="headerlink" title="spark的rpc: spark2.0为什么放弃了akka 而用netty？"></a>spark的rpc: spark2.0为什么放弃了akka 而用netty？</h3><ol>
<li>很多Spark用户也使用Akka，但是由于Akka不同版本之间无法互相通信，这就要求用户必须使用跟Spark完全一样的Akka版本，导致用户无法升级Akka。</li>
<li>Spark的Akka配置是针对Spark自身来调优的，可能跟用户自己代码中的Akka配置冲突。</li>
<li>Spark用的Akka特性很少，这部分特性很容易自己实现。同时，这部分代码量相比Akka来说少很多，debug比较容易。如果遇到什么bug，也可以自己马上fix，不需要等Akka上游发布新版本。而且，Spark升级Akka本身又因为第一点会强制要求用户升级他们使用的Akka，对于某些用户来说是不现实的。</li>
</ol>
<p><a href="https://www.zhihu.com/question/61638635" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="spark的各种HA"><a href="#spark的各种HA" class="headerlink" title="spark的各种HA?"></a>spark的各种HA?</h3><p>master/worker/executor/driver/task的ha</p>
<ul>
<li><h4 id="Master异常"><a href="#Master异常" class="headerlink" title="Master异常"></a>Master异常</h4><p>spark可以在集群运行时启动一个或多个standby Master,当 Master 出现异常时,会根据规则启动某个standby master接管,在standlone模式下有如下几种配置</p>
<ul>
<li><p>ZOOKEEPER</p>
<p>集群数据持久化到zk中,当master出现异常时,zk通过选举机制选出新的master,新的master接管是需要从zk获取持久化信息</p>
</li>
<li><p>FILESYSTEM</p>
<p>集群元数据信息持久化到本地文件系统, 当master出现异常时,只需要在该机器上重新启动master,启动后新的master获取持久化信息并根据这些信息恢复集群状态</p>
</li>
<li><p>CUSTOM</p>
<p>自定义恢复方式,对 standloneRecoveryModeFactory 抽象类 进行实现并把该类配置到系统中,当master出现异常时,会根据用户自定义行为恢复集群</p>
</li>
<li><p>None</p>
<p>不持久化集群的元数据, 当 master出现异常时, 新启动的Master 不进行恢复集群状态,而是直接接管集群</p>
</li>
</ul>
</li>
<li><h4 id="Worker异常"><a href="#Worker异常" class="headerlink" title="Worker异常"></a>Worker异常</h4><p>Worker 以定时发送心跳给 Master, 让 Master 知道 Worker 的实时状态,当worker出现超时时,Master 调用 timeOutDeadWorker 方法进行处理,在处理时根据 Worker 运行的是 Executor 和 Driver 分别进行处理</p>
<ul>
<li>如果是Executor, Master先把该 Worker 上运行的Executor 发送信息ExecutorUpdate给对应的Driver,告知Executor已经丢失,同时把这些Executor从其应用程序列表删除, 另外, 相关Executor的异常也需要处理</li>
<li>如果是Driver, 则判断是否设置重新启动,如果需要,则调用Master.shedule方法进行调度,分配合适节点重启Driver, 如果不需要重启, 则删除该应用程序</li>
</ul>
</li>
<li><h4 id="Executor异常"><a href="#Executor异常" class="headerlink" title="Executor异常"></a>Executor异常</h4><ol>
<li>Executor发生异常时由ExecutorRunner捕获该异常并发送ExecutorStateChanged信息给Worker</li>
<li>Worker接收到消息时, 在Worker的 handleExecutorStateChanged 方法中, 根据Executor状态进行信息更新,同时把Executor状态发送给Master</li>
<li>Master在接受Executor状态变化消息之后,如果发现其是异常退出,会尝试可用的Worker节点去启动Executor</li>
</ol>
</li>
</ul>
<h3 id="spark的内存管理机制-spark-1-6前后分析对比-spark2-0-做出来哪些优化？"><a href="#spark的内存管理机制-spark-1-6前后分析对比-spark2-0-做出来哪些优化？" class="headerlink" title="spark的内存管理机制,spark 1.6前后分析对比, spark2.0 做出来哪些优化？"></a>spark的内存管理机制,spark 1.6前后分析对比, spark2.0 做出来哪些优化？</h3><p><strong>spark的内存结构分为3大块:storage/execution/系统自留</strong></p>
<ul>
<li><p><strong>storage 内存</strong>：用于缓存 RDD、展开 partition、存放 Direct Task Result、存放广播变量。在 Spark Streaming receiver 模式中，也用来存放每个 batch 的 blocks</p>
</li>
<li><p><strong>execution 内存</strong>：用于 shuffle、join、sort、aggregation 中的缓存、buffer</p>
</li>
<li><p><strong>系统自留</strong>:</p>
<ul>
<li>在 spark 运行过程中使用：比如序列化及反序列化使用的内存，各个对象、元数据、临时变量使用的内存，函数调用使用的堆栈等</li>
<li>作为误差缓冲：由于 storage 和 execution 中有很多内存的使用是估算的，存在误差。当 storage 或 execution 内存使用超出其最大限制时，有这样一个安全的误差缓冲在可以大大减小 OOM 的概率</li>
</ul>
</li>
</ul>
<hr>
<h4 id="1-6版本以前的问题"><a href="#1-6版本以前的问题" class="headerlink" title="1.6版本以前的问题"></a>1.6版本以前的问题</h4><ul>
<li>旧方案最大的问题是 storage 和 execution 的内存大小都是固定的，不可改变，即使 execution 有大量的空闲内存且 storage 内存不足，storage 也无法使用 execution 的内存，只能进行 spill，反之亦然。所以，在很多情况下存在资源浪费</li>
<li>旧方案中，只有 execution 内存支持 off heap，storage 内存不支持 off heap</li>
</ul>
<h4 id="新方案的改进"><a href="#新方案的改进" class="headerlink" title="新方案的改进"></a>新方案的改进</h4><ul>
<li>新方案 storage 和 execution 内存可以互相借用，当一方内存不足可以向另一方借用内存，提高了整体的资源利用率</li>
<li>新方案中 execution 内存和 storage 内存均支持 off heap</li>
</ul>
<h3 id="spark-中的广播变量？"><a href="#spark-中的广播变量？" class="headerlink" title="spark 中的广播变量？"></a>spark 中的广播变量？</h3><p><a href="https://www.jianshu.com/p/6ef7f0a44fbf" target="_blank" rel="noopener">图片来源</a> /<a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/7-Broadcast.md" target="_blank" rel="noopener">文字来源</a></p>
<p><img src="/2019/11/24/大数据相关/spark中的广播变量.png" alt=""></p>
<p><strong>顾名思义，broadcast 就是将数据从一个节点发送到其他各个节点上去。这样的场景很多，比如 driver 上有一张表，其他节点上运行的 task 需要 lookup 这张表，那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。如何实现一个可靠高效的 broadcast 机制是一个有挑战性的问题。先看看 Spark 官网上的一段话：</strong></p>
<p>Broadcast variables allow the programmer to keep a <strong>read-only</strong> variable cached on each <strong>machine</strong> rather than shipping a copy of it with <strong>tasks</strong>. They can be used, for example, to give every node a copy of a <strong>large input dataset</strong> in an efficient manner. Spark also attempts to distribute broadcast variables using <strong>efficient</strong> broadcast algorithms to reduce communication cost.</p>
<h4 id="问题：为什么只能-broadcast-只读的变量？"><a href="#问题：为什么只能-broadcast-只读的变量？" class="headerlink" title="问题：为什么只能 broadcast 只读的变量？"></a>问题：为什么只能 broadcast 只读的变量？</h4><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？还会涉及 fault-tolerance 的问题。为了避免维护数据一致性问题，Spark 目前只支持 broadcast 只读变量。</p>
<h4 id="问题：broadcast-到节点而不是-broadcast-到每个-task？"><a href="#问题：broadcast-到节点而不是-broadcast-到每个-task？" class="headerlink" title="问题：broadcast 到节点而不是 broadcast 到每个 task？"></a>问题：broadcast 到节点而不是 broadcast 到每个 task？</h4><p>因为每个 task 是一个线程，而且同在一个进程运行 tasks 都属于同一个 application。因此每个节点（executor）上放一份就可以被所有 task 共享。</p>
<h4 id="问题：-具体怎么用-broadcast？"><a href="#问题：-具体怎么用-broadcast？" class="headerlink" title="问题： 具体怎么用 broadcast？"></a>问题： 具体怎么用 broadcast？</h4><p>driver program 例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val data = List(1, 2, 3, 4, 5, 6)</span><br><span class="line">val bdata = sc.broadcast(data)</span><br><span class="line"></span><br><span class="line">val rdd = sc.parallelize(1 to 6, 2)</span><br><span class="line">val observedSizes = rdd.map(_ =&gt; bdata.value.size)</span><br></pre></td></tr></table></figure>
<p>driver 使用 <code>sc.broadcast()</code> 声明要 broadcast 的 data，bdata 的类型是 Broadcast。</p>
<p>当 <code>rdd.transformation(func)</code> 需要用 bdata 时，直接在 func 中调用，比如上面的例子中的 map() 就使用了 bdata.value.size。</p>
<h4 id="问题：怎么实现-broadcast？"><a href="#问题：怎么实现-broadcast？" class="headerlink" title="问题：怎么实现 broadcast？"></a>问题：怎么实现 broadcast？</h4><p>broadcast 的实现机制很有意思：</p>
<ol>
<li>分发 task 的时候先分发 bdata 的元信息</li>
</ol>
<p>Driver 先建一个本地文件夹用以存放需要 broadcast 的 data，并启动一个可以访问该文件夹的 HttpServer。当调用<code>val bdata = sc.broadcast(data)</code>时就把 data 写入文件夹，同时写入 driver 自己的 blockManger 中（StorageLevel 为内存＋磁盘），获得一个 blockId，类型为 BroadcastBlockId。当调用<code>rdd.transformation(func)</code>时，如果 func 用到了 bdata，那么 driver submitTask() 的时候会将 bdata 一同 func 进行序列化得到 serialized task，<strong>注意序列化的时候不会序列化 bdata 中包含的 data。</strong>上一章讲到 serialized task 从 driverActor 传递到 executor 时使用 Akka 的传消息机制，消息不能太大，而实际的 data 可能很大，所以这时候还不能 broadcast data。</p>
<blockquote>
<p>driver 为什么会同时将 data 放到磁盘和 blockManager 里面？放到磁盘是为了让 HttpServer 访问到，放到 blockManager 是为了让 driver program 自身使用 bdata 时方便（其实我觉得不放到 blockManger 里面也行）。</p>
</blockquote>
<p><strong>那么什么时候传送真正的 data？</strong>在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，如果不在就使用下面的两种 fetch 方式之一去将 data fetch 过来。得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。</p>
<p>下面探讨 broadcast data 时候的两种实现方式：</p>
<ol>
<li>HttpBroadcast</li>
</ol>
<p>顾名思义，HttpBroadcast 就是每个 executor 通过的 http 协议连接 driver 并从 driver 那里 fetch data。</p>
<p>Driver 先准备好要 broadcast 的 data，调用<code>sc.broadcast(data)</code>后会调用工厂方法建立一个 HttpBroadcast 对象。该对象做的第一件事就是将 data 存到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘，blockId 类型为 BroadcastBlockId。</p>
<p>同时 driver 也会将 broadcast 的 data 写到本地磁盘，例如写入后得到 <code>/var/folders/87/grpn1_fn4xq5wdqmxk31v0l00000gp/T/spark-6233b09c-3c72-4a4d-832b-6c0791d0eb9c/broadcast_0</code>， 这个文件夹作为 HttpServer 的文件目录。</p>
<blockquote>
<p>Driver 和 executor 启动的时候，都会生成 broadcastManager 对象，调用 HttpBroadcast.initialize()，driver 会在本地建立一个临时目录用来存放 broadcast 的 data，并启动可以访问该目录的 httpServer。</p>
</blockquote>
<p><strong>Fetch data：</strong>在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，<strong>如果不在就使用 http 协议连接 driver 上的 httpServer，将 data fetch 过来。</strong>得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。</p>
<p>HttpBroadcast 最大的问题就是 <strong>driver 所在的节点可能会出现网络拥堵</strong>，因为 worker 上的 executor 都会去 driver 那里 fetch 数据。</p>
<ol>
<li>TorrentBroadcast</li>
</ol>
<p>为了解决 HttpBroadast 中 driver 单点网络瓶颈的问题，Spark 又设计了一种 broadcast 的方法称为 TorrentBroadcast，<strong>这个类似于大家常用的 BitTorrent 技术。</strong>基本思想就是将 data 分块成 data blocks，然后假设有 executor fetch 到了一些 data blocks，那么这个 executor 就可以被当作 data server 了，随着 fetch 的 executor 越来越多，有更多的 data server 加入，data 就很快能传播到全部的 executor 那里去了。</p>
<p>HttpBroadcast 是通过传统的 http 协议和 httpServer 去传 data，在 TorrentBroadcast 里面使用在上一章介绍的 blockManager.getRemote() =&gt; NIO ConnectionManager 传数据的方法来传递，读取数据的过程与读取 cached rdd 的方式类似，可以参阅 <a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md" target="_blank" rel="noopener">CacheAndCheckpoint</a> 中的最后一张图。</p>
<p>下面讨论 TorrentBroadcast 的一些细节：</p>
<p><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/PNGfigures/TorrentBroadcast.png" target="_blank" rel="noopener"><img src="https://github.com/JerryLead/SparkInternals/raw/master/markdown/PNGfigures/TorrentBroadcast.png" alt="TorrentBroadcast"></a></p>
<p>driver 端：</p>
<p>Driver 先把 data 序列化到 byteArray，然后切割成 BLOCK_SIZE（由 <code>spark.broadcast.blockSize = 4MB</code> 设置）大小的 data block，每个 data block 被 TorrentBlock 对象持有。切割完 byteArray 后，会将其回收，因此内存消耗虽然可以达到 2 * Size(data)，但这是暂时的。</p>
<p>完成分块切割后，就将分块信息（称为 meta 信息）存放到 driver 自己的 blockManager 里面，StorageLevel 为内存＋磁盘，同时会通知 driver 自己的 blockManagerMaster 说 meta 信息已经存放好。<strong>通知 blockManagerMaster 这一步很重要，因为 blockManagerMaster 可以被 driver 和所有 executor 访问到，信息被存放到 blockManagerMaster 就变成了全局信息。</strong></p>
<p>之后将每个分块 data block 存放到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘。存放后仍然通知 blockManagerMaster 说 blocks 已经存放好。到这一步，driver 的任务已经完成。</p>
<p>Executor 端：</p>
<p>executor 收到 serialized task 后，先反序列化 task，这时候会反序列化 serialized task 中包含的 bdata 类型是 TorrentBroadcast，也就是去调用 TorrentBroadcast.readObject()。这个方法首先得到 bdata 对象，<strong>然后发现 bdata 里面没有包含实际的 data。怎么办？</strong>先询问所在的 executor 里的 blockManager 是会否包含 data（通过查询 data 的 broadcastId），包含就直接从本地 blockManager 读取 data。否则，就通过本地 blockManager 去连接 driver 的 blockManagerMaster 获取 data 分块的 meta 信息，获取信息后，就开始了 BT 过程。</p>
<p><strong>BT 过程：</strong>task 先在本地开一个数组用于存放将要 fetch 过来的 data blocks <code>arrayOfBlocks = new Array[TorrentBlock](totalBlocks)</code>，TorrentBlock 是对 data block 的包装。然后打乱要 fetch 的 data blocks 的顺序，比如如果 data block 共有 5 个，那么打乱后的 fetch 顺序可能是 3-1-2-4-5。然后按照打乱后的顺序去 fetch 一个个 data block。fetch 的过程就是通过 “本地 blockManager －本地 connectionManager－driver/executor 的 connectionManager－driver/executor 的 blockManager－data” 得到 data，这个过程与 fetch cached rdd 类似。<strong>每 fetch 到一个 block 就将其存放到 executor 的 blockManager 里面，同时通知 driver 上的 blockManagerMaster 说该 data block 多了一个存储地址。</strong>这一步通知非常重要，意味着 blockManagerMaster 知道 data block 现在在 cluster 中有多份，下一个不同节点上的 task 再去 fetch 这个 data block 的时候，可以有两个选择了，而且会随机选择一个去 fetch。这个过程持续下去就是 BT 协议，随着下载的客户端越来越多，data block 服务器也越来越多，就变成 p2p下载了。关于 BT 协议，Wikipedia 上有一个<a href="http://zh.wikipedia.org/wiki/BitTorrent_(%E5%8D%8F%E8%AE%AE" target="_blank" rel="noopener">动画</a>)。</p>
<p>整个 fetch 过程结束后，task 会开一个大 Array[Byte]，大小为 data 的总大小，然后将 data block 都 copy 到这个 Array，然后对 Array 中 bytes 进行反序列化得到原始的 data，这个过程就是 driver 序列化 data 的反过程。</p>
<p>最后将 data 存放到 task 所在 executor 的 blockManager 里面，StorageLevel 为内存＋磁盘。显然，这时候 data 在 blockManager 里存了两份，不过等全部 executor 都 fetch 结束，存储 data blocks 那份可以删掉了。</p>
<h4 id="问题：broadcast-RDD-会怎样"><a href="#问题：broadcast-RDD-会怎样" class="headerlink" title="问题：broadcast RDD 会怎样?"></a>问题：broadcast RDD 会怎样?</h4><p><a href="http://weibo.com/u/1410938285" target="_blank" rel="noopener">@Andrew-Xia</a> 回答道：不会怎样，就是这个rdd在每个executor中实例化一份。</p>
<h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>公共数据的 broadcast 是很实用的功能，在 Hadoop 中使用 DistributedCache，比如常用的<code>-libjars</code>就是使用 DistributedCache 来将 task 依赖的 jars 分发到每个 task 的工作目录。不过分发前 DistributedCache 要先将文件上传到 HDFS。这种方式的主要问题是<strong>资源浪费</strong>，如果某个节点上要运行来自同一 job 的 4 个 mapper，那么公共数据会在该节点上存在 4 份（每个 task 的工作目录会有一份）。但是通过 HDFS 进行 broadcast 的好处在于<strong>单点瓶颈不明显</strong>，因为公共 data 首先被分成多个 block，然后不同的 block 存放在不同的节点。这样，只要所有的 task 不是同时去同一个节点 fetch 同一个 block，网络拥塞不会很严重。</p>
<p>对于 Spark 来讲，broadcast 时考虑的不仅是如何将公共 data 分发下去的问题，还要考虑如何让同一节点上的 task 共享 data。</p>
<p>对于第一个问题，Spark 设计了两种 broadcast 的方式，传统存在单点瓶颈问题的 HttpBroadcast，和类似 BT 方式的 TorrentBroadcast。HttpBroadcast 使用传统的 client-server 形式的 HttpServer 来传递真正的 data，而 TorrentBroadcast 使用 blockManager 自带的 NIO 通信方式来传递 data。TorrentBroadcast 存在的问题是<strong>慢启动</strong>和<strong>占内存</strong>，慢启动指的是刚开始 data 只在 driver 上有，要等 executors fetch 很多轮 data block 后，data server 才会变得可观，后面的 fetch 速度才会变快。executor 所占内存的在 fetch 完 data blocks 后进行反序列化时需要将近两倍 data size 的内存消耗。不管哪一种方式，driver 在分块时会有两倍 data size 的内存消耗。</p>
<p>对于第二个问题，每个 executor 都包含一个 blockManager 用来管理存放在 executor 里的数据，将公共数据存放在 blockManager 中（StorageLevel 为内存＋磁盘），可以保证在 executor 执行的 tasks 能够共享 data。</p>
<p>其实 Spark 之前还尝试了一种称为 TreeBroadcast 的机制，详情可以见技术报告 <a href="http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf" target="_blank" rel="noopener">Performance and Scalability of Broadcast in Spark</a>。</p>
<p>更深入点，broadcast 可以用多播协议来做，不过多播使用 UDP，不是可靠的，仍然需要应用层的设计一些可靠性保障机制。</p>
<h3 id="数据倾斜-怎样去处理数据倾斜？"><a href="#数据倾斜-怎样去处理数据倾斜？" class="headerlink" title="数据倾斜,怎样去处理数据倾斜？"></a>数据倾斜,怎样去处理数据倾斜？</h3><p>数据倾斜是一种很常见的问题（依据二八定律），简单来说，比方WordCount中某个Key对应的数据量非常大的话，就会产生数据倾斜，导致两个后果：</p>
<ul>
<li>OOM（单或少数的节点）；</li>
<li>拖慢整个Job执行时间（其他已经完成的节点都在等这个还在做的节点）</li>
</ul>
<h4 id="数据倾斜主要分为两类-聚合倾斜-和-join倾斜"><a href="#数据倾斜主要分为两类-聚合倾斜-和-join倾斜" class="headerlink" title="数据倾斜主要分为两类: 聚合倾斜 和 join倾斜"></a>数据倾斜主要分为两类: 聚合倾斜 和 join倾斜</h4><ul>
<li><p><strong>聚合倾斜</strong></p>
<ul>
<li><p><strong>双重聚合（局部聚合+全局聚合）</strong></p>
<p><strong>场景</strong>: 对RDD进行reduceByKey等聚合类shuffle算子，SparkSQL的groupBy做分组聚合这两种情况<br> 思路：首先通过map给每个key打上n以内的随机数的前缀并进行局部聚合，即(hello, 1) (hello, 1) (hello, 1) (hello, 1)变为(1_hello, 1) (1_hello, 1) (2_hello, 1)，并进行reduceByKey的局部聚合，然后再次map将key的前缀随机数去掉再次进行全局聚合；<br> <strong>原理</strong>: 对原本相同的key进行随机数附加，变成不同key，让原本一个task处理的数据分摊到多个task做局部聚合，规避单task数据过量。之后再去随机前缀进行全局聚合；<br> 优点：效果非常好（对聚合类Shuffle操作的倾斜问题）；<br> 缺点：范围窄（仅适用于聚合类的Shuffle操作，join类的Shuffle还需其它方案）</p>
</li>
</ul>
</li>
<li><p><strong>join倾斜</strong></p>
<ul>
<li><p><strong>将reduce join转为map join</strong></p>
<p><strong>场景</strong>: 对RDD或Spark SQL使用join类操作或语句，且join操作的RDD或表比较小（百兆或1,2G）； 思路：使用broadcast和map类算子实现join的功能替代原本的join，彻底规避shuffle。对较小RDD直接collect到内存，并创建broadcast变量；并对另外一个RDD执行map类算子，在该算子的函数中，从broadcast变量（collect出的较小RDD）与当前RDD中的每条数据依次比对key，相同的key执行你需要方式的join；</p>
<p><strong>原理</strong>: 若RDD较小，可采用广播小的RDD，并对大的RDD进行map，来实现与join同样的效果。简而言之，用broadcast-map代替join，规避join带来的shuffle（无Shuffle无倾斜）； 优点：效果很好（对join操作导致的倾斜），根治； </p>
<p><strong>缺点</strong>：适用场景小（大表+小表），广播（driver和executor节点都会驻留小表数据）小表也耗内存</p>
</li>
<li><p><strong>采样倾斜key并分拆join操作</strong></p>
<p><strong>场景</strong>: 两个较大的（无法采用方案五）RDD/Hive表进行join时，且一个RDD/Hive表中少数key数据量过大，另一个RDD/Hive表的key分布较均匀（RDD中两者之一有一个更倾斜）；<br><strong>思路</strong>:</p>
<ol>
<li>对更倾斜rdd1进行采样（RDD.sample）并统计出数据量最大的几个key；</li>
<li>对这几个倾斜的key从原本rdd1中拆出形成一个单独的rdd1_1，并打上0~n的随机数前缀，被拆分的原rdd1的另一部分（不包含倾斜key）又形成一个新rdd1_2；</li>
<li>对rdd2过滤出rdd1倾斜的key，得到rdd2_1，并将其中每条数据扩n倍，对每条数据按顺序附加0~n的前缀，被拆分出key的rdd2也独立形成另一个rdd2_2； 【个人认为，这里扩了n倍，最后union完还需要将每个倾斜key对应的value减去(n-1)】</li>
<li>将加了随机前缀的rdd1_1和rdd2_1进行join（此时原本倾斜的key被打散n份并被分散到更多的task中进行join）； 【个人认为，这里应该做两次join，两次join中间有一个map去前缀】</li>
<li>另外两个普通的RDD（rdd1_2、rdd2_2）照常join；</li>
<li>最后将两次join的结果用union结合得到最终的join结果。 原理：对join导致的倾斜是因为某几个key，可将原本RDD中的倾斜key拆分出原RDD得到新RDD，并以加随机前缀的方式打散n份做join，将倾斜key对应的大量数据分摊到更多task上来规避倾斜；</li>
</ol>
<p><strong>优点</strong>: 前提是join导致的倾斜（某几个key倾斜），避免占用过多内存（只需对少数倾斜key扩容n倍）；<br><strong>缺点</strong>: 对过多倾斜key不适用。</p>
</li>
<li><p><strong>用随机前缀和扩容RDD进行join</strong></p>
<p><strong>场景</strong>: RDD中有大量key导致倾斜； 思路：与方案六类似。</p>
<ol>
<li>查看RDD/Hive表中数据分布并找到造成倾斜的RDD/表；</li>
<li>对倾斜RDD中的每条数据打上n以内的随机数前缀；</li>
<li>对另外一个正常RDD的每条数据扩容n倍，扩容出的每条数据依次打上0到n的前缀；</li>
<li>对处理后的两个RDD进行join。</li>
</ol>
<p><strong>原理</strong>: 与方案六只有唯一不同在于这里对不倾斜RDD中所有数据进行扩大n倍，而不是找出倾斜key进行扩容；<br><strong>优点</strong>: 对join类的数据倾斜都可处理，效果非常显著；<br><strong>缺点</strong>: 缓解，扩容需要大内存</p>
</li>
</ul>
</li>
</ul>
<p><a href="https://juejin.im/post/5ccd5cc7f265da03474e1249#heading-9" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://blog.csdn.net/qq_35394891/article/details/82260907" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="分析一下一段spark代码中哪些部分在Driver端执行-哪些部分在Worker端执行？"><a href="#分析一下一段spark代码中哪些部分在Driver端执行-哪些部分在Worker端执行？" class="headerlink" title="分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行？"></a>分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行？</h3><p>Driver Program是用户编写的提交给Spark集群执行的application，它包含两部分</p>
<ul>
<li><strong>作为驱动</strong>： Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、计算任务分发到各个计算节点(Worker)、计算资源的分配等。</li>
<li><strong>计算逻辑本身</strong>，当计算任务在Worker执行时，执行计算逻辑完成application的计算任务</li>
</ul>
<p>一般来说transformation算子均是在worker上执行的,其他类型的代码在driver端执行</p>
<h3 id="Spark经典程序"><a href="#Spark经典程序" class="headerlink" title="Spark经典程序"></a>Spark经典程序</h3><h4 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package yun.mao</span><br><span class="line"></span><br><span class="line">import org.apache.spark._</span><br><span class="line">import org.apache.spark.SparkContext._</span><br><span class="line"></span><br><span class="line">object WordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val inputFile = args(0)</span><br><span class="line">    val outputFile = args(1)</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;wordCount&quot;)</span><br><span class="line">    // Create a Scala Spark Context.</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    // Load our input data.</span><br><span class="line">    val input =  sc.textFile(inputFile)</span><br><span class="line">    // Split up into words.</span><br><span class="line">    val words = input.flatMap(_.split(&quot; &quot;))</span><br><span class="line">    // Transform into word and count.</span><br><span class="line">    val counts = words.map((_, 1)).reduceByKey(_+_)</span><br><span class="line">    // Save the word count back out to a text file, causing evaluation.</span><br><span class="line">    counts.saveAsTextFile(outputFile)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="pagerank"><a href="#pagerank" class="headerlink" title="pagerank"></a>pagerank</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">package yun.mao</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * @Classname PageRank</span><br><span class="line">  * @Description TODO</span><br><span class="line">  * @Date 19-3-30 下午12:41</span><br><span class="line">  * @Created by mao&lt;tianmao818@qq.com&gt;</span><br><span class="line">  */</span><br><span class="line">import org.apache.spark.&#123;HashPartitioner, SparkConf, SparkContext&#125;</span><br><span class="line">object PageRank &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;page rank&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val iters=10</span><br><span class="line">    val lines=sc.textFile(args(0))</span><br><span class="line">    val links =lines.map&#123;s=&gt;</span><br><span class="line">      val parts=s.split(&quot;\\s+&quot;)</span><br><span class="line">      (parts(0),parts(1))</span><br><span class="line">    &#125;.distinct().groupByKey().cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    var ranks=links.mapValues(v=&gt;1.0)</span><br><span class="line">    /*</span><br><span class="line">    * 地址1---&gt;地址2   (指向链接2的越多,链接2 rank越高)</span><br><span class="line">    * (4,1.0)</span><br><span class="line">    * (2,1.0)</span><br><span class="line">    * (3,1.0)</span><br><span class="line">    * (1,1.0)</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    ranks.foreach(println)</span><br><span class="line"></span><br><span class="line">    val tmp=links.join(ranks)</span><br><span class="line"></span><br><span class="line">    tmp.foreach(println)</span><br><span class="line">    /*</span><br><span class="line">    * (3,(CompactBuffer(1),1.0))</span><br><span class="line">    * (1,(CompactBuffer(3, 2, 4),1.0))</span><br><span class="line">    * (4,(CompactBuffer(1),1.0))</span><br><span class="line">    * (2,(CompactBuffer(1),1.0))</span><br><span class="line">    * */</span><br><span class="line">    println(&quot;----------------values-------------&quot;)</span><br><span class="line">    tmp.values.foreach(println)</span><br><span class="line">    /*</span><br><span class="line">    * (CompactBuffer(1),1.0)</span><br><span class="line">    * (CompactBuffer(3, 2, 4),1.0)</span><br><span class="line">    * (CompactBuffer(1),1.0)</span><br><span class="line">    * (CompactBuffer(1),1.0)</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    for(i &lt;-1 to iters)&#123;</span><br><span class="line">      //对urls中的url分别计算从当前的节点获取了多少权重</span><br><span class="line">      val contribs=links.join(ranks).values.flatMap&#123;case(urls,rank)=&gt;</span><br><span class="line">        val size=urls.size</span><br><span class="line">        urls.map(url=&gt;(url,rank/size))</span><br><span class="line">      &#125;</span><br><span class="line">      println(s&quot;--------$&#123;i&#125;-----------&quot;)</span><br><span class="line">      contribs.foreach(println)</span><br><span class="line">      ranks=contribs.reduceByKey(_+_).mapValues(0.15+0.85*_)</span><br><span class="line">    &#125;</span><br><span class="line">    val output=ranks.collect()</span><br><span class="line">    output.foreach(tup=&gt;println(s&quot;$&#123;tup._1&#125; has rank: $&#123;tup._2&#125;&quot;))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><ul>
<li>TF-IDF介绍:</li>
</ul>
<p>实质在于统计词汇在当前文档中的频率和在所有文档中的频率,在当前的文档中出现的频率越高重要性越高,在所有的文档中出现的频率越高重要性越低.它可以体现一个文档中词语在语料库中的重要程度。</p>
<ul>
<li>使用20 Newsgroups数据集</li>
</ul>
<p>(<a href="http://qwone.com/~jason/20Newsgroups/" target="_blank" rel="noopener">http://qwone.com/~jason/20Newsgroups/</a>)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">package yun.mao</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * @Classname DocumentClassification</span><br><span class="line">  * @Description TODO</span><br><span class="line">  * @Date 19-3-30 下午5:19</span><br><span class="line">  * @Created by mao&lt;tianmao818@qq.com&gt;</span><br><span class="line">  */</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.apache.spark.mllib.classification.NaiveBayes</span><br><span class="line">import org.apache.spark.mllib.evaluation.MulticlassMetrics</span><br><span class="line">import org.apache.spark.mllib.feature.&#123;HashingTF, IDF&#125;</span><br><span class="line">import org.apache.spark.mllib.linalg.SparseVector</span><br><span class="line">import org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line">import org.apache.spark.mllib.linalg.&#123;SparseVector =&gt; SV&#125;</span><br><span class="line">import org.apache.spark.mllib.util.MLUtils</span><br><span class="line">//import org.apache.spark.ml.feature.HashingTF</span><br><span class="line">//import org.apache.spark.ml.feature.IDF</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * A simple Spark app in Scala</span><br><span class="line">  */</span><br><span class="line">object DocumentClassification &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;TF-IDF Document classification&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val path = args(0)</span><br><span class="line">    //训练数据</span><br><span class="line">    val rdd = sc.wholeTextFiles(path)</span><br><span class="line">    val text = rdd.map &#123; case (file, text) =&gt; text &#125;</span><br><span class="line">    val newsgroups = rdd.map &#123; case (file, text) =&gt; file.split(&quot;/&quot;).takeRight(2).head &#125;</span><br><span class="line">    val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap</span><br><span class="line">    val dim = math.pow(2, 18).toInt</span><br><span class="line"></span><br><span class="line">    val hashingTF = new HashingTF(dim)</span><br><span class="line"></span><br><span class="line">    var tokens = text.map(doc =&gt; TFIDFExtraction.tokenize(doc))</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    * HashingTF.transform()的输入/输出?</span><br><span class="line">    * 输入:</span><br><span class="line">    * 输出:多个[65,618,852,992,1194],[1.0,1.0,1.0,1.0,1.0])],前半部分是单词的hash值,后半部分是对应的频率</span><br><span class="line">    * 每一个文档对应这这样的一个记录</span><br><span class="line">    * */</span><br><span class="line">    val tf = hashingTF.transform(tokens)</span><br><span class="line">    tf.cache</span><br><span class="line">    val v = tf.first.asInstanceOf[SV]</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    * IDF().fit()的输入/输出?</span><br><span class="line">    * 输入:</span><br><span class="line">    * 输出:</span><br><span class="line">    * */</span><br><span class="line">    val idf = new IDF().fit(tf)</span><br><span class="line">    /*</span><br><span class="line">    * IDF().fit(tf).transform(tf)的输入输出?</span><br><span class="line">    * 输入:</span><br><span class="line">    * 输出:每一项计算的值[65,618,852,992,1194],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453]</span><br><span class="line">    * 输出前半部分是单词的hash值,后半部分是对应的tfidf数值,输出是所有的文档集合,一个文档有一个这样的记录</span><br><span class="line">    * */</span><br><span class="line">    val tfidf = idf.transform(tf)</span><br><span class="line"></span><br><span class="line">    //zip()将两个序列组织成为字典的形式!</span><br><span class="line">    val zipped = newsgroups.zip(tfidf)</span><br><span class="line">    println(zipped.first())</span><br><span class="line">    val train = zipped.map &#123; case (topic, vector) =&gt; &#123;</span><br><span class="line">      LabeledPoint(newsgroupsMap(topic), vector)</span><br><span class="line">    &#125; &#125;</span><br><span class="line"></span><br><span class="line">    //TODO uncomment to generate libsvm format</span><br><span class="line">    MLUtils.saveAsLibSVMFile(train,&quot;./output/20news-by-date-train-libsvm&quot;)</span><br><span class="line"></span><br><span class="line">    train.cache</span><br><span class="line">    val model = NaiveBayes.train(train, lambda = 0.1)</span><br><span class="line"></span><br><span class="line">    //测试数据</span><br><span class="line">    val testPath = args(1)</span><br><span class="line">    val testRDD = sc.wholeTextFiles(testPath)</span><br><span class="line">    val testLabels = testRDD.map &#123; case (file, text) =&gt;</span><br><span class="line">      val topic = file.split(&quot;/&quot;).takeRight(2).head</span><br><span class="line">      newsgroupsMap(topic)</span><br><span class="line">    &#125;</span><br><span class="line">    val testTf = testRDD.map &#123; case (file, text) =&gt; hashingTF.transform(TFIDFExtraction.tokenize(text)) &#125;</span><br><span class="line">    //idf利用的是已经训练好的</span><br><span class="line">    val testTfIdf = idf.transform(testTf)</span><br><span class="line">    val zippedTest = testLabels.zip(testTfIdf)</span><br><span class="line">    val test = zippedTest.map &#123; case (topic, vector) =&gt; &#123;</span><br><span class="line">      println(topic)</span><br><span class="line">      println(vector)</span><br><span class="line">      LabeledPoint(topic, vector)</span><br><span class="line">    &#125; &#125;</span><br><span class="line"></span><br><span class="line">    //支持向量机</span><br><span class="line">    MLUtils.saveAsLibSVMFile(test,&quot;./output/20news-by-date-test-libsvm&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))</span><br><span class="line">    val accuracy = 1.0 * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / test.count()</span><br><span class="line">    println(accuracy)</span><br><span class="line">    // Updated Dec 2016 by Rajdeep</span><br><span class="line">    //0.7928836962294211</span><br><span class="line">    val metrics = new MulticlassMetrics(predictionAndLabel)</span><br><span class="line">    println(metrics.accuracy)</span><br><span class="line">    println(metrics.weightedFalsePositiveRate)</span><br><span class="line">    println(metrics.weightedPrecision)</span><br><span class="line">    println(metrics.weightedFMeasure)</span><br><span class="line">    println(metrics.weightedRecall)</span><br><span class="line">    //0.7822644376431702</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //朴素贝叶斯</span><br><span class="line">    val rawTokens = rdd.map &#123; case (file, text) =&gt; text.split(&quot; &quot;) &#125;</span><br><span class="line">    val rawTF = rawTokens.map(doc =&gt; hashingTF.transform(doc))</span><br><span class="line">    val rawTrain = newsgroups.zip(rawTF).map &#123; case (topic, vector) =&gt; LabeledPoint(newsgroupsMap(topic), vector) &#125;</span><br><span class="line">    val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)</span><br><span class="line">    val rawTestTF = testRDD.map &#123; case (file, text) =&gt; hashingTF.transform(text.split(&quot; &quot;)) &#125;</span><br><span class="line">    val rawZippedTest = testLabels.zip(rawTestTF)</span><br><span class="line">    val rawTest = rawZippedTest.map &#123; case (topic, vector) =&gt; LabeledPoint(topic, vector) &#125;</span><br><span class="line">    val rawPredictionAndLabel = rawTest.map(p =&gt; (rawModel.predict(p.features), p.label))</span><br><span class="line">    val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x =&gt; x._1 == x._2).count() / rawTest.count()</span><br><span class="line">    println(rawAccuracy)</span><br><span class="line">    // 0.7661975570897503</span><br><span class="line">    val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)</span><br><span class="line">    println(rawMetrics.weightedFMeasure)</span><br><span class="line">    // older value 0.7628947184990661</span><br><span class="line">    // dec 2016 : 0.7653320418573546</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">object TFIDFExtraction &#123;</span><br><span class="line">  def tokenize(line: String): Seq[String] = &#123;</span><br><span class="line">    line.split(&quot;&quot;&quot;\W+&quot;&quot;&quot;)</span><br><span class="line">      .map(_.toLowerCase)</span><br><span class="line">      .filter(token =&gt; regex.pattern.matcher(token).matches)</span><br><span class="line">      .filterNot(token =&gt; stopwords.contains(token))</span><br><span class="line">      .filterNot(token =&gt; rareTokens.contains(token))</span><br><span class="line">      .filter(token =&gt; token.size &gt;= 2)</span><br><span class="line">      .toSeq</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><h3 id="讲一下flink的运行架构？"><a href="#讲一下flink的运行架构？" class="headerlink" title="讲一下flink的运行架构？"></a>讲一下flink的运行架构？</h3><p><img src="/2019/11/24/大数据相关/flink架构图.png" alt=""></p>
<p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p>
<ul>
<li><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li>
<li><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</li>
<li><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</li>
</ul>
<p><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="http://shiyanjun.cn/archives/1508.html" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="讲一下flink的作业执行流程？"><a href="#讲一下flink的作业执行流程？" class="headerlink" title="讲一下flink的作业执行流程？"></a>讲一下flink的作业执行流程？</h3><p> <img src="/2019/11/24/大数据相关/flinkRuntime.png" alt=""></p>
<p><strong>以yarn模式Per-job方式为例概述作业提交执行流程</strong></p>
<ol>
<li><p>当执行executor() 之后,会首先在本地client 中将代码转化为可以提交的 JobGraph</p>
<p>如果提交为Per-Job模式,则首先需要启动AM, client会首先向资源系统申请资源, 在yarn下即为申请container开启AM, 如果是Session模式的话则不需要这个步骤</p>
</li>
<li><p>Yarn分配资源, 开启AM</p>
</li>
<li><p>Client将Job提交给Dispatcher</p>
</li>
<li><p>Dispatcher 会开启一个新的 JobManager线程</p>
</li>
<li><p>JM 向Flink 自己的 Resourcemanager申请slot资源来执行任务</p>
</li>
<li><p>RM 向 Yarn申请资源来启动 TaskManger (Session模式跳过此步)</p>
</li>
<li><p>Yarn 分配 Container 来启动 taskManger (Session模式跳过此步)</p>
</li>
<li><p>Flink 的 RM 向 TM 申请 slot资源来启动 task</p>
</li>
<li><p>TM 将待分配的 slot 提供给 JM</p>
</li>
<li><p>JM 提交 task, TM 会启动新的线程来执行任务,开始启动后就可以通过 shuffle模块进行 task之间的数据交换</p>
</li>
</ol>
<p><a href="https://www.bilibili.com/video/av52394455?t=343" target="_blank" rel="noopener">参考视频</a></p>
<h3 id="flink具体是如何实现exactly-once-语义？"><a href="#flink具体是如何实现exactly-once-语义？" class="headerlink" title="flink具体是如何实现exactly once 语义？"></a>flink具体是如何实现exactly once 语义？</h3><p>在谈到 flink 所实现的 exactly-once语义时,主要是2个层面上的,首先 flink在0.9版本以后已经实现了基于state的内部一致性语义, 在1.4版本以后也可以实现端到端 Exactly-Once语义</p>
<ul>
<li><h4 id="状态-Exactly-Once"><a href="#状态-Exactly-Once" class="headerlink" title="状态 Exactly-Once"></a>状态 Exactly-Once</h4><p>Flink 提供 exactly-once 的状态（state）投递语义，这为有状态的（stateful）计算提供了准确性保证。也就是状态是不会重复使用的,有且仅有一次消费</p>
</li>
</ul>
<p><img src="/2019/11/24/大数据相关/flink故障恢复.png" alt=""></p>
<p>​    这里需要注意的一点是如何理解state语义的exactly-once,并不是说在flink中的所有事件均只会处理一次,而是所有的事件所影响生成的state只有作用一次.</p>
<p>​    在上图中, 假设每两条消息后出发一次checkPoint操作,持久化一次state. TaskManager 在 处理完 event c 之后被shutdown, 这时候当 JobManager重启task之后, TaskManager  会从 checkpoint 1 处恢复状态,重新执行流处理,也就是说 此时 event c 事件 的的确确是会被再一次处理的. 那么 这里所说的一致性语义是何意思呢? 本身,flink每处理完一条数据都会记录当前进度到 state中, 也就是说在 故障前, 处理完 event c 这件事情已经记录到了state中,但是,由于在checkPoint 2 之前, 就已经发生了宕机,那么 event c 对于state的影响并没有被记录下来,对于整个flink内部系统来说就好像没有发生过一样, 在 故障恢复后, 当触发 checkpoint 2 时, event c 的 state才最终被保存下来. <strong>所以说,可以这样理解, 进入flink 系统中的 事件 永远只会被 一次state记录并checkpoint下来,而state是永远不会发生重复被消费的, 这也就是 flink内部的一致性语义,就叫做 状态 Exactly once.</strong></p>
<ul>
<li><h4 id="端到端（end-to-end）Exactly-Once"><a href="#端到端（end-to-end）Exactly-Once" class="headerlink" title="端到端（end-to-end）Exactly-Once"></a>端到端（end-to-end）Exactly-Once</h4></li>
</ul>
<p>2017年12月份发布的Apache Flink 1.4版本，引进了一个重要的特性：TwoPhaseCommitSinkFunction.，它抽取了两阶段提交协议的公共部分，使得构建端到端Excatly-Once的Flink程序变为了可能。这些外部系统包括Kafka0.11及以上的版本，以及一些其他的数据输入（data sources）和数据接收(data sink)。它提供了一个抽象层，需要用户自己手动去实现Exactly-Once语义.</p>
<p>为了提供端到端Exactly-Once语义，除了Flink应用程序本身的状态，Flink写入的外部存储也需要满足这个语义。也就是说，这些外部系统必须提供提交或者回滚的方法，然后通过Flink的checkpoint来协调</p>
<p><a href="https://www.whitewood.me/2018/10/16/Flink-Exactly-Once-%E6%8A%95%E9%80%92%E5%AE%9E%E7%8E%B0%E6%B5%85%E6%9E%90/" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://my.oschina.net/u/992559/blog/1819948" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="flink-的-window-实现机制？"><a href="#flink-的-window-实现机制？" class="headerlink" title="flink 的 window 实现机制？"></a>flink 的 window 实现机制？</h3><p>Flink 中定义一个窗口主要需要以下三个组件。</p>
<ul>
<li><strong>Window Assigner：</strong>用来决定某个元素被分配到哪个/哪些窗口中去。</li>
<li><strong>Trigger：</strong>触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。</li>
<li><strong>Evictor：</strong>可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。</li>
</ul>
<h4 id="Window-的实现"><a href="#Window-的实现" class="headerlink" title="Window 的实现"></a>Window 的实现</h4><p><img src="/2019/11/24/大数据相关/flink中window的实现.png" alt=""></p>
<p>首先上图中的组件都位于一个算子（window operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给 WindowAssigner。WindowAssigner 会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，<code>Window</code>本身只是一个ID标识符，其内部可能存储了一些元数据，如<code>TimeWindow</code>中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在 Key/Value State 中，key为<code>Window</code>，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了 Flink 的 State 机制（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/state.html" target="_blank" rel="noopener">state 文档</a>）。</p>
<p>每一个窗口都拥有一个属于自己的 Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者 fire + purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被 purge 了。在purge之前，窗口会一直占用着内存。</p>
<p>当Trigger fire了，窗口中的元素集合就会交给<code>Evictor</code>（如果指定了的话）。Evictor 主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有 Evictor 的话，窗口中的所有元素会一起交给函数进行计算。</p>
<p>计算函数收到了窗口的元素（可能经过了 Evictor 的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的<code>sum()</code>,<code>min()</code>,<code>max()</code>，还有 <code>ReduceFunction</code>，<code>FoldFunction</code>，还有<code>WindowFunction</code>。WindowFunction 是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p>
<p>Flink 对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了 Evictor，则不会启用对聚合窗口的优化，因为 Evictor 需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。</p>
<p><a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="flink的window分类？"><a href="#flink的window分类？" class="headerlink" title="flink的window分类？"></a>flink的window分类？</h3><p><strong>flink中的窗口主要分为3大类共5种窗口</strong>:</p>
<p><img src="/2019/11/24/大数据相关/flink中window分类.png" alt=""></p>
<ul>
<li><p><strong>Time Window 时间窗口</strong></p>
<ul>
<li><p><strong>Tumbing Time Window 滚动时间窗口</strong></p>
<p>实现统计每一分钟(或其他长度)窗口内 计算的效果</p>
</li>
<li><p><strong>Sliding Time Window 滑动时间窗口</strong></p>
<p>实现每过xxx时间 统计 xxx时间窗口的效果. 比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。</p>
</li>
</ul>
</li>
<li><p><strong>Count Window 计数窗口</strong></p>
<ul>
<li><p><strong>Tumbing Count Window  滚动计数窗口</strong></p>
<p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window）</p>
</li>
<li><p><strong>Sliding Count Window   滑动计数窗口</strong></p>
<p>和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和</p>
</li>
</ul>
</li>
<li><p><strong>Session Window  会话窗口</strong></p>
<p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）</p>
</li>
</ul>
<h3 id="flink-的-state-存储？"><a href="#flink-的-state-存储？" class="headerlink" title="flink 的 state 存储？"></a>flink 的 state 存储？</h3><p>Apache Flink内部有四种state的存储实现，具体如下：</p>
<ul>
<li><strong>基于内存的HeapStateBackend</strong> - 在debug模式使用，不 建议在生产模式下应用；</li>
<li><strong>基于HDFS的FsStateBackend</strong> - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li>
<li><strong>基于RocksDB的RocksDBStateBackend</strong> - 本地文件+异步HDFS持久化；</li>
<li><strong>基于Niagara(Alibaba内部实现)NiagaraStateBackend</strong> - 分布式持久化- 在Alibaba生产环境应用；</li>
</ul>
<p><a href="https://juejin.im/post/5c87dbdbe51d45494c77d607" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="flink是如何实现反压的？"><a href="#flink是如何实现反压的？" class="headerlink" title="flink是如何实现反压的？"></a>flink是如何实现反压的？</h3><p>flink的反压经历了两个发展阶段,分别是基于TCP的反压(<1.5)和基于credit的反压(>1.5)</1.5)和基于credit的反压(></p>
<ul>
<li><h4 id="基于-TCP-的反压"><a href="#基于-TCP-的反压" class="headerlink" title="基于 TCP 的反压"></a>基于 TCP 的反压</h4><p>flink中的消息发送通过RS(ResultPartition),消息接收通过IC(InputGate),两者的数据都是以 LocalBufferPool的形式来存储和提取,进一步的依托于Netty的NetworkBufferPool,之后更底层的便是依托于TCP的滑动窗口机制,当IC端的buffer池满了之后,两个task之间的滑动窗口大小便为0,此时RS端便无法再发送数据</p>
<p>基于TCP的反压最大的问题是会造成整个TaskManager端的反压,所有的task都会受到影响</p>
</li>
<li><h4 id="基于-Credit-的反压"><a href="#基于-Credit-的反压" class="headerlink" title="基于 Credit 的反压"></a>基于 Credit 的反压</h4><p>RS与IC之间通过backlog和credit来确定双方可以发送和接受的数据量的大小以提前感知,而不是通过TCP滑动窗口的形式来确定buffer的大小之后再进行反压</p>
<p><img src="/2019/11/24/大数据相关/flink基于credit的反压.png" alt=""></p>
</li>
</ul>
<p><a href="https://www.bilibili.com/video/av55487329" target="_blank" rel="noopener">参考视频</a></p>
<p><a href="https://blog.csdn.net/u010376788/article/details/92086752" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://blog.csdn.net/u010376788/article/details/95047250" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="flink的部署模式都有哪些？"><a href="#flink的部署模式都有哪些？" class="headerlink" title="flink的部署模式都有哪些？"></a>flink的部署模式都有哪些？</h3><p><strong>flink可以以多种方式部署,包括standlone模式/yarn/Mesos/Kubernetes/Docker/AWS/Google Compute Engine/MAPR等</strong></p>
<p>一般公司中主要采用 on yarn模式</p>
<h3 id="讲一下flink-on-yarn的部署？"><a href="#讲一下flink-on-yarn的部署？" class="headerlink" title="讲一下flink on yarn的部署？"></a>讲一下flink on yarn的部署？</h3><p>Flink作业提交有两种类型:</p>
<ul>
<li><h4 id="yarn-session"><a href="#yarn-session" class="headerlink" title="yarn session"></a>yarn session</h4><p>需要先启动集群，然后在提交作业，接着会向yarn申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交.</p>
<ul>
<li><p>客户端模式</p>
<p>对于客户端模式而言，你可以启动多个yarn session，一个yarn session模式对应一个JobManager,并按照需求提交作业，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止.</p>
</li>
<li><p>分离式模式</p>
<p>对于分离式模式，并不像客户端那样可以启动多个yarn session，如果启动多个，会出现下面的session一直处在等待状态。JobManager的个数只能是一个，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止</p>
</li>
</ul>
</li>
<li><h4 id="Flink-run-Per-Job"><a href="#Flink-run-Per-Job" class="headerlink" title="Flink run(Per-Job)"></a>Flink run(Per-Job)</h4><p>直接在YARN上提交运行Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job,即没提交一个作业会根据自身的情况，向yarn申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Session</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>共享Dispatcher和Resource Manager</td>
<td>Dispatcher和Resource Manager</td>
</tr>
<tr>
<td>共享资源(即 TaskExecutor)</td>
<td>按需要申请资源 (即 TaskExecutor)</td>
</tr>
<tr>
<td>适合规模小,执行时间短的作业</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2019/11/24/大数据相关/flinkOnYarn.png" alt=""></p>
<h3 id="flink中的时间概念-eventTime-和-processTime的区别？"><a href="#flink中的时间概念-eventTime-和-processTime的区别？" class="headerlink" title="flink中的时间概念 , eventTime 和 processTime的区别？"></a>flink中的时间概念 , eventTime 和 processTime的区别？</h3><p>Flink中有三种时间概念,分别是 Processing Time、Event Time 和 Ingestion Time</p>
<ul>
<li><h4 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h4><p>Processing Time 是指事件被处理时机器的系统时间。</p>
<p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件</p>
</li>
<li><h4 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h4><p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制</p>
</li>
<li><h4 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h4><p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳</p>
<p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）</p>
<p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印</p>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/55322400" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="flink中的session-Window怎样使用？"><a href="#flink中的session-Window怎样使用？" class="headerlink" title="flink中的session Window怎样使用？"></a>flink中的session Window怎样使用？</h3><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口进行计算,窗口的触发条件是 Session Gap, 是指在规定的时间内如果没有数据活跃接入,则认为窗口结束,然后触发窗口结果</p>
<p>Session Windows窗口类型比较适合非连续性数据处理或周期性产生数据的场景,根据用户在线上某段时间内的活跃度对用户行为进行数据统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val sessionWindowStream = inputStream</span><br><span class="line">.keyBy(_.id)</span><br><span class="line">//使用EventTimeSessionWindow 定义 Event Time 滚动窗口</span><br><span class="line">.window(EventTimeSessionWindow.withGap(Time.milliseconds(10)))</span><br><span class="line">.process(......)</span><br></pre></td></tr></table></figure>
<p>Session Window 本质上没有固定的起止时间点,因此底层计算逻辑和Tumbling窗口及Sliding 窗口有一定的区别,</p>
<p>Session Window 为每个进入的数据都创建了一个窗口,最后再将距离窗口Session Gap 最近的窗口进行合并,然后计算窗口结果</p>
<h3 id="flink中的session-Window怎样使用"><a href="#flink中的session-Window怎样使用" class="headerlink" title="flink中的session Window怎样使用"></a>flink中的session Window怎样使用</h3><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口进行计算,窗口的触发条件是 Session Gap, 是指在规定的时间内如果没有数据活跃接入,则认为窗口结束,然后触发窗口结果</p>
<p>Session Windows窗口类型比较适合非连续性数据处理或周期性产生数据的场景,根据用户在线上某段时间内的活跃度对用户行为进行数据统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val sessionWindowStream = inputStream</span><br><span class="line">.keyBy(_.id)</span><br><span class="line">//使用EventTimeSessionWindow 定义 Event Time 滚动窗口</span><br><span class="line">.window(EventTimeSessionWindow.withGap(Time.milliseconds(10)))</span><br><span class="line">.process(......)</span><br></pre></td></tr></table></figure>
<p>Session Window 本质上没有固定的起止时间点,因此底层计算逻辑和Tumbling窗口及Sliding 窗口有一定的区别,</p>
<p>Session Window 为每个进入的数据都创建了一个窗口,最后再将距离窗口Session Gap 最近的窗口进行合并,然后计算窗口结果</p>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="Hbase-架构"><a href="#Hbase-架构" class="headerlink" title="Hbase 架构"></a>Hbase 架构</h3><p><img src="/2019/11/24/大数据相关/hbase架构图.png" alt=""></p>
<p><strong>Hbase主要包含HMaster/HRegionServer/Zookeeper</strong></p>
<ul>
<li><p><strong>HRegionServer 负责实际数据的读写. 当访问数据时, 客户端直接与RegionServer通信.</strong></p>
<p>HBase的表根据Row Key的区域分成多个Region, 一个Region包含这这个区域内所有数据. 而Region server负责管理多个Region, 负责在这个Region server上的所有region的读写操作. </p>
</li>
<li><p><strong>HMaster 负责管理Region的位置, DDL(新增和删除表结构)</strong></p>
<ul>
<li>协调RegionServer</li>
<li>在集群处于数据恢复或者动态调整负载时,分配Region到某一个RegionServer中</li>
<li>管控集群,监控所有Region Server的状态</li>
<li>提供DDL相关的API, 新建(create),删除(delete)和更新(update)表结构.</li>
</ul>
</li>
<li><p><strong>Zookeeper 负责维护和记录整个Hbase集群的状态</strong></p>
<p>zookeeper探测和记录Hbase集群中服务器的状态信息.如果zookeeper发现服务器宕机,它会通知Hbase的master节点.</p>
</li>
</ul>
<h3 id="hbase-如何设计-rowkey？"><a href="#hbase-如何设计-rowkey？" class="headerlink" title="hbase 如何设计 rowkey？"></a>hbase 如何设计 rowkey？</h3><ul>
<li><p><strong>RowKey长度原则</strong></p>
<p>Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。</p>
<p>原因如下：</p>
<ul>
<li>数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率；</li>
<li>MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。</li>
<li>目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。</li>
</ul>
</li>
<li><p><strong>RowKey散列原则</strong></p>
<p>如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。</p>
</li>
<li><p><strong>RowKey唯一原则</strong></p>
<p>必须在设计上保证其唯一性。</p>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/30074408" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="http://www.nosqlnotes.com/technotes/hbase/hbase-rowkey-design/" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="hbase的存储结构"><a href="#hbase的存储结构" class="headerlink" title="hbase的存储结构"></a>hbase的存储结构</h3><p><img src="/2019/11/24/大数据相关/hbase逻辑结构.png" alt=""></p>
<p><strong>Hbase的优点及应用场景</strong>:</p>
<ol>
<li>半结构化或非结构化数据:<br>对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。</li>
<li>记录很稀疏：<br>RDBMS的行有多少列是固定的。为null的列浪费了存储空间。HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。</li>
<li>多版本号数据：<br>依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。</li>
<li>仅要求最终一致性：<br>对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致）</li>
<li>高可用和海量数据以及很大的瞬间写入量：<br>WAL解决高可用，支持PB级数据，put性能高<br>适用于插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。（HBase的写操作更加高效）</li>
<li>业务场景简单：<br>不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。</li>
</ol>
<p><strong>Hbase的缺点：</strong></p>
<ol>
<li>单一RowKey固有的局限性决定了它不可能有效地支持多条件查询</li>
<li>不适合于大范围扫描查询</li>
<li>不直接支持 SQL 的语句查询</li>
</ol>
<p><a href="https://www.iteye.com/blog/forlan-2364661" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://blog.csdn.net/liaynling/article/details/81199238" target="_blank" rel="noopener">参考文章2</a></p>
<p><a href="https://juejin.im/post/5c31cf486fb9a04a102f6f89#heading-2" target="_blank" rel="noopener">参考文章3</a></p>
<h3 id="hbase的HA实现-zookeeper在其中的作用？"><a href="#hbase的HA实现-zookeeper在其中的作用？" class="headerlink" title="hbase的HA实现,zookeeper在其中的作用？"></a>hbase的HA实现,zookeeper在其中的作用？</h3><p> HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。<br>配置HBase高可用，只需要启动两个HMaster，让Zookeeper自己去选择一个Master Acitve即可</p>
<p>zk的在这里起到的作用就是用来管理master节点,以及帮助hbase做master选举</p>
<h3 id="HMaster宕机的时候-哪些操作还能正常工作？"><a href="#HMaster宕机的时候-哪些操作还能正常工作？" class="headerlink" title="HMaster宕机的时候,哪些操作还能正常工作？"></a>HMaster宕机的时候,哪些操作还能正常工作？</h3><p>对表内数据的增删查改是可以正常进行的,因为hbase client 访问数据只需要通过 zookeeper 来找到 rowkey 的具体 region 位置即可. 但是对于创建表/删除表等的操作就无法进行了,因为这时候是需要HMaster介入, 并且region的拆分,合并,迁移等操作也都无法进行了</p>
<h3 id="hbase的写数据的流程？"><a href="#hbase的写数据的流程？" class="headerlink" title="hbase的写数据的流程？"></a>hbase的写数据的流程？</h3><ol>
<li>Client先访问zookeeper，从.META.表获取相应region信息，然后从meta表获取相应region信息 </li>
<li>根据namespace、表名和rowkey根据meta表的数据找到写入数据对应的region信息 </li>
<li>找到对应的regionserver 把数据先写到WAL中，即HLog，然后写到MemStore上 </li>
<li>MemStore达到设置的阈值后则把数据刷成一个磁盘上的StoreFile文件。 </li>
<li>当多个StoreFile文件达到一定的大小后(这个可以称之为小合并，合并数据可以进行设置，必须大于等于2，小于10——hbase.hstore.compaction.max和hbase.hstore.compactionThreshold，默认为10和3)，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。） </li>
<li>当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split）【可称之为大合并，该阈值通过hbase.hregion.max.filesize设置，默认为10G】，并由Hmaster分配到相应的HRegionServer，实现负载均衡</li>
</ol>
<h3 id="hbase读数据的流程？"><a href="#hbase读数据的流程？" class="headerlink" title="hbase读数据的流程？"></a>hbase读数据的流程？</h3><ol>
<li>首先，客户端需要获知其想要读取的信息的Region的位置，这个时候，Client访问hbase上数据时并不需要Hmaster参与（HMaster仅仅维护着table和Region的元数据信息，负载很低），只需要访问zookeeper，从meta表获取相应region信息(地址和端口等)。【Client请求ZK获取.META.所在的RegionServer的地址。】</li>
<li>客户端会将该保存着RegionServer的位置信息的元数据表.META.进行缓存。然后在表中确定待检索rowkey所在的RegionServer信息（得到持有对应行键的.META表的服务器名）。【获取访问数据所在的RegionServer地址】</li>
<li>根据数据所在RegionServer的访问信息，客户端会向该RegionServer发送真正的数据读取请求。服务器端接收到该请求之后需要进行复杂的处理。</li>
<li>先从MemStore找数据，如果没有，再到StoreFile上读(为了读取的效率)。</li>
</ol>
<p><a href="https://blog.csdn.net/HaixWang/article/details/79520141#%E8%AF%BB%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/?rkfcfo=fy6gy1" target="_blank" rel="noopener">参考文章2</a></p>
<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="kafka-的架构？"><a href="#kafka-的架构？" class="headerlink" title="kafka 的架构？"></a>kafka 的架构？</h3><h3 id="kafka-与其他消息组件对比？"><a href="#kafka-与其他消息组件对比？" class="headerlink" title="kafka 与其他消息组件对比？"></a>kafka 与其他消息组件对比？</h3><p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/why-mq.md" target="_blank" rel="noopener">推荐阅读文章</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>RocketMQ</th>
<th>Kafka</th>
</tr>
</thead>
<tbody>
<tr>
<td>单机吞吐量</td>
<td>万级，比 RocketMQ、Kafka 低一个数量级</td>
<td>同 ActiveMQ</td>
<td>10 万级，支撑高吞吐</td>
<td>10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景</td>
</tr>
<tr>
<td>topic 数量对吞吐量的影响</td>
<td></td>
<td></td>
<td>topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic</td>
<td>topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源</td>
</tr>
<tr>
<td>时效性</td>
<td>ms 级</td>
<td>微秒级，这是 RabbitMQ 的一大特点，延迟最低</td>
<td>ms 级</td>
<td>延迟在 ms 级以内</td>
</tr>
<tr>
<td>可用性</td>
<td>高，基于主从架构实现高可用</td>
<td>同 ActiveMQ</td>
<td>非常高，分布式架构</td>
<td>非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用</td>
</tr>
<tr>
<td>消息可靠性</td>
<td>有较低的概率丢失数据</td>
<td>基本不丢</td>
<td>经过参数优化配置，可以做到 0 丢失</td>
<td>同 RocketMQ</td>
</tr>
<tr>
<td>功能支持</td>
<td>MQ 领域的功能极其完备</td>
<td>基于 erlang 开发，并发能力很强，性能极好，延时很低</td>
<td>MQ 功能较为完善，还是分布式的，扩展性好</td>
<td>功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用</td>
</tr>
</tbody>
</table>
</div>
<h3 id="kafka-实现高吞吐的原理？"><a href="#kafka-实现高吞吐的原理？" class="headerlink" title="kafka 实现高吞吐的原理？"></a>kafka 实现高吞吐的原理？</h3><ul>
<li>读写文件依赖OS文件系统的页缓存，而不是在JVM内部缓存数据，利用OS来缓存，内存利用率高</li>
<li>sendfile技术（零拷贝），避免了传统网络IO四步流程</li>
<li>支持End-to-End的压缩</li>
<li>顺序IO以及常量时间get、put消息</li>
<li>Partition 可以很好的横向扩展和提供高并发处理</li>
</ul>
<p><a href="https://www.jianshu.com/p/d6a73be9d803" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://blog.csdn.net/stark_summer/article/details/50144591" target="_blank" rel="noopener">参考文章2</a></p>
<h3 id="kafka怎样保证不重复消费？"><a href="#kafka怎样保证不重复消费？" class="headerlink" title="kafka怎样保证不重复消费？"></a>kafka怎样保证不重复消费？</h3><p>此问题其实等价于保证消息队列消费的幂等性</p>
<p>主要需要结合实际业务来操作:</p>
<ul>
<li>比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。</li>
<li>比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。</li>
<li>比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。</li>
<li>比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。</li>
</ul>
<p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="kafka怎样保证不丢失消息？"><a href="#kafka怎样保证不丢失消息？" class="headerlink" title="kafka怎样保证不丢失消息？"></a>kafka怎样保证不丢失消息？</h3><h4 id="消费端弄丢了数据"><a href="#消费端弄丢了数据" class="headerlink" title="消费端弄丢了数据"></a>消费端弄丢了数据</h4><p>唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边<strong>自动提交了 offset</strong>，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。</p>
<p>这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要<strong>关闭自动提交</strong> offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是<strong>可能会有重复消费</strong>，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。</p>
<p>生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。</p>
<h4 id="Kafka-弄丢了数据"><a href="#Kafka-弄丢了数据" class="headerlink" title="Kafka 弄丢了数据"></a>Kafka 弄丢了数据</h4><p>这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。</p>
<p>生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。</p>
<p>所以此时一般是要求起码设置如下 4 个参数：</p>
<ul>
<li>给 topic 设置 <code>replication.factor</code> 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。</li>
<li>在 Kafka 服务端设置 <code>min.insync.replicas</code> 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。</li>
<li>在 producer 端设置 <code>acks=all</code>：这个是要求每条数据，必须是<strong>写入所有 replica 之后，才能认为是写成功了</strong>。</li>
<li>在 producer 端设置 <code>retries=MAX</code>（很大很大很大的一个值，无限次重试的意思）：这个是<strong>要求一旦写入失败，就无限重试</strong>，卡在这里了。</li>
</ul>
<p>我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。</p>
<h4 id="生产者会不会弄丢数据？"><a href="#生产者会不会弄丢数据？" class="headerlink" title="生产者会不会弄丢数据？"></a>生产者会不会弄丢数据？</h4><p>如果按照上述的思路设置了 <code>acks=all</code>，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。</p>
<p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.md" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="kafka-与-spark-streaming-集成-如何保证-exactly-once-语义？"><a href="#kafka-与-spark-streaming-集成-如何保证-exactly-once-语义？" class="headerlink" title="kafka 与 spark streaming 集成,如何保证 exactly once 语义？"></a>kafka 与 spark streaming 集成,如何保证 exactly once 语义？</h3><ul>
<li><h3 id="Spark-Streaming上游对接kafka时保证Exactly-Once"><a href="#Spark-Streaming上游对接kafka时保证Exactly-Once" class="headerlink" title="Spark Streaming上游对接kafka时保证Exactly Once"></a>Spark Streaming上游对接kafka时保证Exactly Once</h3><p>Spark Streaming使用Direct模式对接上游kafka。无论kafka有多少个partition， 使用Direct模式总能保证SS中有相同数量的partition与之相对， 也就是说SS中的KafkaRDD的并发数量在Direct模式下是由上游kafka决定的。 在这个模式下，kafka的offset是作为KafkaRDD的一部分存在，会存储在checkpoints中， 由于checkpoints只存储offset内容，而不存储数据，这就使得checkpoints是相对轻的操作。 这就使得SS在遇到故障时，可以从checkpoint中恢复上游kafka的offset，从而保证exactly once</p>
</li>
<li><h3 id="Spark-Streaming输出下游保证Exactly-once"><a href="#Spark-Streaming输出下游保证Exactly-once" class="headerlink" title="Spark Streaming输出下游保证Exactly once"></a>Spark Streaming输出下游保证Exactly once</h3><ul>
<li><p>第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。</p>
<p>多次尝试总是写入相同的数据，例如，saveAs<em>*</em>Files 总是将相同的数据写入生成的文件</p>
</li>
</ul>
</li>
<li><ul>
<li><p>使用事务更新</p>
<p>所有更新都是事务性的，以便更新完全按原子进行。这样做的一个方法如下： 使用批处理时间(在foreachRDD中可用)和RDD的partitionIndex（分区索引）来创建identifier（标识符)。 该标识符唯一地标识streaming application 中的blob数据。 使用该identifier，blob 事务地更新到外部系统中。也就是说，如果identifier尚未提交，则以 (atomicall)原子方式提交分区数据和identifier。否则，如果已经提交，请跳过更新。</p>
</li>
</ul>
</li>
</ul>
<p><a href="http://www.aihacks.life/post/spark-streaming%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81exactly-once%E8%AF%AD%E4%B9%89/" target="_blank" rel="noopener">参考文章1</a></p>
<p><a href="https://www.jianshu.com/p/10de8f3b1be8" target="_blank" rel="noopener">参考文章2</a></p>
<p><a href="https://blog.csdn.net/cymvp/article/details/52605987" target="_blank" rel="noopener">参考文章3</a></p>
<h3 id="ack-有哪几种-生产中怎样选择？"><a href="#ack-有哪几种-生产中怎样选择？" class="headerlink" title="ack 有哪几种, 生产中怎样选择？"></a>ack 有哪几种, 生产中怎样选择？</h3><p>ack=0/1/-1的不同情况：</p>
<ul>
<li><p>Ack = 0</p>
<p>producer不等待broker的ack，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</p>
</li>
<li><p>Ack = 1</p>
<p>producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</p>
</li>
<li><p>Ack = -1</p>
<p>producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack，数据一般不会丢失，延迟时间长但是可靠性高。</p>
</li>
</ul>
<p><strong>生产中主要以 Ack=-1为主,如果压力过大,可切换为Ack=1. Ack=0的情况只能在测试中使用.</strong></p>
<h3 id="如何通过-offset-寻找数据？"><a href="#如何通过-offset-寻找数据？" class="headerlink" title="如何通过 offset 寻找数据？"></a>如何通过 offset 寻找数据？</h3><p>如果consumer要找offset是1008的消息，那么，</p>
<p>1，按照二分法找到小于1008的segment，也就是00000000000000001000.log和00000000000000001000.index</p>
<p>2，用目标offset减去文件名中的offset得到消息在这个segment中的偏移量。也就是1008-1000=8，偏移量是8。</p>
<p>3，再次用二分法在index文件中找到对应的索引，也就是第三行6,45。</p>
<p>4，到log文件中，从偏移量45的位置开始（实际上这里的消息offset是1006），顺序查找，直到找到offset为1008的消息。查找期间kafka是按照log的存储格式来判断一条消息是否结束的。</p>
<p><a href="https://blog.csdn.net/lkforce/article/details/77854813" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="如何清理过期数据？（删除-amp-压缩）"><a href="#如何清理过期数据？（删除-amp-压缩）" class="headerlink" title="如何清理过期数据？（删除&amp;压缩）"></a>如何清理过期数据？（删除&amp;压缩）</h3><ul>
<li><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>log.cleanup.policy=delete启用删除策略</p>
<ul>
<li>直接删除，删除后的消息不可恢复。可配置以下两个策略：<br>清理超过指定时间清理：<br>log.retention.hours=16</li>
<li>超过指定大小后，删除旧的消息：<br>log.retention.bytes=1073741824<br>为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。</li>
</ul>
</li>
<li><h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><p>将数据压缩，只保留每个key最后一个版本的数据。<br>首先在broker的配置中设置log.cleaner.enable=true启用cleaner，这个默认是关闭的。<br>在topic的配置中设置log.cleanup.policy=compact启用压缩策略。</p>
<p><img src="/2019/11/24/大数据相关/kafka压缩.png" alt=""></p>
<p>如上图，在整个数据流中，每个Key都有可能出现多次，压缩时将根据Key将消息聚合，只保留最后一次出现时的数据。这样，无论什么时候消费消息，都能拿到每个Key的最新版本的数据。<br>压缩后的offset可能是不连续的，比如上图中没有5和7，因为这些offset的消息被merge了，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，比如，当试图获取offset为5的消息时，实际上会拿到offset为6的消息，并从这个位置开始消费。<br>这种策略只适合特俗场景，比如消息的key是用户ID，消息体是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。<br>压缩策略支持删除，当某个Key的最新版本的消息没有内容时，这个Key将被删除，这也符合以上逻辑。</p>
</li>
</ul>
<p><a href="https://blog.csdn.net/honglei915/article/details/49683065" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="1条message中包含哪些信息？"><a href="#1条message中包含哪些信息？" class="headerlink" title="1条message中包含哪些信息？"></a>1条message中包含哪些信息？</h3><div class="table-container">
<table>
<thead>
<tr>
<th><strong>Field</strong></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attributes</td>
<td>该字节包含有关消息的元数据属性。 最低的2位包含用于消息的压缩编解码器。 其他位应设置为0。</td>
</tr>
<tr>
<td>Crc</td>
<td>CRC是消息字节的其余部分的CRC32。 这用于检查代理和使用者上的消息的完整性。</td>
</tr>
<tr>
<td></td>
<td>key是用于分区分配的可选参数。 key可以为null。</td>
</tr>
<tr>
<td>MagicByte</td>
<td>这是用于允许向后兼容的消息二进制格式演变的版本ID。 当前值为0。</td>
</tr>
<tr>
<td>Offset</td>
<td>这是kafka中用作日志序列号的偏移量。 当producer发送消息时，它实际上并不知道偏移量，并且可以填写它喜欢的任何值。</td>
</tr>
<tr>
<td>Value</td>
<td>该值是实际的消息内容，作为不透明的字节数组。 Kafka支持递归消息，在这种情况下，它本身可能包含消息集。 消息可以为null。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="讲一下zookeeper在kafka中的作用？"><a href="#讲一下zookeeper在kafka中的作用？" class="headerlink" title="讲一下zookeeper在kafka中的作用？"></a>讲一下zookeeper在kafka中的作用？</h3><p><img src="/2019/11/24/大数据相关/kafka在zk中的存储结构.png" alt=""></p>
<h4 id="zk的作用主要有如下几点"><a href="#zk的作用主要有如下几点" class="headerlink" title="zk的作用主要有如下几点:"></a>zk的作用主要有如下几点:</h4><ol>
<li>kafka的元数据都存放在zk上面,由zk来管理</li>
<li>0.8之前版本的kafka, consumer的消费状态，group的管理以及 offset的值都是由zk管理的,现在offset会保存在本地topic文件里</li>
<li>负责borker的lead选举和管理</li>
</ol>
<h3 id="kafka-可以脱离-zookeeper-单独使用吗？"><a href="#kafka-可以脱离-zookeeper-单独使用吗？" class="headerlink" title="kafka 可以脱离 zookeeper 单独使用吗？"></a>kafka 可以脱离 zookeeper 单独使用吗？</h3><p>kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。</p>
<h3 id="kafka有几种数据保留策略？"><a href="#kafka有几种数据保留策略？" class="headerlink" title="kafka有几种数据保留策略？"></a>kafka有几种数据保留策略？</h3><p>kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。</p>
<h3 id="kafka同时设置了7天和10G清除数据-到第5天的时候消息到达了10G-这个时候kafka如何处理？"><a href="#kafka同时设置了7天和10G清除数据-到第5天的时候消息到达了10G-这个时候kafka如何处理？" class="headerlink" title="kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理？"></a>kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理？</h3><p>这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。</p>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="zookeeper是什么-都有哪些功能？"><a href="#zookeeper是什么-都有哪些功能？" class="headerlink" title="zookeeper是什么,都有哪些功能？"></a>zookeeper是什么,都有哪些功能？</h3><h3 id="zk-有几种部署模式？"><a href="#zk-有几种部署模式？" class="headerlink" title="zk 有几种部署模式？"></a>zk 有几种部署模式？</h3><p>zookeeper有两种运行模式: 集群模式和单机模式,还有一种伪集群模式,在单机模式下模拟集群的zookeeper服务</p>
<h3 id="zk-是怎样保证主从节点的状态同步？"><a href="#zk-是怎样保证主从节点的状态同步？" class="headerlink" title="zk 是怎样保证主从节点的状态同步？"></a>zk 是怎样保证主从节点的状态同步？</h3><p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p>
<h3 id="说一下-zk-的通知机制？"><a href="#说一下-zk-的通知机制？" class="headerlink" title="说一下 zk 的通知机制？"></a>说一下 zk 的通知机制？</h3><p>客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变</p>
<h3 id="zk-的分布式锁实现方式？"><a href="#zk-的分布式锁实现方式？" class="headerlink" title="zk 的分布式锁实现方式？"></a>zk 的分布式锁实现方式？</h3><p>使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：</p>
<ol>
<li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li>
<li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则<strong>监听刚好在自己之前一位的子节点删除消息</strong>，获得子节点变更通知后重复此步骤直至获得锁；</li>
<li>执行业务代码；</li>
<li>完成业务流程后，删除对应的子节点释放锁。</li>
</ol>
<p><a href="http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="分布式一致性协议"><a href="#分布式一致性协议" class="headerlink" title="分布式一致性协议"></a>分布式一致性协议</h3><p>常见的分布式一致性协议有: 两阶段提交协议，三阶段提交协议，向量时钟，RWN协议，paxos协议，Raft协议. zk采用的是paxos协议.</p>
<h4 id="两阶段提交协议-2PC"><a href="#两阶段提交协议-2PC" class="headerlink" title="两阶段提交协议(2PC)"></a>两阶段提交协议(2PC)</h4><p>两阶段提交协议，简称2PC，是比较常用的解决分布式事务问题的方式，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性(A)的常用手段。</p>
<h4 id="三阶段提交协议-3PC"><a href="#三阶段提交协议-3PC" class="headerlink" title="三阶段提交协议(3PC)"></a>三阶段提交协议(3PC)</h4><p>3PC就是在2PC基础上将2PC的提交阶段细分位两个阶段：预提交阶段和提交阶段</p>
<h4 id="向量时钟"><a href="#向量时钟" class="headerlink" title="向量时钟"></a>向量时钟</h4><p>通过向量空间祖先继承的关系比较, 使数据保持最终一致性,这就是向量时钟的基本定义。</p>
<h4 id="NWR协议"><a href="#NWR协议" class="headerlink" title="NWR协议"></a>NWR协议</h4><p>NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在Amazon的Dynamo云存储系统中，就应用NWR来控制一致性。<br>让我们先来看看这三个字母的含义：<br>N：在分布式存储系统中，有多少份备份数据<br>W：代表一次成功的更新操作要求至少有w份数据写入成功<br>R： 代表一次成功的读数据操作要求至少有R份数据成功读取<br>NWR值的不同组合会产生不同的一致性效果，当W+R&gt;N的时候，整个系统对于客户端来讲能保证强一致性。当W+R 以常见的N=3、W=2、R=2为例：<br>N=3，表示，任何一个对象都必须有三个副本（Replica），W=2表示，对数据的修改操作（Write）只需要在3个Replica中的2个上面完成就返回，R=2表示，从三个对象中要读取到2个数据对象，才能返回。<br>在分布式系统中，数据的单点是不允许存在的。即线上正常存在的Replica数量是1的情况是非常危险的，因为一旦这个Replica再次错误，就 可能发生数据的永久性错误。假如我们把N设置成为2，那么，只要有一个存储节点发生损坏，就会有单点的存在。所以N必须大于2。N约高，系统的维护和整体 成本就越高。工业界通常把N设置为3。<br>当W是2、R是2的时候，W+R&gt;N，这种情况对于客户端就是强一致性的。</p>
<h4 id="paxos协议"><a href="#paxos协议" class="headerlink" title="paxos协议"></a>paxos协议</h4><p>Paxos算法是<strong>Lamport</strong>于1990年提出的一种基于<strong>消息传递的一致性算法</strong>，</p>
<ul>
<li>数据库高可用性难题<ul>
<li>痛点：如何处理主备库之间的数据同步。</li>
<li>如何选择主节点</li>
</ul>
</li>
<li>基本需求：<ul>
<li>数据不丢失</li>
<li>服务持续可用</li>
<li>自动的主备切换</li>
</ul>
</li>
<li>目标：<strong>谋求尽早形成多数派</strong></li>
</ul>
<p><a href="https://zh.wikipedia.org/wiki/Paxos算法" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95</a></p>
<p><a href="https://www.zhihu.com/question/19787937/answer/107750652" target="_blank" rel="noopener">https://www.zhihu.com/question/19787937/answer/107750652</a></p>
<p><a href="http://chuansong.me/n/2189245" target="_blank" rel="noopener">架构师需要了解的Paxos原理，历程及实践</a></p>
<h4 id="Raft协议"><a href="#Raft协议" class="headerlink" title="Raft协议"></a>Raft协议</h4><p>Raft是一个通过管理一个副本日志的一致性算法。它提供了跟(multi-)Paxos一样有效的功能，但是它的架构和Paxos不一样；它比Paxos更加容易理解，并且能用于生产环境中。为了加强理解，raft把一致性的问题分成了三个子问题，例如<strong>leader election, log replication, and safety</strong>,在Raft集群中，<strong>有且仅有一个Leader</strong>，在Leader运行正常的情况下，一个节点服务器要么就是Leader，要么就是Follower。Follower直到Leader故障了，才有可能变成candidate。</p>
<p><strong>角色：</strong></p>
<ul>
<li><strong>follower</strong></li>
<li><strong>leader</strong></li>
<li><strong>candidate：</strong>If followers don’t hear from a leader then they can become a candidate.</li>
</ul>
<p>子问题：</p>
<ul>
<li>Leader Election.</li>
<li>Log Replication</li>
<li></li>
</ul>
<p><em>In Search of an Understandable Consensus Algorithm</em><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ongaro, D., & Ousterhout, J. (2014). In search of an understandable consensus algorithm. In *2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14)* (pp. 305-319).">[10]</span></a></sup></p>
<p><a href="https://raft.github.io/" target="_blank" rel="noopener">https://raft.github.io/</a></p>
<p><a href="http://thesecretlivesofdata.com/raft/" target="_blank" rel="noopener">Raft协议的动画</a></p>
<p><strong>视频教程</strong></p>
<iframe src="http://open.iqiyi.com/developer/player_js/coopPlayerIndex.html?vid=28fc6d071129b2cdc993b22ed4895cf2&tvId=43770694409&accessToken=2.f22860a2479ad60d8da7697274de9346&appKey=3955c3425820435e86d0f4cdfe56f5e7&appId=1368&height=100%&width=100%" frameborder="0" allowfullscreen="true" width="480" height="350"></iframe>

<p><a href="https://blog.csdn.net/chdhust/article/details/52651741" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="讲一下leader-选举过程？"><a href="#讲一下leader-选举过程？" class="headerlink" title="讲一下leader 选举过程？"></a>讲一下leader 选举过程？</h3><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinod  Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad Agarwal, &amp;  Eric Baldeschwieler. (2013). Apache Hadoop YARN: yet another resource  negotiator. <em>Proceedings of the 4th annual Symposium on Cloud Computing</em>. ACM.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://hadoop.apache.org/docs/<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vavilapalli, V. K., Murthy, A. C., Douglas, C., Agarwal, S., Konar, M., Evans, R., ... &amp; Saha, B. (2013, October). Apache hadoop yarn: Yet another resource negotiator. In <em>Proceedings of the 4th annual Symposium on Cloud Computing</em> (p. 5). ACM.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://hadoop.apache.org/docs/r2.9.1/hadoop-project-dist/hadoop-hdfs/api/overview-summary.html<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://github.com/maomao1994/TPC-H/<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.damaoguo.site/2019/03/30/spark%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/" target="_blank" rel="noopener">http://www.damaoguo.site/2019/03/30/spark%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://www.tutorialspoint.com/hive/hive_introduction.htm<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ongaro, D., &amp; Ousterhout, J. (2014). In search of an understandable consensus algorithm. In <em>2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14)</em> (pp. 305-319).<a href="#fnref:10" rev="footnote"> ↩</a></span></li></ol></div></div>
      
    </div>

    


    
    
    

    
      <div>
         <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      </div>
    

    
      <div>
        

      </div>
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>我知道是不会有人点的，但万一有人想不开呢？</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="袤锅 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="袤锅 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/BigData/" rel="tag"><i class="fa fa-tag"></i> BigData</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/24/Linux命令/" rel="next" title="Linux命令">
                <i class="fa fa-chevron-left"></i> Linux命令
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/01/MySQL相关/" rel="prev" title="MySQL相关">
                MySQL相关 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
		  <div>
  <font color='#FFC0CB' size=20>Mao&Ping Adventure:</font><canvas id="canvasDiyBlock" style="width:60%;">x</canvas>。
</div>
<script>
	/*生成倒计时数字*/
	function getTimeDiff(){
		var date1= '2020/10/04 00:00:00';  //开始时间  
		var date2 = new Date();    //结束时间  
		var date3 = date2.getTime() - new Date(date1).getTime();   //时间差的毫秒数        

		//计算出相差天数  
		var days=Math.floor(date3/(24*3600*1000))  
		//计算出小时数  
		var leave1=date3%(24*3600*1000)    //计算天数后剩余的毫秒数  
		var hours=Math.floor(leave1/(3600*1000))  
		//计算相差分钟数  
		var leave2=leave1%(3600*1000)        //计算小时数后剩余的毫秒数  
		var minutes=Math.floor(leave2/(60*1000))  
		//计算相差秒数  
		var leave3=leave2%(60*1000)      //计算分钟数后剩余的毫秒数  
		var seconds=Math.round(leave3/1000) 
		return ("00000"+days).substr(-5)+":"+("00"+hours).substr(-2)+":"+("00"+minutes).substr(-2)+":"+("00"+seconds).substr(-2)
	}
(function () {
  var digit =
    [
      [
        [0, 0, 1, 1, 1, 0, 0],
        [0, 1, 1, 0, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 0, 1, 1, 0],
        [0, 0, 1, 1, 1, 0, 0]
      ],//0
      [
        [0, 0, 0, 1, 1, 0, 0],
        [0, 1, 1, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1]
      ],//1
      [
        [0, 1, 1, 1, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 1, 1, 1, 1, 1]
      ],//2
      [
        [1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 0]
      ],//3
      [
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 1, 0],
        [0, 0, 1, 1, 1, 1, 0],
        [0, 1, 1, 0, 1, 1, 0],
        [1, 1, 0, 0, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 1, 1]
      ],//4
      [
        [1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 0],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 0]
      ],//5
      [
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 1, 1, 0, 0, 0],
        [0, 1, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 0]
      ],//6
      [
        [1, 1, 1, 1, 1, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0],
        [0, 0, 1, 1, 0, 0, 0]
      ],//7
      [
        [0, 1, 1, 1, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 1, 1, 0]
      ],//8
      [
        [0, 1, 1, 1, 1, 1, 0],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [1, 1, 0, 0, 0, 1, 1],
        [0, 1, 1, 1, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1, 1, 0],
        [0, 0, 0, 1, 1, 0, 0],
        [0, 1, 1, 0, 0, 0, 0]
      ],//9
      [
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]
      ]//:
    ];

  var canvas = document.getElementById('canvasDiyBlock');

  if (canvas.getContext) {
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100, W = 1200;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10, 10, 50, 50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height / 20 - 1;
	var timeDiff = getTimeDiff();
    (function () {
      var temp = /(\d)(\d)(\d)(\d)(\d):(\d)(\d):(\d)(\d):(\d)(\d)/.exec(timeDiff);
      //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
	  // 天(万,千,百,十,个) 小时(十,个) 分钟(十，个) 秒(十 个)
      data.push(temp[1], temp[2], temp[3], temp[4], temp[5], 10, temp[6], temp[7], 10, temp[8], temp[9], 10, temp[10], temp[11]);
    })();
	

    /*生成点阵数字*/
    function renderDigit(index, num) {
      for (var i = 0; i < digit[num].length; i++) {
        for (var j = 0; j < digit[num][i].length; j++) {
          if (digit[num][i][j] == 1) {
            cxt.beginPath();
            cxt.arc(14 * (R + 2) * index + j * 2 * (R + 1) + (R + 1), i * 2 * (R + 1) + (R + 1), R, 0, 2 * Math.PI);
            cxt.closePath();
            cxt.fill();
          }
        }
      }
    }

    /*更新时钟*/
    function updateDigitTime() {
      var changeNumArray = [];
	  var timeDiff = getTimeDiff();
      var temp = /(\d)(\d)(\d)(\d)(\d):(\d)(\d):(\d)(\d):(\d)(\d)/.exec(timeDiff);
      var NewData = [];
      NewData.push(temp[1], temp[2], temp[3], temp[4], temp[5], 10, temp[6], temp[7], 10, temp[8], temp[9], 10, temp[10], temp[11]);

      for (var i = data.length - 1; i >= 0; i--) {
        //时间发生变化
        if (NewData[i] !== data[i]) {
          //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
          changeNumArray.push(i + '_' + (Number(data[i]) + 1) % 10);
        }
      }
      //增加小球
      for (var i = 0; i < changeNumArray.length; i++) {
        addBalls.apply(this, changeNumArray[i].split('_'));
      }
      data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls() {
      for (var i = 0; i < balls.length; i++) {
        balls[i].stepY += balls[i].disY;
        balls[i].x += balls[i].stepX;
        balls[i].y += balls[i].stepY;
        if (balls[i].x > W + R || balls[i].y > H + R) {
          balls.splice(i, 1);
          i--;
        }
      }
    }

    /*增加要运动的小球*/
    function addBalls(index, num) {
      var numArray = [1, 2, 3];
      var colorArray = ["#3BE", "#09C", "#A6C", "#93C", "#9C0", "#690", "#FB3", "#F80", "#F44", "#C00"];
      for (var i = 0; i < digit[num].length; i++) {
        for (var j = 0; j < digit[num][i].length; j++) {
          if (digit[num][i][j] == 1) {
            var ball = {
              x: 14 * (R + 2) * index + j * 2 * (R + 1) + (R + 1),
              y: i * 2 * (R + 1) + (R + 1),
              stepX: Math.floor(Math.random() * 4 - 2),
              stepY: -2 * numArray[Math.floor(Math.random() * numArray.length)],
              color: colorArray[Math.floor(Math.random() * colorArray.length)],
              disY: 1
            };
            balls.push(ball);
          }
        }
      }
    }

    /*渲染*/
    function render() {
      //重置画布宽度，达到清空画布的效果
      canvas.height = 100;
      //渲染时钟
      for (var i = 0; i < data.length; i++) {
        renderDigit(i, data[i]);
      }
      //渲染小球
      for (var i = 0; i < balls.length; i++) {
        cxt.beginPath();
        cxt.arc(balls[i].x, balls[i].y, R, 0, 2 * Math.PI);
        cxt.fillStyle = balls[i].color;
        cxt.closePath();
        cxt.fill();
      }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function () {
      //更新时钟
      updateDigitTime();
      //更新小球状态
      updateBalls();
      //渲染
      render();
    }, 50);
  }
})();
</script>

          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNjEzNC8xMjY2OQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/boy.jpg"
                alt="袤锅" />
            
              <p class="site-author-name" itemprop="name">袤锅</p>
              <p class="site-description motion-element" itemprop="description">我尽我力，我尽我心</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">41</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">30</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/damaoguo" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:tianmao818@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html" title="Hexo个性化设置" target="_blank">Hexo个性化设置</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop"><span class="nav-number">1.</span> <span class="nav-text">Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS架构？-2"><span class="nav-number">1.1.</span> <span class="nav-text">HDFS架构？[2]</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#HDFS-1"><span class="nav-number">1.1.1.</span> <span class="nav-text">HDFS-1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HDFS-2"><span class="nav-number">1.1.2.</span> <span class="nav-text">HDFS-2</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS比较（主要体现在jdk版本，HA等）"><span class="nav-number">1.2.</span> <span class="nav-text">HDFS比较（主要体现在jdk版本，HA等）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS相关的例子？"><span class="nav-number">1.3.</span> <span class="nav-text">HDFS相关的例子？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn架构？（资源管理）"><span class="nav-number">1.4.</span> <span class="nav-text">Yarn架构？（资源管理）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ResourceManager（RM）"><span class="nav-number">1.4.1.</span> <span class="nav-text">ResourceManager（RM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NodeManager（NM）"><span class="nav-number">1.4.2.</span> <span class="nav-text">NodeManager（NM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ApplicationMaster（AM）"><span class="nav-number">1.4.3.</span> <span class="nav-text">ApplicationMaster（AM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Container"><span class="nav-number">1.4.4.</span> <span class="nav-text">Container</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MapReduce过程？"><span class="nav-number">1.5.</span> <span class="nav-text">MapReduce过程？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Map阶段"><span class="nav-number">1.5.1.</span> <span class="nav-text">Map阶段:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reduce阶段"><span class="nav-number">1.5.2.</span> <span class="nav-text">Reduce阶段:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例-4"><span class="nav-number">1.5.3.</span> <span class="nav-text">案例[4]</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn-调度MapReduce？"><span class="nav-number">1.6.</span> <span class="nav-text">Yarn 调度MapReduce？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hdfs写流程？"><span class="nav-number">1.7.</span> <span class="nav-text">hdfs写流程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hdfs读流程？"><span class="nav-number">1.8.</span> <span class="nav-text">hdfs读流程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hdfs创建一个文件的流程？-类的调用过程"><span class="nav-number">1.9.</span> <span class="nav-text">hdfs创建一个文件的流程？(类的调用过程)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop1-x-和hadoop-2-x-的区别？"><span class="nav-number">1.10.</span> <span class="nav-text">hadoop1.x 和hadoop 2.x 的区别？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hadoop1-x："><span class="nav-number">1.10.1.</span> <span class="nav-text">hadoop1.x：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hadoop2-x："><span class="nav-number">1.10.2.</span> <span class="nav-text">hadoop2.x：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop-HA介绍？"><span class="nav-number">1.11.</span> <span class="nav-text">hadoop HA介绍？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop的常用配置文件？"><span class="nav-number">1.12.</span> <span class="nav-text">hadoop的常用配置文件？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小文件过多会有什么危害-如何避免？"><span class="nav-number">1.13.</span> <span class="nav-text">小文件过多会有什么危害,如何避免？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动hadoop集群会分别启动哪些进程-各自的作用？"><span class="nav-number">1.14.</span> <span class="nav-text">启动hadoop集群会分别启动哪些进程,各自的作用？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HIVE"><span class="nav-number">2.</span> <span class="nav-text">HIVE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍"><span class="nav-number">2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HIVE架构"><span class="nav-number">2.2.</span> <span class="nav-text">HIVE架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive的数据类型"><span class="nav-number">2.3.</span> <span class="nav-text">hive的数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL和JDBC"><span class="nav-number">2.4.</span> <span class="nav-text">SQL和JDBC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive-内部表和外部表的区别？"><span class="nav-number">2.5.</span> <span class="nav-text">hive 内部表和外部表的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive中-sort-by-order-by-cluster-by-distribute-by-的区别？"><span class="nav-number">2.6.</span> <span class="nav-text">Hive中 sort by / order by / cluster by / distribute by 的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive的metastore的三种模式？"><span class="nav-number">2.7.</span> <span class="nav-text">hive的metastore的三种模式？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hive-中-join都有哪些？"><span class="nav-number">2.8.</span> <span class="nav-text">hive 中 join都有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Impala-和-hive-的查询有哪些区别？"><span class="nav-number">2.9.</span> <span class="nav-text">Impala 和 hive 的查询有哪些区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive中大表join小表的优化方法？"><span class="nav-number">2.10.</span> <span class="nav-text">Hive中大表join小表的优化方法？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-Sql-是怎样解析成MR-job的"><span class="nav-number">2.11.</span> <span class="nav-text">Hive Sql 是怎样解析成MR job的?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-UDF简单介绍？"><span class="nav-number">2.12.</span> <span class="nav-text">Hive UDF简单介绍？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL题-按照学生科目分组-取每个科目的TopN？"><span class="nav-number">2.13.</span> <span class="nav-text">SQL题: 按照学生科目分组, 取每个科目的TopN？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL题-获取每个用户的前1-4次的数据？"><span class="nav-number">2.14.</span> <span class="nav-text">SQL题: 获取每个用户的前1/4次的数据？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark"><span class="nav-number">3.</span> <span class="nav-text">Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-的运行架构？"><span class="nav-number">3.1.</span> <span class="nav-text">spark 的运行架构？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一个spark程序的执行流程？"><span class="nav-number">3.2.</span> <span class="nav-text">一个spark程序的执行流程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的shuffle介绍？"><span class="nav-number">3.3.</span> <span class="nav-text">spark的shuffle介绍？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#三种Writer的分类"><span class="nav-number">3.3.1.</span> <span class="nav-text">三种Writer的分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark的-partitioner-都有哪些"><span class="nav-number">3.4.</span> <span class="nav-text">Spark的 partitioner 都有哪些?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-有哪几种join？"><span class="nav-number">3.5.</span> <span class="nav-text">spark 有哪几种join？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD有哪些特点？"><span class="nav-number">3.6.</span> <span class="nav-text">RDD有哪些特点？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下宽依赖和窄依赖？"><span class="nav-number">3.7.</span> <span class="nav-text">讲一下宽依赖和窄依赖？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark中的算子都有哪些？"><span class="nav-number">3.8.</span> <span class="nav-text">Spark中的算子都有哪些？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Value数据类型的Transformation算子"><span class="nav-number">3.8.1.</span> <span class="nav-text">1. Value数据类型的Transformation算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Key-Value数据类型的Transfromation算子"><span class="nav-number">3.8.2.</span> <span class="nav-text">2. Key-Value数据类型的Transfromation算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Action算子"><span class="nav-number">3.8.3.</span> <span class="nav-text">3. Action算子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD的缓存级别都有哪些？"><span class="nav-number">3.9.</span> <span class="nav-text">RDD的缓存级别都有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-懒加载？"><span class="nav-number">3.10.</span> <span class="nav-text">RDD 懒加载？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的几种部署方式？"><span class="nav-number">3.11.</span> <span class="nav-text">spark的几种部署方式？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-on-yarn-模式下的-cluster模式和-client模式有什么区别？"><span class="nav-number">3.12.</span> <span class="nav-text">spark on yarn 模式下的 cluster模式和 client模式有什么区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark运行原理-从提交一个jar到最后返回结果-整个过程？"><span class="nav-number">3.13.</span> <span class="nav-text">spark运行原理,从提交一个jar到最后返回结果,整个过程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的stage是如何划分的？"><span class="nav-number">3.14.</span> <span class="nav-text">spark的stage是如何划分的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的rpc-spark2-0为什么放弃了akka-而用netty？"><span class="nav-number">3.15.</span> <span class="nav-text">spark的rpc: spark2.0为什么放弃了akka 而用netty？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的各种HA"><span class="nav-number">3.16.</span> <span class="nav-text">spark的各种HA?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Master异常"><span class="nav-number">3.16.1.</span> <span class="nav-text">Master异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker异常"><span class="nav-number">3.16.2.</span> <span class="nav-text">Worker异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Executor异常"><span class="nav-number">3.16.3.</span> <span class="nav-text">Executor异常</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark的内存管理机制-spark-1-6前后分析对比-spark2-0-做出来哪些优化？"><span class="nav-number">3.17.</span> <span class="nav-text">spark的内存管理机制,spark 1.6前后分析对比, spark2.0 做出来哪些优化？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6版本以前的问题"><span class="nav-number">3.17.1.</span> <span class="nav-text">1.6版本以前的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新方案的改进"><span class="nav-number">3.17.2.</span> <span class="nav-text">新方案的改进</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-中的广播变量？"><span class="nav-number">3.18.</span> <span class="nav-text">spark 中的广播变量？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题：为什么只能-broadcast-只读的变量？"><span class="nav-number">3.18.1.</span> <span class="nav-text">问题：为什么只能 broadcast 只读的变量？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#问题：broadcast-到节点而不是-broadcast-到每个-task？"><span class="nav-number">3.18.2.</span> <span class="nav-text">问题：broadcast 到节点而不是 broadcast 到每个 task？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#问题：-具体怎么用-broadcast？"><span class="nav-number">3.18.3.</span> <span class="nav-text">问题： 具体怎么用 broadcast？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#问题：怎么实现-broadcast？"><span class="nav-number">3.18.4.</span> <span class="nav-text">问题：怎么实现 broadcast？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#问题：broadcast-RDD-会怎样"><span class="nav-number">3.18.5.</span> <span class="nav-text">问题：broadcast RDD 会怎样?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussion"><span class="nav-number">3.18.6.</span> <span class="nav-text">Discussion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据倾斜-怎样去处理数据倾斜？"><span class="nav-number">3.19.</span> <span class="nav-text">数据倾斜,怎样去处理数据倾斜？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据倾斜主要分为两类-聚合倾斜-和-join倾斜"><span class="nav-number">3.19.1.</span> <span class="nav-text">数据倾斜主要分为两类: 聚合倾斜 和 join倾斜</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析一下一段spark代码中哪些部分在Driver端执行-哪些部分在Worker端执行？"><span class="nav-number">3.20.</span> <span class="nav-text">分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark经典程序"><span class="nav-number">3.21.</span> <span class="nav-text">Spark经典程序</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#wordcount"><span class="nav-number">3.21.1.</span> <span class="nav-text">wordcount</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pagerank"><span class="nav-number">3.21.2.</span> <span class="nav-text">pagerank</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TF-IDF"><span class="nav-number">3.21.3.</span> <span class="nav-text">TF-IDF</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flink"><span class="nav-number">4.</span> <span class="nav-text">Flink</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下flink的运行架构？"><span class="nav-number">4.1.</span> <span class="nav-text">讲一下flink的运行架构？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下flink的作业执行流程？"><span class="nav-number">4.2.</span> <span class="nav-text">讲一下flink的作业执行流程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink具体是如何实现exactly-once-语义？"><span class="nav-number">4.3.</span> <span class="nav-text">flink具体是如何实现exactly once 语义？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#状态-Exactly-Once"><span class="nav-number">4.3.1.</span> <span class="nav-text">状态 Exactly-Once</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#端到端（end-to-end）Exactly-Once"><span class="nav-number">4.3.2.</span> <span class="nav-text">端到端（end-to-end）Exactly-Once</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink-的-window-实现机制？"><span class="nav-number">4.4.</span> <span class="nav-text">flink 的 window 实现机制？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Window-的实现"><span class="nav-number">4.4.1.</span> <span class="nav-text">Window 的实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink的window分类？"><span class="nav-number">4.5.</span> <span class="nav-text">flink的window分类？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink-的-state-存储？"><span class="nav-number">4.6.</span> <span class="nav-text">flink 的 state 存储？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink是如何实现反压的？"><span class="nav-number">4.7.</span> <span class="nav-text">flink是如何实现反压的？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于-TCP-的反压"><span class="nav-number">4.7.1.</span> <span class="nav-text">基于 TCP 的反压</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于-Credit-的反压"><span class="nav-number">4.7.2.</span> <span class="nav-text">基于 Credit 的反压</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink的部署模式都有哪些？"><span class="nav-number">4.8.</span> <span class="nav-text">flink的部署模式都有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下flink-on-yarn的部署？"><span class="nav-number">4.9.</span> <span class="nav-text">讲一下flink on yarn的部署？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#yarn-session"><span class="nav-number">4.9.1.</span> <span class="nav-text">yarn session</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-run-Per-Job"><span class="nav-number">4.9.2.</span> <span class="nav-text">Flink run(Per-Job)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink中的时间概念-eventTime-和-processTime的区别？"><span class="nav-number">4.10.</span> <span class="nav-text">flink中的时间概念 , eventTime 和 processTime的区别？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Processing-Time"><span class="nav-number">4.10.1.</span> <span class="nav-text">Processing Time</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Event-Time"><span class="nav-number">4.10.2.</span> <span class="nav-text">Event Time</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ingestion-Time"><span class="nav-number">4.10.3.</span> <span class="nav-text">Ingestion Time</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink中的session-Window怎样使用？"><span class="nav-number">4.11.</span> <span class="nav-text">flink中的session Window怎样使用？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flink中的session-Window怎样使用"><span class="nav-number">4.12.</span> <span class="nav-text">flink中的session Window怎样使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HBase"><span class="nav-number">5.</span> <span class="nav-text">HBase</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hbase-架构"><span class="nav-number">5.1.</span> <span class="nav-text">Hbase 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase-如何设计-rowkey？"><span class="nav-number">5.2.</span> <span class="nav-text">hbase 如何设计 rowkey？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase的存储结构"><span class="nav-number">5.3.</span> <span class="nav-text">hbase的存储结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase的HA实现-zookeeper在其中的作用？"><span class="nav-number">5.4.</span> <span class="nav-text">hbase的HA实现,zookeeper在其中的作用？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HMaster宕机的时候-哪些操作还能正常工作？"><span class="nav-number">5.5.</span> <span class="nav-text">HMaster宕机的时候,哪些操作还能正常工作？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase的写数据的流程？"><span class="nav-number">5.6.</span> <span class="nav-text">hbase的写数据的流程？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase读数据的流程？"><span class="nav-number">5.7.</span> <span class="nav-text">hbase读数据的流程？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka"><span class="nav-number">6.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-的架构？"><span class="nav-number">6.1.</span> <span class="nav-text">kafka 的架构？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-与其他消息组件对比？"><span class="nav-number">6.2.</span> <span class="nav-text">kafka 与其他消息组件对比？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-实现高吞吐的原理？"><span class="nav-number">6.3.</span> <span class="nav-text">kafka 实现高吞吐的原理？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka怎样保证不重复消费？"><span class="nav-number">6.4.</span> <span class="nav-text">kafka怎样保证不重复消费？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka怎样保证不丢失消息？"><span class="nav-number">6.5.</span> <span class="nav-text">kafka怎样保证不丢失消息？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#消费端弄丢了数据"><span class="nav-number">6.5.1.</span> <span class="nav-text">消费端弄丢了数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-弄丢了数据"><span class="nav-number">6.5.2.</span> <span class="nav-text">Kafka 弄丢了数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生产者会不会弄丢数据？"><span class="nav-number">6.5.3.</span> <span class="nav-text">生产者会不会弄丢数据？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-与-spark-streaming-集成-如何保证-exactly-once-语义？"><span class="nav-number">6.6.</span> <span class="nav-text">kafka 与 spark streaming 集成,如何保证 exactly once 语义？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming上游对接kafka时保证Exactly-Once"><span class="nav-number">6.7.</span> <span class="nav-text">Spark Streaming上游对接kafka时保证Exactly Once</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming输出下游保证Exactly-once"><span class="nav-number">6.8.</span> <span class="nav-text">Spark Streaming输出下游保证Exactly once</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ack-有哪几种-生产中怎样选择？"><span class="nav-number">6.9.</span> <span class="nav-text">ack 有哪几种, 生产中怎样选择？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何通过-offset-寻找数据？"><span class="nav-number">6.10.</span> <span class="nav-text">如何通过 offset 寻找数据？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何清理过期数据？（删除-amp-压缩）"><span class="nav-number">6.11.</span> <span class="nav-text">如何清理过期数据？（删除&amp;压缩）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#删除"><span class="nav-number">6.11.1.</span> <span class="nav-text">删除</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#压缩"><span class="nav-number">6.11.2.</span> <span class="nav-text">压缩</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1条message中包含哪些信息？"><span class="nav-number">6.12.</span> <span class="nav-text">1条message中包含哪些信息？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下zookeeper在kafka中的作用？"><span class="nav-number">6.13.</span> <span class="nav-text">讲一下zookeeper在kafka中的作用？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#zk的作用主要有如下几点"><span class="nav-number">6.13.1.</span> <span class="nav-text">zk的作用主要有如下几点:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-可以脱离-zookeeper-单独使用吗？"><span class="nav-number">6.14.</span> <span class="nav-text">kafka 可以脱离 zookeeper 单独使用吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka有几种数据保留策略？"><span class="nav-number">6.15.</span> <span class="nav-text">kafka有几种数据保留策略？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka同时设置了7天和10G清除数据-到第5天的时候消息到达了10G-这个时候kafka如何处理？"><span class="nav-number">6.16.</span> <span class="nav-text">kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Zookeeper"><span class="nav-number">7.</span> <span class="nav-text">Zookeeper</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zookeeper是什么-都有哪些功能？"><span class="nav-number">7.1.</span> <span class="nav-text">zookeeper是什么,都有哪些功能？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zk-有几种部署模式？"><span class="nav-number">7.2.</span> <span class="nav-text">zk 有几种部署模式？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zk-是怎样保证主从节点的状态同步？"><span class="nav-number">7.3.</span> <span class="nav-text">zk 是怎样保证主从节点的状态同步？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#说一下-zk-的通知机制？"><span class="nav-number">7.4.</span> <span class="nav-text">说一下 zk 的通知机制？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zk-的分布式锁实现方式？"><span class="nav-number">7.5.</span> <span class="nav-text">zk 的分布式锁实现方式？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分布式一致性协议"><span class="nav-number">7.6.</span> <span class="nav-text">分布式一致性协议</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#两阶段提交协议-2PC"><span class="nav-number">7.6.1.</span> <span class="nav-text">两阶段提交协议(2PC)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三阶段提交协议-3PC"><span class="nav-number">7.6.2.</span> <span class="nav-text">三阶段提交协议(3PC)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#向量时钟"><span class="nav-number">7.6.3.</span> <span class="nav-text">向量时钟</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NWR协议"><span class="nav-number">7.6.4.</span> <span class="nav-text">NWR协议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#paxos协议"><span class="nav-number">7.6.5.</span> <span class="nav-text">paxos协议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Raft协议"><span class="nav-number">7.6.6.</span> <span class="nav-text">Raft协议</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#讲一下leader-选举过程？"><span class="nav-number">7.7.</span> <span class="nav-text">讲一下leader 选举过程？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#引用"><span class="nav-number">8.</span> <span class="nav-text">引用</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        

<script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>


<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袤锅</span>

  

  
</div>





  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>




  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Mist</a> v6.2.0</div>





        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.2.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.2.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.2.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.2.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.2.0"></script>



  



	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  


  
  

  

  

  

  

  


  
  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>

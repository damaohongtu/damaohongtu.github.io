<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[干好工作18法]]></title>
    <url>%2F2030%2F07%2F18%2F%E5%B9%B2%E5%A5%BD%E5%B7%A5%E4%BD%9C18%E6%B3%95%2F</url>
    <content type="text"><![CDATA[copy 自微信公众号“泽平宏观”，“泽平宏观”copy自网络 方法是为达到某种目标而采取的途径、步骤、手段等，是人类认识和改造客观世界的明灯和路标。方法十分重要，方法得当事半功倍，方法失当事倍功半。 方向正确以后，方法便为王。工作中只有掌握科学的工作方法，才能确保高效，圆满完成各项任务，提升工作的境界和水平。 不要差不多，盯住完美 长计划、短安排、立即做 日清月结，有条不紊 学习工作化，工作学习化 注重积累，始终在研究的状态下工作 信息要对称，善于沟通 分工不分家，主动补台 执行有力，反馈及时 跳出自身看自身，立足自己看自己 事情要一桩桩做 想问题，做事情要尽肯能合情合理 自觉按照职能职责做事，永远忠于职守 分清轻重缓急，抓本质，抓重点，抓关键 掌握特点，把握规律 始终保持适度的紧张感 不多事，不误事，不坏事 急事缓办，缓事急办 务实和务虚结合 一、不要差不多，盯住最完美“差不多”是我们平时常说的一句口头语。很多人学习上一知半解、浅尝辄止；工作中只求过得去，不求过得硬，满足于应付了事；生活中粗心大意、随意邋遢等等，其实都是“差不多”心理使然。“差不多”心态看似没有什么大碍，但是若干个小的“差不多”，集中起来就会导致“差很多”，1%的疏漏往往会造成100%的错误，正所谓差之毫厘、谬以千里，上错一点、下错一片，长期下去对工作对事业不利、对自身成长不利、对单位形象也不利。其实质是一个态度问题，与能力基本无关，但与一个人的品行、性格、习惯有关。 鲁迅先生曾专门批评过“马马虎虎”现象，胡适先生还写过一篇寓言故事叫《差不多先生》，这位“差不多先生”十字常常写成千字，千字常常写成十字，最终因为找错医生而一命呜呼。故事虽然滑稽可笑，但其处事方式，至今仍是不少人的写照。世界上的事最需要“认真”，也最怕“认真”，所以一定要强化精品意识、细节意识，时刻拥有“没有最好，只有更好”的理念，养成严肃、严格、严谨地对待工作的习惯，绝不忽视任何一个细节，绝不放过任何一个疑点，要做就把一件事做到极致，把“严细实”要求贯穿办文、办会、办事的全过程，切实做到“文经我手无差错、事交我办请放心”，自觉杜绝“差不多”，追求最完美。 二、长计划、短安排、立即做长计划，就是说要着眼明天、着眼未来、着眼长远。凡事预则立，不预则废。一个对人生和工作有计划的人，他就能胸怀大局、放眼长远，不为一时一地的不利所困。俗话说得好，愚者赚今朝，智者赚明天。有人说，工作天天短平快、年年马拉松。其实这其中也要有一个科学合理的长期计划的问题，切忌贪一时之功、图一时之名，而要脚踏实地、从长计议。 当然，不积跬步无以致千里，光有长期计划还不够，还要善于将其具体化、阶段化，也就是要有短安排，从细从实，每天给自己制定一个小的目标，计划好今天要完成的事情，这样不仅可以知道每天要做些什么、做了些什么，还可以对工作进行有效控制，让每一个小目标、短安排的成绩，都成为成功路上的阶梯和里程碑。 不管是长计划还是短安排，都要立即行动、马上就办，将工作落到实处，否则都只是一句空话。这样坚持一段时间，就会发现，计划的工作都能如期完成，工作效果也会非常明显，工作给我们带来的快乐也容易获得。只有做到了长计划、短安排，才能真正实现有序、有效；也只有把“立即做”当成自己的座右铭，并形成习惯，才能不断进步。 三、日清月结，有条不紊所谓“日清月结”，是指办理现金出纳业务必须按日清理、按月结账。它原本是一个财务术语，运用到工作当中，就是要“今日事，今日毕”，每过一段时间就及时“回头看”，检查审视一下自己的工作，确保任务不拖延、事情不遗漏。如果今日事明日做，那一定是“日日待明日，明日何其多”，工作就永远拎不清、无章法、效果差。有些人责任心不强，工作没有规划目标，任务稍重一点，就有畏难情绪，找理由拖延，觉得今天做不完的，明天还可以接着做。殊不知今天的事情做不完，明天的事情也会做不完，“躲过了初一，躲不过十五”，在拖延中自己并不快乐，反而会累积许多压力，严重影响工作效率，甚至导致自信心下降。 所谓“有条不紊”，就是说话做事有条有理，不打乱仗，这是一个非常重要的习惯，甚至影响一个人的成功和发展。工作往往人少事多，如果缺乏条理性，就会忙乱而效率不高、效果不好。做好日清月结，要统筹规划、有条不紊，不轻视怠慢眼前和当下的工作，把今天该做的做好，把明天要做的计划好、准备好，努力做到事不过夜、案无积卷，从容不迫、井然有序地应对复杂工作。 四、学习工作化，工作学习化所谓“学习工作化，工作学习化”，就是要在学中干、在干中学，两手抓、两不误、两促进。事实反复告诉我们，学习力的高低是人与人之间拉开距离的重要因素。一个人只要做到坚持学习、善于学习、快速地学习，就一定会有所成就。当然，学习既是一件快乐的事，也是一件苦差事，再加上工作任务繁重，如何长久地保持学习激情、提高工作效率，其奥妙就是在学的过程中结合工作，在干的过程中感悟学习，互相启发促进，自然而然就会在心中不断迸发出热情的火花。有人认为，一个人一生中90%的学习都是在工作中实现的，这是有一定道理的。 大家应该把学习作为一种精神追求、一种工作状态、一种生活方式，下得苦功夫，求得真学问。要树立“不学习无以立”的意识，坚持向书本学习、向实践学习，边学边用，边用边学，在学习与工作的良性互动中不断增强本领，超越自我，不能把两者对立起来，搞成“两张皮”。 五、注重积累，始终在研究状态下工作合抱之木，生于毫末；百丈之台，起于垒土。任何事物都要有量的积累才有可能发生质的变化，但也不是说积累越多就越好，如果没有研究和思考，积累也只是把一堆东西堆砌在一起，做一个“储物柜”而已，工作、学习都是如此。每个人都有自身的职能职责，要想做好工作，必须能沉下身来、静下心来、置身事中，广泛了解自身工作所涉及的业务知识，做到底数清、情况明，成为本职工作的通才。有了一定的知识积累，还要注重调查研究，就是用脑子干活做事，不仅苦干实干，还要巧干会干。在面临许多新情况、新问题、新矛盾时，必须在研究状态下工作，提出解决问题的点子、办法，这就要求我们要领会上级的精神、吃透书本的理论、借鉴别人的经验，结合工作实际，身在事之中、心在事之上，把握大局，多谋善断、敢于拍板决策。要强化问题导向、目标导向和效果导向，知行合一，日积月累，就能不断提升自身的专业素养、专业方法、专业能力，就能成为一个专家、成为工作的行家里手。 六、信息要对称，善于沟通现在是信息时代，信息对称、沟通及时十分重要。每个领导干部要有及时获取信息的能力，要有总结归纳信息的能力，要努力成为提供相关信息的源头，要重视沟通协调。沟通是人与人之间思想和信息的交换，是人类集体活动的基础。世界上的事情，都需要沟通，沟通是人们必备的基本能力。及时有效的沟通，才能达成协调一致的意见、形成步调统一的行动。 信息对称是做好自身工作、提高工作水平一个很重要的要素。对自己所从事的工作，要了解上面的要求、左右的情况、下面的进展，就要增强主动沟通的意识，确保上情准确下达、下情及时上传，着力构建上下贯通、左右衔接、内外一体、立体交叉的运转体系，实现各方面工作无缝对接，形成“整体一盘棋，同唱一台戏”的良好格局。 向上沟通要及时，该请示的请示，该报告的报告，既要提出问题，还要给出意见建议，上级决定了的事项要全力落实；同级沟通要真诚，互相尊重，换位思考，积极配合，不设障碍；对下沟通要体谅，不能蛮横霸道，颐指气使，要准确了解下属的优点和长处，有针对性地安排部署工作，关心关爱下属，增强亲和力、凝聚力。 七、分工不分家，主动补台同心山成玉，协力土变金。刘邦、张良、萧何、韩信相互协作补台才有了大汉天下，廉颇、蔺相如“将相和”才有了赵国的祥和稳定。团队是一个集体，团结协作、主动补台不只是一种工作方法，更是一种品行操守、一种胸怀胸襟。互相补台，好戏连台；互相拆台，一起垮台。工作中有人补台，就可能避免错误，或是将损失降到最低，若是各人自扫门前雪，不管他人瓦上霜，站在城楼看风景，结果“城门失火，殃及池鱼”，一荣俱荣，一损俱损。 很多工作不是哪一个部门能单独完成的，同一个部门的工作也不是哪一个人能单独完成的，没有谁可以包打天下。要做到分工不分家，既提高个人单兵作战能力，也提高团队的整体作战能力，超越个体认知和个体力量的局限，发挥1+1&gt;2的效果。班子成员之间、部门之间、同事之间，要重视互相补台，还要善于补台。帮别人补台，当无名英雄，时间久了，大家终会认清你的为人，最后都愿意为你补台。当然，补台也不是说毫无主见的盲从，更重要的是发现问题和不足，大胆提出意见，修正决策，不断完善；补台更不是毫无原则的迁就，对涉及个人利益的小事要讲风格，至于事关原则性的问题，则要敢于“拆台”，这样的拆台恰恰是为大局更好地补台。 八、执行有力，反馈及时一个行动胜过一打纲领。对大多数人而言，执行力是第一位的能力。提高执行力，要有强烈的责任感和进取心，要有从小事做起、从点滴做起的实干精神，要有较强的工作能力，要有健全的制度规则作保障，更要有及时反馈的“复命意识”和“划句号”的能力。事毕不回复，就像任务完成了99%，只有这1%没落实，虽然就差这么一丁点，事情却没有到位。实际工作中，绝不能搞先斩后奏、边斩边奏、甚至斩而不奏，也不能等任务全部完成了才反馈，应该注意适时反馈、阶段性反馈，一方面可以让领导和同事放心，另一方面及时反馈情况又能为正确决策提供依据，特别是执行中遇到困难、发现问题时更需要及时反馈，以便重新调整思路和办法，从而更好地化解矛盾、解决问题。只要是和岗位职责有关的事，都要及时反馈，做到凡事有交代、件件有着落、事事有回音。 九、跳出自身看自身，立足自己看自己人生是一个不断认识自我、完善自我的过程，一个人不可能用自己的眼睛完全看清自己，不识庐山真面目，只缘身在此山中。有些人经常自我感觉良好、夜郎自大，就是把自己局限在了一个较为狭窄的时空内，殊不知天外有天、人外有人。认清自我最好的办法就是“跳出自身看自身”。要学会登高望远，放开视野去比较，在一览众山小中看清自己的位置、自己的渺小，看到别人看不到的；要学会用“第三只眼”看自身，不以自我为中心，用旁观者的心态，高出事物的一两个层次来审视自己；要学会以人为镜，见贤思齐，照出自己的差距和不足，明确方向和目标。另一方面，又要找准自己的定位，立足此时此地的人生思考问题，扬长避短，不纠结过往，不忧心未来，做好当下正在做的事，过好眼前的生活，立足自己看自己，这是一切工作的原点。要善于正确认识自己，不能听了几句表扬就妄自尊大、自以为是，也不能挨了几句批评就妄自菲薄、自我否定；要能够搞清楚现状是什么样，未来要怎么发展，吃透上情，了解下情，把自己的潜能发挥到最佳状态，干一行爱一行精一行，活在当下，做好自己。跳出自身看自身，可以看得更加明白；立足自己看自己，能够走得更加顺畅。 十、事情要一桩一桩地做人们常说，饭要一口一口地吃，日子要一天一天地过，文章要一篇一篇地写，事情要一桩一桩地做。这些都是大白话、大实话，富有哲理。“心急吃不了热豆腐”“一口吃不成大胖子”，我们很多工作都不是一朝一夕、一蹴而就的，有的需要好几年甚至好几届领导班子传承接力，不能急于求成、心浮气躁。明朝吕坤在《续小儿语》中说：“大凡做一件事，就要当一件事；若还苟且粗疏，定不成一件事。”就是说无论什么事情，要取得实效、赢得胜利，都不能东一榔头西一棒槌、打一枪换一个地方，一阵风、不落实，更不能脚踩西瓜皮——滑到哪里算哪里。 部门工作事务繁杂，既要会总体把握、分步实施、统筹推进，更要发扬钉钉子精神，一件事不做则已，做必做到底，做到最后胜利。不能三心二意，猴子掰棒子，抓一个丢一个，如果这样就什么事也做不成。要咬定青山不放松，一茬接着一茬干，做好做透做实每一件事，用足够的耐心和韧劲来面对工作生活，不折腾、不反复，久久为功、绵绵用力、一抓到底，积小胜为大胜。 十一、想问题、做事情要尽可能合情合理人有人情，物有物理，合情合理就是要合乎情理、合乎原则，两者兼顾。现实社会中，有时候合情不一定合理，合理不一定合情。中国自古就是人情社会，“投桃报李”“滴恩泉报”“千里送鹅毛，礼轻情义重”等传统观念在人们的头脑中根深蒂固。人不可能生活在真空里、没有感情，肯定也要讲人情，不近人情的人是缺少情商、缺少魅力和感召力的。但是，人情也有其世俗庸俗乃至功利丑恶的一面，只讲人情不讲原则的人，颠倒人情与原则的关系，丧失自己的立场和原则，迟早要“栽跟头”。 当然，情与理也不是完全对立的，一个人既不能太死板机械，更不能太圆滑世故，尤其面对重大利益和重要人事问题，要自觉和严密地设置人情防火墙，自己不去突破，也严防别人逾越。要重视人际关系，但不可刻意去追求搞好人际关系，要学会以简单对复杂，别人复杂，自己要简单。要同情弱者有善心，努力做到情理兼顾，在不好兼顾的情况下，坚持原则就是最好的选择，也是唯一的选择，只有这样才能保全自己，也才能做到“自己不打倒自己，别人永远打不倒你”。 十二、自觉按职能职责做事，永远忠于职守处在什么岗位就要履行什么职责，岗位就是责任，职务就是责任。不管是在公司部门还是在各自的家庭里，每个人都有一定的职能职责，这是我们做事的前提和方向，方向不对，努力白费。明确了职能职责，更要忠于职守，就是忠诚地担起自己的岗位责任和职责操守，时刻提醒自己工作就意味着责任，这是所有职业规范的基本要求。每个人首先要熟悉自己工作岗位的职能职责，对自己职能职责范围内的事，都要主动地去做，尽心尽力地去做，千万不要事事等领导来安排，那不是一个好员工。其实，每个人都做好了自己的本职工作，当好一个“循吏”，那也是大贡献。当然，也要有更高的追求，要努力做个“能吏”，发扬“职业精神”“工匠精神”，以强烈的事业心和责任感来对待工作，自觉做到“有信念，讲规矩、有纪律，讲道德、有品行，讲奉献、有作为”，苦累面前多思得，工作当中多思责，“专心致志，以事其业”，做到在其位、谋其政、负其责、尽其力，干大活、出新彩、干出水平。 十三、分清轻重缓急，抓本质、抓重点、抓关键轻重缓急是指各项工作有主要的和次要的，有急于要办的和可以慢一点办的。面对纷繁复杂的工作，要学会运用辩证法，善于“弹钢琴”，把最重要最紧迫的放在第一位，不太重要不太紧迫的放在第二位，依此类推，分出轻重缓急。要善于紧盯大事要事打攻坚战、紧盯急事难事打歼灭战、紧盯薄弱环节打持久战，牢牢把控工作节奏、力度和质量，善于抓本质抓重点抓关键，切实做到“打鼓打到重心处、工作抓到要害上”。抓本质，就是要善于透过现象看本质，知其然更要知其所以然，客观全面、深刻系统、辩证历史地看问题，坚持打破砂锅问到底，深挖细查，为工作打牢基础。抓重点，就是要抓主要矛盾和矛盾的主要方面，始终能分清主次、合理布局工作力量，以重点带动一般，不平均用力、撒胡椒面，不“眉毛胡子一把抓”，一把抓不如抓一把，都想满把抓反倒都抓不住，要把好钢用在刀刃上。抓关键，就是要把握关键少数，掌控关键环节，认准关键时机，“射人先射马，擒贼先擒王”，牵牛牵住牛鼻子，打蛇打到七寸上，牢牢把握工作主动权，集中精力，扭住不放，持续用力，善作善成。 十四、掌握特点，把握规律特点是事物本身所具有的特别之处，任何事物都有其自身与众不同的特性，也就是个性。但无论事物如何复杂、如何变化，其背后都存在着必然的内在联系和发展趋势，这就是规律，也可以称之为共性。规律具有稳定性、重复性、普遍性和客观性，我们不能任意创造和改变，但可以认识和利用它来改造自然、改造社会、造福人类。实践证明，善于把握规律，才能认识事物发展的轨迹和趋势，形成分析问题、解决问题的新思路和新举措。倘若没有掌握特点、把握规律，将会是盲目推进，不得要领，甚至事与愿违。工作同样如此，不同的部门和岗位都有其自身特点和规律，要想得心应手、从容应对、体现水平，就要掌握特点、把握规律。 首先要正视每一件工作的个体性，以高度的责任感和严谨的工作态度，认真对待，全面分析，合理施策，有针对性地把每一个问题解决好、每一项工作落实好。同时，我们所从事的工作和岗位也有共同的规律，不能拍脑袋决策，否则就要吃苦头，造成损失。要善于把平时零碎、肤浅、表面的感性认识，上升为全面、系统、本质的理性认识，使思想和行动既不落后也不超越于客观实际，增强认识规律、找准规律、把握规律的能力，提高运用规律的水平，从而在具体工作中增强主动性和有效性。 十五、始终保持适度的紧张感生命是需要永远激活的，天有日月星，人有精气神，工作必须在状态。对每一个人来说，压力太大会崩溃，但没有一定的压力，不保持适度的紧张感，对身体、对生命、对工作都是负能量。鲁迅先生有句名言：“生活太安逸了，工作就会被生活所累。”青蛙在温水里待得太久，就会跳不出来；人如果太闲适，就容易生出事、干坏事。井无压力不出油，人无压力轻飘飘，有压力不一定是坏事，适度的紧张感对于一个单位、一个团队、一个组织、一个人的健康等方方面面都有好处，它能使我们不忘“初心”，远离职业倦怠，激发工作热情，始终让思维和行动保持在平均水准以上，甚至可以迸发出超出想象的能力。工作任务重、节奏快，更要人在岗上、岗在心上，时刻警醒、积极适应、快速跟进，在落细落小落实上下功夫，以“一日不为，三日不安”的责任感和“时不我待，只争朝夕”的紧迫感，一心一意谋工作、干事业，推动创新发展。当然，也不能过于紧张，要劳逸结合，生动活泼，正确面对工作、生活、人际关系等多方面的压力，避免造成心理失衡和精神压抑。 十六、不多事、不误事、不坏事不多事、不误事、不坏事，看似简单，要求不高，实则蕴含深刻。“不多事”就是尽好本职、守好本分，看好自己的门，走好自己的路，做好自己的事，不该看的不看，不该问的不问，不该说的不说，不该做的不做，不越位抢球，不无事生非，做老实人不吃亏。知事晓事不多事，就会太平无事。“不误事”就是勇于担当、敢于负责，坚持高标准、严要求，把岗位作为锻炼自己的舞台，认真履职尽责，把工作作为展示自己才能的载体，这是一种牢记使命、敬畏岗位的责任自觉。也只有责任才能使一个人坚持和长久。“不坏事”就是走得端、行得正，不坏别人的事、不坏大家的事、不坏集体的事。现在职场里，有些人名利思想作怪、价值取向错位、红眼病严重，喜欢搬弄是非、挑拨离间、混淆黑白，他们不是跟事过不去，而是跟人过不去、跟心过不去。岂不知，举头三尺有神明，多行不义必自毙。要有阳光心态，光明磊落，坦坦荡荡，与人为善。 十七、急事缓办、缓事急办所谓“急事”往往是突发事件、紧急事件、影响全局的事，让人措手不及，无法选择、不能回避。人的一生，遇急遇险在所难免，能够坦然面对，急事缓办、缓事急办才是大智慧。急事急办可能会忙中出错，急上加急就会错漏百出，难以弥补。急中生智也是有的，但可遇不可求，急中生智是超常态，急中失措才是常态，所以说很多急事反而急不得。急事缓办体现的是一个人沉着冷静、深思熟虑的智慧、勇气和应急能力，遇到急事要赶不要急，应当冷静思考、从容应对，不急于表态，不随便答复，考虑周全后再去妥善办理。 所谓“缓事”，是指常规性、日常性事务或者预先知道需要做的事，都是你必须做的职责内的事，类似统计报表、会议纪要、旬报月报等等，有的人往往认为这个事是一个周或者一个月以后的事，现在不用着急，以后再说，最后缓事都变成了急事，时间到了就措手不及，弄得一团糟。缓事急办显示的是一个人的工作态度、工作的计划性和条理性，对缓事要有计划，抽空及时做，不要拖延，要事先安排，以免临时抱佛脚，忙乱而又得不到好结果。 十八、务虚与务实相结合务实是指脚踏实地，从实际出发，说实话、办实事、想实招、求实效；务虚则指仔细分析，深入研究，搞清楚为什么做、做什么、怎么做。如果说务实是决胜千里之外的实践，务虚就是运筹帷幄之中的谋划。一般来说，人们比较警惕务虚不务实的毛病，却不大重视只务实而不善或不会务虚的做法，如果过分强调“埋头拉车”，忽视“抬头看路”，就会陷入事无巨细、疲于奔命的困境，工作就很难突破和提高。孙悟空随唐僧去西天取经，经历九九八十一难，每渡过一难，他都会腾云驾雾到空中，看看妖魔鬼怪在何方，并思考如何应对，这其实就是务虚；如果发现妖怪，那就跳下来打，这就是务实。没有务虚，务实就没有方向性，所务之“实”就可能是一种盲动或蛮干；没有务实，目标计划则都停留在想象阶段，一切就都是空想。务虚是为了更好地务实，要务好实必须务好虚。每个人要在务实中生存，更要在务虚中提升，不能借口真抓实干否定务虚的重要性，错误地认为务虚就是空谈道理、只说不干，更不能借口务虚，不干实事、不求实效、坐而论道。既要真抓实干、求真务实，又要善于谋划、注重总结提升。一定要正确理解务虚与务实的关系，将二者有机结合，相互促进，相辅相成，不断提高工作水平。]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大袤锅架构师之路]]></title>
    <url>%2F2021%2F07%2F18%2F%E5%A4%A7%E8%A2%A4%E9%94%85%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[大袤锅架构师之路，设计稿（更新ing）]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis答疑解惑]]></title>
    <url>%2F2021%2F03%2F28%2FRedis%E7%AD%94%E7%96%91%E8%A7%A3%E6%83%91%2F</url>
    <content type="text"><![CDATA[​ Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。本文从存储、基础数据结构、基础理论、使用以及实际应用场景对Redis进行详细解析。（重度引用老钱@公众号【码洞】、《Redis设计与实现》） 1. 存储1.1 字符串SDS（Simple Dynamic String）1.1.1 内部结构123456struct SDS&lt;T&gt; &#123; T capacity; // 数组容量 T len; // 数组长度 byte flags; // 特殊标识位，不理睬它 byte[] content; // 数组内容&#125; 上面的 SDS 结构使用了范型 T，为什么不直接用 int 呢，这是因为当字符串比较短时，len 和 capacity 可以使用 byte 和 short 来表示，Redis 为了对内存做极致的优化，不同长度的字符串使用不同的结构体来表示。 前面我们提到字符串是可以修改的字符串，它要支持 append 操作。如果数组没有冗余空间，那么追加操作必然涉及到分配新数组，然后将旧内容复制过来，再 append 新内容。如果字符串的长度非常长，这样的内存分配和复制开销就会非常大。因此可以说取出来拼接后再存储开销巨大，某工作5年+的人竟然推荐我存list的新加元素的时候先取出来拼接成字符串再存回去！！！这里就知道是有多SB的想法。 SDS除了保存数据库的之外，还被用来做缓冲区使用！！！ 与c字符串不同，SDS完全杜绝了内存溢出的问题。 减少字符串扩充带来的内存分配次数，（SDS扩容的时候可能会多分配一些空间，即空间预分配，另外在释放内存的时候采用惰性释放）； 1.1.2 embstr vs rawRedis 的字符串有两种存储方式，在长度特别短时，使用 emb 形式存储 (embeded)，当长度超过 44 时，使用 raw 形式存储。首先来了解一下 Redis 对象头结构体，所有的 Redis 对象都有下面的这个结构头: 1234567struct RedisObject &#123; int4 type; // 4bits int4 encoding; // 4bits int24 lru; // 24bits int32 refcount; // 4bytes void *ptr; // 8bytes，64-bit system&#125; robj; 不同的对象具有不同的类型 type(4bit)，同一个类型的 type 会有不同的存储形式 encoding(4bit)，为了记录对象的 LRU 信息，使用了 24 个 bit 来记录 LRU 信息。每个对象都有个引用计数，当引用计数为零时，对象就会被销毁，内存被回收。ptr 指针将指向对象内容 (body) 的具体存储位置。这样一个 RedisObject 对象头需要占据 16 字节的存储空间。 embstr 存储形式是这样一种存储形式，它将 RedisObject 对象头和 SDS 对象连续存在一起，使用 malloc 方法一次分配。而 raw 存储形式不一样，它需要两次 malloc，两个对象头在内存地址上一般是不连续的。 1.1.3 embstr最大容纳的长度为啥是44 RedisObject对象头占据16个字节 SDS对象头至少3个字节 内存分配器 jemalloc/tcmalloc 等分配内存大小的单位都是 2、4、8、16、32、64。为了能容纳一个完整的 embstr 对象，jemalloc 最少会分配 32 字节的空间，如果字符串再稍微长一点，那就是 64 字节的空间。如果总体超出了 64 字节，Redis 认为它是一个大字符串，不再使用 emdstr 形式存储，而该用 raw 形式。 留给 content 的长度最多只有 45(64-19) 字节了。字符串又是以\0结尾，所以 embstr 最大能容纳的字符串长度就是 44。 1.1.4 扩容字符串在长度小于 1M 之前，扩容空间采用加倍策略，也就是保留 100% 的冗余空间。当长度超过 1M 之后，为了避免加倍后的冗余空间过大而导致浪费，每次扩容只会多分配 1M 大小的冗余空间。 1.2 dict除了hash结构使用了字典（dict）外，整个Redis数据库中所有的key和value也组成了一个全局的字典。 1.2.1 内部结构dict 结构内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的。但是在 dict 扩容缩容时，需要分配新的 hashtable，然后进行渐进式搬迁，这时候两个 hashtable 存储的分别是旧的 hashtable 和新的 hashtable。待搬迁结束后，旧的 hashtable 被删除，新的 hashtable 取而代之。 1.2.1 Rehash1.2.1.1 Rehash步骤 计算负载因子 负载因子=哈希表已经保存的节点数量/hash表的大小 load_factor =ht[0].used / ht[0].size 为字典的ht1哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量，也就是ht[0].used属性的值： 如果当前执行的是扩展操作，那么ht1的大小为第一个大于等于ht[0].used*2的2^n. 如果执行的是收缩操作，那么ht1的大小为第一个大于等于ht[0].used的2^n. 将保存在ht[0]中的所有键值对rehash到ht1上面：rehash指的是重新计算键的hash值和索引值，然后键键值对放置到ht1哈希表的指定位置上 当ht[0]包含的所有键值对都迁移到ht1之后，ht[0]变成空表，释放ht[0]，将ht1设置为ht[0],并在ht1新创建一个空白的哈希表，为下一次rehash做准备。 1.2.1.2 渐进式hash​ 大字典的扩容是比较耗时间的，需要重新申请新的数组，然后将旧字典所有链表中的元素重新挂接到新的数组下面，这是一个O(n)级别的操作，作为单线程的Redis表示很难承受这样耗时的过程。步子迈大了会扯着蛋，所以Redis使用渐进式rehash小步搬迁。虽然慢一点，但是肯定可以搬完。搬迁操作埋伏在当前字典的后续指令中(来自客户端的hset/hdel指令等)，但是有可能客户端闲下来了，没有了后续指令来触发这个搬迁，那么Redis就置之不理了么？当然不会，优雅的Redis怎么可能设计的这样潦草。Redis还会在定时任务中对字典进行主动搬迁。 1.2.2 查找过程 hashtable的元素在第二维的链表上，首先需要确定元素在哪一个链表上。 1.2.3 扩容条件 正常情况下，当 hash 表中元素的个数等于第一维数组的长度时，就会开始扩容，扩容的新数组是原数组大小的 2 倍。不过如果 Redis 正在做 bgsave，为了减少内存页的过多分离 (Copy On Write)，Redis 尽量不去扩容 (dict_can_resize)。 但是如果 hash 表已经非常满了，元素的个数已经达到了第一维数组长度的 5 倍 (dict_force_resize_ratio)，说明 hash 表已经过于拥挤了，这个时候就会强制扩容。 1.2.4 缩容条件当 hash 表因为元素的逐渐删除变得越来越稀疏时，Redis 会对 hash 表进行缩容来减少 hash 表的第一维数组空间占用。缩容的条件是元素个数低于数组长度的 10%。缩容不会考虑 Redis 是否正在做 bgsave。 1.3 ziplist Redis 为了节约内存空间使用，zset 和 hash 容器对象在元素个数较少的时候，采用压缩列表 (ziplist) 进行存储。压缩列表是一块连续的内存空间，元素之间紧挨着存储，没有任何冗余空隙。 1234567struct ziplist&lt;T&gt; &#123; int32 zlbytes; // 整个压缩列表占用字节数 int32 zltail_offset; // 最后一个元素距离压缩列表起始位置的偏移量，用于快速定位到最后一个节点 int16 zllength; // 元素个数 T[] entries; // 元素内容列表，挨个挨个紧凑存储 int8 zlend; // 标志压缩列表的结束，值恒为 0xFF&#125; 压缩列表为了支持双向遍历，所以才会有 ztail_offset 这个字段，用来快速定位到最后一个元素，然后倒着遍历。 1.3.1 添加元素因为 ziplist 都是紧凑存储，没有冗余空间 (对比一下 Redis 的字符串结构)。意味着插入一个新的元素就需要调用 realloc 扩展内存。取决于内存分配器算法和当前的 ziplist 内存大小，realloc 可能会重新分配新的内存空间，并将之前的内容一次性拷贝到新的地址，也可能在原有的地址上进行扩展，这时就不需要进行旧内容的内存拷贝。如果 ziplist 占据内存太大，重新分配内存和拷贝内存就会有很大的消耗。所以 ziplist 不适合存储大型字符串，存储的元素也不宜过多。 1.4 intset当set集合容纳的元素都是整数并且元素个数较少的时候，Redis会使用intset来存储结合元素。intset 是紧凑的数组结构，同时支持 16 位、32 位和 64 位整数。 12345struct intset&lt;T&gt; &#123; int32 encoding; // 决定整数位宽是 16 位、32 位还是 64 位 int32 length; // 元素个数 int&lt;T&gt; contents; // 整数数组，可以是 16 位、32 位和 64 位&#125; 1.5 linkedlist1.6 quicklist 考虑链表的附加空间相对太高，prev和next指针就要占据16个字节（64位系统为8个字节），另外每一个节点的内存是单独存储的，加剧了内存的碎片化，影响内存的效率，在后续的版本中，使用quicklist代替了ziplist和linkedlist。 quicklist是ziplist和linkedlist的混合体，它将linkedlist按段切分，每一段使用ziplist来紧凑存储，多个ziplist之间使用双向指针连接。 123456789101112131415161718192021222324struct ziplist &#123; ...&#125;struct ziplist_compressed &#123; int32 size; byte[] compressed_data;&#125;struct quicklistNode &#123; quicklistNode* prev; quicklistNode* next; ziplist* zl; // 指向压缩列表 int32 size; // ziplist 的字节总数 int16 count; // ziplist 中的元素数量 int2 encoding; // 存储形式 2bit，原生字节数组还是 LZF 压缩存储 ...&#125;struct quicklist &#123; quicklistNode* head; quicklistNode* tail; long count; // 元素总数 int nodes; // ziplist 节点的个数 int compressDepth; // LZF 算法压缩深度 ...&#125; 1.7 跳跃表（skiplist）​ Redis 的 zset 是一个复合结构，一方面它需要一个 hash 结构来存储 value 和 score 的对应关系，另一方面需要提供按照 score 来排序的功能，还需要能够指定 score 的范围来获取 value 列表的功能，这就需要另外一个结构「跳跃列表」。zset 的内部实现是一个 hash 字典加一个跳跃列表 (skiplist)。hash 结构在讲字典结构时已经详细分析过了，它很类似于 Java 语言中的 HashMap 结构。本节我们来讲跳跃列表，它比较复杂，读者要有心理准备。 http://zhangtielei.com/posts/blog-redis-skiplist.html https://www.bilibili.com/video/BV1Q4411S76S?from=search&amp;seid=13428887929805854246 skiplist的演示动画见这里。 设想如果跳跃列表只有一层会怎样？插入删除操作需要定位到相应的位置节点 (定位到最后一个比「我」小的元素，也就是第一个比「我」大的元素的前一个)，定位的效率肯定比较差，复杂度将会是 O(n)，因为需要挨个遍历。也许你会想到二分查找，但是二分查找的结构只能是有序数组。跳跃列表有了多层结构之后，这个定位的算法复杂度将会降到 O(lg(n))。 1234567891011121314151617181920212223typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[];&#125; zskiplistNode; 123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level;&#125; zskiplist; 1.7.1 插入 随机层数：对于每一个新插入的节点，都需要调用一个随机算法给它分配一个合理的层数。直观上期望的目标是 50% 的 Level1，25% 的 Level2，12.5% 的 Level3，一直到最顶层2^-63，因为这里每一层的晋升概率是 50%。不过 Redis 标准源码中的晋升概率只有 25%，也就是代码中的 ZSKIPLIST_P 的值。所以官方的跳跃列表更加的扁平化，层高相对较低，在单个层上需要遍历的节点数量会稍多一点。也正是因为层数一般不高，所以遍历的时候从顶层开始往下遍历会非常浪费。跳跃列表会记录一下当前的最高层数maxLevel，遍历时从这个 maxLevel 开始遍历性能就会提高很多。 找出搜索路径 创建新的节点（随机分配层数） 将搜索路径上的节点和这个新的节点串联起来 可能需要更新最大层数 以下为插入80和45到skiplist中演示。 1.7.2 删除 找出搜索路径 对每一层相关的节点重排前向后向指针 更新最大层数maxlevel 1.7.3 修改​ Redis中对分数的修改就很粗暴，直接删除了重新插入，根本就不会判断值到底有没有发生改变！无脑删除无脑重新插入。 1.7.4 查找​ 跳跃列表有了多层结构之后，这个定位的算法复杂度将会降到 O(lg(n))。 1.7.5 性能比较1.8 listpack​ Redis5.0后引入了新的数据结构listpack，它是对ziplist的改进，在存储空间上更加节省，结构上比ziplist更加精简。 123456struct listpack&lt;T&gt; &#123; int32 total_bytes; // 占用的总字节数 int16 size; // 元素个数 T[] entries; // 紧凑排列的元素列表 int8 end; // 同 zlend 一样，恒为 0xFF&#125; 1.9 Rax​ Rax 是 Redis 内部比较特殊的一个数据结构，它是一个有序字典树 (基数树 Radix Tree)，按照 key 的字典序排列，支持快速地定位、插入和删除操作。Redis 五大基础数据结构里面，能作为字典使用的有 hash 和 zset。hash 不具备排序功能，zset 则是按照 score 进行排序的。rax 跟 zset 的不同在于它是按照 key 进行排序的。Redis 作者认为 rax 的结构非常易于理解，但是实现却有相当的复杂度，需要考虑很多的边界条件，需要处理节点的分裂、合并，一不小心就会出错。 ​ 你可以将一本英语字典看成一棵 radix tree，它所有的单词都是按照字典序进行排列，每个词汇都会附带一个解释，这个解释就是 key 对应的 value。有了这棵树，你就可以快速地检索单词，还可以查询以某个前缀开头的单词有哪些。 1.10 总结 基础数据结构 数据存储结构 string （1）当长度特别短的时候，使用emb.（2）当长度超过44的时候，使用raw. list （1）早期版本：元素少用ziplist（2）早期版本：元素多用linkedlist（3）现在使用quicklist代替ziplist和linkedlist set （1）底层实现为dict，只不过set中所有的value是NULL.（2）当set集合容纳的元素都是整数并且元素个数较少的时候，Redis会使用intset来存储集合元素 hash （1）dict（字典）。（2）hash容器对象在元素较少的时候，采用压缩列表（ziplist）进行存储。（3） zset （1）zset中存储value和score使用dict（2）zset需要按照score来排序，还需要使用score获取一定范围的value列表，使用跳跃表来实现这一功能 2. 基础数据结构2.1 string2.2 List2.3 Set2.4 hash2.5 zset3. 理论基础3.1 事件 3.1.1 文件事件3.1.1.1 API ae.c/aeCreateFileEvent ae.c/aeDeleteFileEvent ae.c/aeGetFileEvents ae.c/aeWait 函数接受（一个套接字描述符、一个事件类型和一个毫秒参数），在给定的时间内阻塞并等待套接字的给定类型事件产生，当事件成功产生或者等待超时，函数返回！ ae.c/aeApiPoll函数接受一个sys/time.h/struct timeval结构为参数，并在指定的时间内，阻塞并等待所有被aeCreateFileEvent函数设置为监听状态的套接字产生的文件事件，但至少有一个事件产生或者超时，则结束，和aeWait相比，这个函数并没有指定文件事件的类型。 ae.c/aeProcessEvents函数是文件事件分派器，（1）它首先调用aeApiPoll函数等待事件产生，（2）然后遍历所有已经产生的事件，（3）并调用相应的事件处理器来处理这些事件。(文件事件处理器包括：连接应答处理器、命令请求处理器、命令回复处理器、复制处理器) ae.c/aeGetApiName函数返回I/O多路复用程序底层所使用的的I/O多路复用函数库的名称，epoll或者select。 3.1.2 时间事件3.2 事件轮询、多路复用Redis 基于 Reactor 模式开发了自己的网络事件处理器： 这个处理器被称为文件事件处理器（file event handler）： 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。 文件事件处理器的四个组成部分， 它们分别是套接字、 I/O 多路复用程序、 文件事件分派器（dispatcher）、 以及事件处理器。 Redis 的 I/O 多路复用程序的所有功能都是通过包装常见的 select 、 epoll 、 evport 和 kqueue 这些 I/O 多路复用函数库来实现的， 每个 I/O 多路复用函数库在 Redis 源码中都对应一个单独的文件， 比如 ae_select.c 、 ae_epoll.c 、 ae_kqueue.c ， 诸如此类。因为 Redis 为每个 I/O 多路复用函数库都实现了相同的 API ， 所以 I/O 多路复用程序的底层实现是可以互换的。 I/O 多路复用程序总是会将所有产生事件的套接字都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字 补充 epoll 是任务驱动模型，其在高并发场景下保证效率的关键点如下： socket 不再阻塞，服务器线程只需要抛出一个线程，避免大量线程的系统开销（BIO缺陷）； 使用 select 多路复用，轮询发生在内核空间而不是在用户空间，不需要进行频繁的内核调用，避免大量的用户态到内核态转换带来的开销（NIO缺陷）； 使用 mmap 共享内存，不需要进行多次的文件描述符数据的复制过程，节省系统资源（select 多路复用缺陷） 使用 wait 系统调用，即事件驱动模型，且链表天然有序（先后顺序），使事件任务能够顺序处理 3.3 管道pipeline Redis本身是基于Request/Response协议（停等机制）的，正常情况下，客户端发送一个命令，等待Redis返回结果，Redis接收到命令，处理后响应。在这种情况下，如果同时需要执行大量的命令，那就是等待上一条命令应答后再执行，这中间不仅仅多了RTT（Round Time Trip），而且还频繁调用系统IO，发送网络请求。为了提升效率，这时候pipeline出现了，它允许客户端可以一次发送多条命令，而不等待上一条命令执行的结果，这和网络的Nagel算法有点像（TCP_NODELAY选项）。pipeline不仅减少了RTT，同时也减少了IO调用次数（IO调用涉及到用户态到内核态之间的切换）。 应用举例：Redis 事务在发送每个指令到事务缓存队列时都要经过一次网络读写，当一个事务内部的指令较多时，需要的网络 IO 时间也会线性增长。所以通常 Redis 的客户端在执行事务时都会结合 pipeline 一起使用，这样可以将多次 IO 操作压缩为单次 IO 操作。比如我们在使用 Python 的 Redis 客户端时执行事务时是要强制使用 pipeline 的。 3.4 持久化3.4.1 AOFRedis 的服务器进程就是一个事件循环（loop）， 这个循环中的文件事件负责接收客户端的命令请求， 以及向客户端发送命令回复， 而时间事件则负责执行像 serverCron 函数这样需要定时运行的函数。 因为服务器在处理文件事件时可能会执行写命令， 使得一些内容被追加到 aof_buf 缓冲区里面， 所以在服务器每次结束一个事件循环之前， 它都会调用 flushAppendOnlyFile 函数， 考虑是否需要将 aof_buf 缓冲区中的内容写入和保存到 AOF 文件里面， 这个过程可以用以下伪代码表示： 123456789def eventLoop(): while True: # 处理文件事件，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否要将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile() 3.4.1.1 AOF步骤 命令追加（append） 文件写入 文件同步（sync） 3.4.1.2 AOF重写步骤 （1）执行客户端发送来的命令。 （2）将执行后的命令追加到AOF缓冲区。 （3）将执行后的写命令追加到AOF重写缓冲区。 3.4.1.3 使用到的指令或方法或系统函数 flushAppendOnlyFile 参数appendSync：always，everysecond，no 系统函数：fsync，fdatasync可以强制操作系统将缓存数据写入到磁盘中，从而确保数据的安全性。 3.4.2 RDB 有两个Redis命令可以用来生成RDB文件，一个是SAVE，另外一个是BGSAVE。SAVE命令会阻塞Redis服务器进程，知道RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令。 BGSAVE命令会派生一个子进程，由子进程负责创建RDB文件。 伪代码如下： 123456789101112131415161718def SAVE(): # 创建RDB文件 rdbSave() def BGSAVE(): # 创建子进程 pid = fork() if pip == 0: # 子进程负责创建RDB文件 rdbSave() # 完成之后向父进程发送信号 signal_parent() elif pid &gt; 0: # 父进程继续处理命令请求，并通过轮询等待子进程的信号 handle_request_and_wait_signal() else: # 处理出错的情况 handle_fork_error() 3.4.2.1 使用到的指令或方法或系统函数 SAVE BGSAVE fork 3.4.3 混合持久化3.5 通信协议3.6 事务3.6.1 事务的基本作用每个事务的操作都有 begin、commit 和 rollback，begin 指示事务的开始，commit 指示事务的提交，rollback 指示事务的回滚。Redis 的事务根本不能算「原子性」，而仅仅是满足了事务的「隔离性」，隔离性中的串行化———当前执行的事务有着不被其它事务打断的权利。 123456789begin();try &#123; command1(); command2(); .... commit();&#125; catch(Exception e) &#123; rollback();&#125; Redis 在形式上看起来也差不多，分别是 multi/exec/discard。 multi 指示事务的开始; exec 指示事务的执行; discard 指示事务的丢弃。 3.6.2 watch（一种乐观锁机制）3.7 内存分配3.7.1 内存管理基础 jemalloc， 3.7.2 Redis的内存淘汰策略3.7.2.1 淘汰策略当Redis内存超过物理内存限制的时候，内存数据会开始和磁盘数据产生频繁的交换（swap），交换会让Redis性能急剧下降。 当实际内存超过maxmemory的时候，Redis提供了几种可选的策略，让用户自己决定如何腾出新的空间以继续提供读写服务。 noviction（不继续写请求） volatile-lru（尝试淘汰设置了过期时间的key，最少使用的key优先被淘汰，没有设置过期时间的key不会被淘汰） volatile-ttl（同上，但是淘汰的策略不是LRU，而是key剩余寿命ttl的值，ttl越小越优先淘汰） volatile-random（淘汰的key是过期集合中随机的key） allkeys-lru（淘汰的key对象是全体key的集合） allkeys-random（淘汰的key对象是全体key的集合） 总结：带有volatile的会对设置了过期时间的key进行淘汰，带有allkeys的会堆所有的key进行淘汰。 3.7.2.2 LRU算法LRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的页面予以淘汰。 LRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。 LFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面。 1234567891011121314151617181920212223242526272829# 基于Python放入OrderedDict(双向链表+字典)实现一个简单的LRUfrom collrctions import OrderedDictclass LRUDict(OrderedDict): def __init__(self, capacity): self.capicity=capicity self.items=OrderedDict() def __setitem__(self, key, value): old_value = self.items.get(key) if old_value is not None: self.items.pop(key) self.items[key] = value elif len(self.items) &lt; self.capicity: self.item[key] = value else: self.items.popitem(last=True) self.items[key] = value def __getitem__(self, key): value = self.item.get(key) if value is not None: self.items.pop(key) self.items[key] = value return value def __repr__(self): return repr(self.items)# 使用d = LRUDict(10)for i in range(15): d[i] = iprint d 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// LinkedHashMap实现LRU;// 关键在于重写removeEldestEntryimport java.util.ArrayList;import java.util.Collection;import java.util.LinkedHashMap;import java.util.Map;import java.util.Map.Entry; public class LRULinkedMap&lt;K, V&gt; &#123; /** * 最大缓存大小 */ private int cacheSize; private LinkedHashMap&lt;K, V&gt; cacheMap; public LRULinkedMap(int cacheSize)&#123; this.cacheSize = cacheSize; cacheMap = new LinkedHashMap(16, 0.75F, true)&#123; @Override protected boolean removeEldestEntry(Entry eldest) &#123; if(cacheSize + 1 == cacheMap.size())&#123; return true; &#125;else&#123; return false; &#125; &#125; &#125;; &#125; public void put(K key, V value)&#123; cacheMap.put(key, value); &#125; public V get(K key)&#123; return cacheMap.get(key); &#125; public Collection&lt;Map.Entry&lt;K, V&gt;&gt; getAll()&#123; return new ArrayList&lt;Map.Entry&lt;K, V&gt;&gt;(cacheMap.entrySet()); &#125; public static void main(String[] args) &#123; LRULinkedMap&lt;String, Integer&gt; map = new LRULinkedMap&lt;&gt;(3); map.put("key1", 1); map.put("key2", 2); map.put("key3", 3); for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123; System.out.println(e.getKey()+"====&gt;"+e.getValue()); &#125; System.out.println("\n"); map.put("key4", 4); for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123; System.out.println(e.getKey()+"====&gt;"+e.getValue()); &#125; &#125; &#125; 3.8 Rehash当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式： 12# 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size 3.9 定时任务​ Redis 的定时任务会记录在一个称为最小堆的数据结构中。 3.10 db 3.11 复制3.11.1 完整流程:数据同步+命令传播 3.11.2 重点 Redis2.8以前的复制功能不能高校处理断线重连后的复制情况，Redis2.8之后新添加的部分重同步功能可以解决这个问题。 部分重同步通过复制偏移量、复制积压缓冲区、服务器运行ID三个部分实现。 在复制操作刚开始的时候，从服务器会成为主服务器的客户端，通过向主服务器发送命令请求执行复制步骤，而在复制操作后期，主从服务器会相互成为对方的客户端。 主服务器通过向从服务器传播命令来更新从服务器的状态，保持主从服务器一致，而从服务器则通过向主服务器发送命令来进行心跳检测，以及命令丢失检测。 3.12 哨兵3.12.1 Sentinel系统选举领头SentinelRedis基于对Raft算法的实现领头Sentinel的选举方法。 3.12.2 故障转移重点 Sentinel只是一个运行在特殊模式下的Redis服务器，它使用了和普通模式不同的命令表，所以Sentinel模式能够使用的命令和普通Redis服务器能够使用的命令不同。 Sentinel会读入用户指定的配置文件，为每一个要被监视的主服务器创建相应的实例结构，并创建连向主服务器的命令连接和订阅连接，其中命令连接用于向主服务器发送命令请求，而订阅连接则用于接收指定频道的消息。 Sentinel通过向主服务器发送INFO命令来获得主服务器属下所有从服务器的地址信息，并为这些从服务器创建相应的实例结构，以及连向这些从服务器的命令连接和订阅连接。 在一般的情况下，Sentinel以每10秒1次的频率向被监视的主服务器和从服务器发送INFO命令，当主服务器处于下线状态，或者Sentinel正在对主服务器进行故障转移操作的时候，Sentinel向从服务器发送INFO命令的频率改为1秒1次。 对于监视同一个主服务器和从服务器的多个Sentinel来说，它们以两秒一次的频率，通过向被监视服务器的__sentinel__:hello频道发送消息来向其他的Sentinel宣告自己的存在。 每一个Sentinel也会从__sentinel__:hello频道中接收其他sentinel发来的消息，并根据这些消息为其他的Sentinel创建相应的实例结构，以及命令连接。 Sentinel只会与主服务器和从服务器创建命令连接和订连接，sentinel和sentinel之间只会创建命令连接。 Sentinel会以每秒一次的频率向实例（包括主服务器、从服务器、其他Sentinel）发送PING命令，并根据实例对PING命令的回复来判断实例是否在线，当一个实例在指定的时长中连续向Sentinel发送无效回复时，Sentinel将这个实例判断为主观下线。 当Sentinel将一个主服务器判断为主观下线时，它会向同样监视这个主服务器的其他Sentinel进行询问，看它是否同意这个主服务器已经进入主观下线状态。 当Sentinel收集到足够多的主观下线投票后，它将主服务器判断为客观下线，并发起一次针对主服务器的故障转移操作。 3.12.3 哨兵客户端原理实现一个哨兵客户端的基本步骤如下： graph TB st(( ))-.->a(1. 遍历哨兵集合获取到一个可用的哨兵节点.因为哨兵节点之间是共享数据的, 任意节点都可以获取到主节点的信息) a-->b(2. 通过 sentinel get-master-addr-by-name master-name API 来获取对应主节点的信息) b-->c(3. 验证获取到的主节点是不是真正的主节点, 防止故障转移期间主节点的变化) c-->d(4.保持和哨兵节点集合的联系,时刻获取关于主节点的相关信息) 1 2 3 4 3.13 集群集群知识点汇总 知识点 描述 节点 槽指派 命令执行 重新分片 转向 故障转移 消息 检查其他节点是否在线基于Gossip协议检查节点是否在线。（Gossip—-p2p核心协议https://zhuanlan.zhihu.com/p/41228196） 重点 节点通过握手将其他节点添加到自己所处的集群中。 集群中的16384个槽可以分别指派给集群中的各个节点，每个节点都会记录哪些槽位指派给了自己，而哪些槽位指配给了其他节点。（槽的数量为什么是16384？） 节点收到一条命令请求的时候，会首先检查这个命令请求要处理的键所在的槽位是否是自己负责。如果不是的话将向客户端返回一个MOVED错误，MOVED错误携带的信息将指引客户端转向至正在负责相关槽位的节点。 对Redis集群的重新分片工作是由redis-trib负责执行的，重新分片的关键在于将属于某一个槽位的所有键值从一个节点转移到另外一个节点。 如果节点A正在迁移i至节点B，那么当节点A没能够在自己的数据库中找到命令指定的数据库键时，节点A将向客户端返回一个ASK错误，指引客户端到节点B继续查找指定的数据库键。 MOVED错误表示槽的负责权已经从一个节点转移到了另外一个节点，而ASK错误只是两个节点在迁移槽的过程中使用的一种临时的措施。 集群里的从节点用于复制主节点，并在主节点下线的时候，代替主节点继续处理命令请求。 集群中的节点通过发送和接受消息来进行通信，常见的消息包括MEET、PING、PONG、PUBLISH、FAIL五种。 3.14 codis​ Codis是一个分布式Redis解决方案,对于上层的应用来说,连接到Codis Proxy和连接原生的RedisServer没有明显的区别,有部分命令不支持。Codis底层会处理请求的转发,不停机的数据迁移等工作,所有后边的一切事情,对于前面的客户端来说是透明的,可以简单的认为后边连接的是一个内存无限大的Redis服务. Codis-proxy 实现redis协议,由于本身是无状态的,因此可以部署很多个节点 Codis-config 是codis的管理工具,包括添加/删除redis节点添加/删除proxy节点,发起数据迁移等操作,自带httpserver,支持管理后台方式管理配置 Codis-server 是codis维护的redis分支,基于2.8.21分支,加入了slot的支持和原子的数据迁移指令; codis-proxy和codis-config只能和这个版本的redis交互才能正常运行 Zookeeper 用于codis集群元数据的存储,维护codis集群节点 3.15 Redis中零拷贝的使用​ 异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到PageCache。在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。 3.16 Redis版本4. 快速使用 一、前言 1、获取key的列表：KEYS pattern 通配符有 ？*[] 和转义 \。 2、key 是否存在： EXISTS key 存在返回 1，不存在返回 0。 3、建立 key 和删除 key：SET key 和 DEL key。 4、根据 key 获取该键所存储的 redis 数据类型：TYPE key。返回是 string、list、hash、set、zset。下面会对这5种返回的 redis 数据类型逐一讲解。 5、rename oldkey newkey：对 key 重命名，如果 newkey 存在则覆盖。 6、renamenx oldkey newkey：对 key 重命名，如果 newkey 存在则不覆盖。 7、randomkey：随即返回一个 key 8、move key db-index：将 key 移动到指定的数据库中，如果 key 不存在或者已经在该数据库中，则返回 0。成功则返回 1。 二、Redis数据类型、Redis数据命令 1、Redis数据类型一字符串类型：这个很好理解，一个key存储一个字符串。如果你要存数据呢？转换成Json或者其他的字符串序列化。 2、Redis数据命令一———字符串类型： 1）赋值：SET key value。如 set hello world 2）取值：GET key。如 get hello。返回是 world 3）自增：INCR key。就是 Mysql的AUTO_INCREMENT。每次执行 INCR key时，该key的值都会+1.若key不存在，则先建立一个0，然后+1，返回 1。如果值不是整数则报错。该操作是原子操作。 4）自减：DECR key。将指定 key 的值减少 1。 如 DECR num，就是 num-1 5）自增 N：INCRBY key increment 用来给指定 key 的值加 increment。如 INCRBY num 5 就是 num+5 6）自减 N：DECRBY key increment 用来给指定 key 的值减 increment。如 DECRBY num 5 就是 num-5 7）增加浮点数：INCRBYFLOAT key increment。 8）向尾部追加：APPEND key value。如set test:key 123、append test:key 456、get test:key 就是 123456 9）获取长度：STRLEN key。 10）同时给多个 key 赋值：MSET title 这是标题 description 这是描述 content 这是内容。 11）同时获取多个 key 的值：MGET title description content 12）位操作之获取：GETBIT key offset。如字符 a 在 redis 中的存储为 01100001（ASCII为98），那么 GETBIT key 2 就是 1，GET key 0 就是 0。 13）位操作之设置：SETBIT key offset value。如字符 a 在 redis 中的存储为 01100001（ASCII为98），那么 SETBIT key 6 0，SETBIT key 5 1 那么 get key 得到的是 b。因为取出的二进制为 01100010。 14）位操作之统计：BITCOUNT key [start] [end]：BITCOUNT key 用来获取 key 的值中二进制是 1 的个数。而 BITCOUNT key start end 则是用来统计key的值中在第 start 和 end 之间的子字符串的二进制是 1 的个数（好绕啊）。 15）位操作之位运算：BITOP operation resultKey key1 key2。operation 是位运算的操作，有 AND，OR，XOR，NOT。resultKey 是把运算结构存储在这个 key 中，key1 和 key2 是参与运算的 key，参与运算的 key 可以指定多个。 3、Redis数据类型二———散列类型： Redis 是以字典（关联数组）的形式存储的，一个 key 对应一个 value。在字符串类型中，value 只能是一个字符串。那么在散列类型，也叫哈希类型中，value 对应的也是一个字典（关联数组）。那么就可以理解，Redis 的哈希类型/散列类型中，key 对应的 value 是一个二维数组。但是字段的值只可以是字符串。也就是说只能是二维数组，不能有更多的维度。 4 Redis 数据命令二——— 散列类型： 1）赋值：HSET key field value。如 hset user name lane。hset user age 23 2）取值：HGET key field。如 hget user name，得到的是 lane。 3）同一个key多个字段赋值：HMSET key field1 value1 field2 value2… 4）同一个KEY多个字段取值：HMGET key field1 fields2… 5）获取KEY的所有字段和所有值：HGETALL key。如 HGETALL user 得到的是 name lane age 23。每个返回都是独立的一行。 6）字段是否存在：HEXISTS key field。存在返回 1，不存在返回 0 7）当字段不存在时赋值：HSETNX key field value。如果 key 下面的字段 field 不存在，则建立 field 字段，且值为 value。如果 field 字段存在，则不执行任何操作。它的效果等于 HEXISTS + HSET。但是这个命令的优点是原子操作。再高的并发也不会怕怕。 8）自增 N：HINCREBY key field increment。同字符串的自增类型，不再阐述。 9）删除字段：DEL key field1 field2… 删除指定KEY的一个或多个字段。 10）只获取字段名：HKEYS key。与 HGETALL 类似，但是只获取字段名，不获取字段值。 11）只获取字段值：HVALS key。与 HGETALL 类似，但是只获取字段值，不获取字段名。 12）获取字段数量：HLEN key。 5、Redis数据类型三———列表类型 列表类型存储了一个有序的字符串列表。常用的操作是向两端插入新的元素。时间复杂度为 O（1）。结构为一个链表。记录头和尾的地址。看到这里，Redis 数据类型的列表类型一个重大的作用呼之欲出，那就是队列。新来的请求插入到尾部，新处理过的从头部删除。另外，比如微博的新鲜事。比如日志。列表类型就是一个下标从 0 开始的数组。由于是链表存储，那么越靠近头和尾的元素操作越快，越靠近中间则越慢。 6、Redis数据命令三———列表类型： 1）向头部插入：LPUSH key value1 value2…。返回增加后的列表长度。 2）向尾部插入：RPUSH key value1 value2…。返回增加后的列表长度。 3）从头部弹出：LPOP key。返回被弹出的元素值。该操作先删除key列表的第一个元素，再将它返回。 4）从尾部弹出：RPOP key。返回被弹出的元素值。 5）列表元素个数：LLEN key。key 不存在返回 0。 6）获取列表的子列表：LRANGE start end。返回第 start 个到第 end 个元素的列表。包含 start 和 end。支持负数索引。-1 表示最后一个元素，-2 表示倒数第二个元素。 7）删除列表中指定值：LREM key count value。删除 key 这个列表中，所有值为 value 的元素，只删除 count。如果有 count+1 个，那么就保留最后一个。count 不存在或者为 0，则删除所有的。如果 count 大于 0，则删除从头到尾的 count 个，如果 count 小于 0，则删除从尾到头的 count 个。 8）获取指定索引值：LINDEX key index。如LINDEX key 0就是列表的第一个元素。index可以是负数。 9）设置索引和值：LSET key index value。这个操作只是修改指定 key 且指定 index 的值。如果 index 不存在，则报错。 10）保留片段，删除其它：LTRIM key start end。保留 start 到 end 之间的所有元素，含 start 和 end。其他全部删除。 11）向列表插入元素：LINSERT key BEFORE/AFTER value1 value2。从列表头开始遍历，发现值为 value1 时停止，将 value2 插入，根据 BEFORE 或者 AFTER 插入到 value1 的前面还是后面。 12）把一个列表的一个元素转到另一个列表：RPOPLPUSH list1 list2。将列表 list1 的右边元素删除，并把该与元素插入到列表 list2 的左边。原子操作。 7、Redis数据类型四———集合类型： 集合类型是为了方便对多个集合进行操作和运算。集合中每个元素不同且没有顺序的概念，每个元素都是且只能是一个字符串。常用操作是对集合插入、删除、判断等操作。时间复杂度尾 O(1)。可以进行交集、并集、差集运算。例如文章 1 的有 3 个标签，是一个 Redis 数据类型集合类型存储。文章 2 有 3 个标签，有一个 Redis 数据类型集合类型存储。文章是 1 是 mysql，文章 2 是讲 redis。那么交集是不是就交出了一个数据库？（假设数据库这个tag在两篇文字都有）。集合类型在 redis 中的存储是一个值为空的散列表。 8、Redis 数据命令四———集合类型： 1）增加：SADD key value。 2）删除：SREM key value。 3）获取指定集合的所有元素：SMEMBERS key。 4）判断某个元素是否存在：SISMEMBER key value。 5）差集运算：SDIFF key1 key2…。对多个集合进行差集运算。 6）交集运算：SINNER key1 key2…。对多个集合进行交集运算。 7）并集运算：SUNION key1 key2…。对多个集合进行并集运算。 8）获取集合中元素个数：SCARD key。返回集合中元素的总个数。 9）对差集、交集、并集运算的结果存放在一个指定的 key 中：SDIFFSTORE storekey key1 key2。对 key1 和 key2 求差集，结果存放在 key 为 storekey 的集合中。SINNERSTORE 和 SUNIONSTORE 类似。 10）获取集合中的随即元素：SRANDMEMBER key [count]。参数 count 可选，如果 count 不存在，则随即一个。count 大于 0，则是不重复的 count 个元素。count 小于 0，则是一共 |count|个 元素，可以重复。 11）随即弹出一个元素：SPOP key。随即从集合中弹出一个元素并删除，将该元素的值返回。 9、Redis 数据类型五———有序集合类型： 集合类型是无序的，每个元素是唯一的。那么有序集合就是有序的，每个元素是唯一的。有序集合类型和集合类型的差别是，有序集合为每个元素配备了一个属性：分数。有序集合就是根据分数来排序的。有序集合是使用散列表和跳跃表实现的。所以和列表相比，操作中间元素的速度也很快。时间复杂度尾 O(log(N))。Redis 数据类型中的有序集合类型比 Redis 数据类型中的列表类型更加耗费资源。 10、Redis数据命令五——-有序集合类型 1）增加：ZADD key sorce1 value1 sorce2 value2…。 2）获取分数：ZSCORE key value。获取key的有序集合中值为 value 的元素的分数。 3）获取排名在某个范围内的元素列表：ZRANFGE key start stop [WITHSCORE]。获取排名在 start 和 end 之间的元素列表，包含 start 和 end2 个元素。每个元素一行。如果有WITHSCORE参数，则一行元素值，一行分数。时间复杂度为O(LOGn+m)。如果分数相同，则 0]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP半包和粘包]]></title>
    <url>%2F2021%2F03%2F13%2FTCP%E5%8D%8A%E5%8C%85%E5%92%8C%E7%B2%98%E5%8C%85%2F</url>
    <content type="text"><![CDATA[粘包的主要原因 发送方每次写入数据 &lt; 套接字(Socket)缓冲区大小 接收方读取套接字(Socket)缓冲区数据不够及时 半包的主要原因： 发送方每次写入数据 &gt; 套接字(Socket)缓冲区大小 发送的数据大于协议的 MTU (Maximum Transmission Unit，最大传输单元)，因此必须拆包 原理 解决方法解决方法就像上面说的，UDP 之所以不会产生粘包和半包问题，主要是因为消息有边界，因此，我们也可以采取类似的思路。 改成短连接将 TCP 连接改成短连接，一个请求一个短连接。这样的话，建立连接到释放连接之间的消息即为传输的信息，消息也就产生了边界。 这样的方法就是十分简单，不需要在我们的应用中做过多修改。但缺点也就很明显了，效率低下，TCP 连接和断开都会涉及三次握手以及四次握手，每个消息都会涉及这些过程，十分浪费性能。 因此，并不推介这种方式。 封装成帧封装成帧(Framing)，也就是原本发送消息的单位是缓冲大小，现在换成了帧，这样我们就可以自定义边界了。一般有4种方式： 固定长度这种方式下，消息边界也就是固定长度即可。 优点就是实现很简单，缺点就是空间有极大的浪费，如果传递的消息中大部分都比较短，这样就会有很多空间是浪费的。 因此，这种方式一般也是不推介的。 分隔符这种方式下，消息边界也就是分隔符本身。 优点是空间不再浪费，实现也比较简单。缺点是当内容本身出现分割符时需要转义，所以无论是发送还是接受，都需要进行整个内容的扫描。 因此，这种方式效率也不是很高，但可以尝试使用。 专门的 length 字段这种方式，就有点类似 Http 请求中的 Content-Length，有一个专门的字段存储消息的长度。作为服务端，接受消息时，先解析固定长度的字段（length字段）获取消息总长度，然后读取后续内容。 优点是精确定位用户数据，内容也不用转义。缺点是长度理论上有限制，需要提前限制可能的最大长度从而定义长度占用字节数。 因此，十分推介用这种方式。 redis半包粘包问题 实验代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package network.tcp;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.ChannelPipeline;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * * */public class NettyServer &#123; private int port; public NettyServer(int port) &#123; this.port = port; bind(); &#125; private void bind() &#123; EventLoopGroup boss = new NioEventLoopGroup(); EventLoopGroup worker = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(boss, worker); bootstrap.channel(NioServerSocketChannel.class); bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // 连接数 bootstrap.option(ChannelOption.TCP_NODELAY, true); // 不延迟，消息立即发送 bootstrap.childOption(ChannelOption.SO_KEEPALIVE, true); // 长连接 bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline p = socketChannel.pipeline(); p.addLast(new NettyProblemServerHandler());// 添加NettyServerHandler，用来处理Server端接收和处理消息的逻辑 &#125; &#125;); ChannelFuture channelFuture = bootstrap.bind(port).sync(); if (channelFuture.isSuccess()) &#123; System.err.println("启动Netty服务成功，端口号：" + this.port); &#125; // 关闭连接 channelFuture.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; System.err.println("启动Netty服务异常，异常信息：" + e.getMessage()); e.printStackTrace(); &#125; finally &#123; boss.shutdownGracefully(); worker.shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new NettyServer(10086); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package network.tcp;import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;public class NettyClient &#123; /* * 服务器端口号 */ private int port; /* * 服务器IP */ private String host; public NettyClient(int port, String host) throws InterruptedException &#123; this.port = port; this.host = host; start(); &#125; private void start() throws InterruptedException &#123; EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class); bootstrap.option(ChannelOption.SO_KEEPALIVE, true); bootstrap.group(eventLoopGroup); bootstrap.remoteAddress(host, port); bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; // socketChannel.pipeline().addLast(new MyProtocolDecoder(1024, 0, 4, 0, 4, true)); socketChannel.pipeline().addLast(new NettyProblemClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(host, port).sync(); if (channelFuture.isSuccess()) &#123; System.err.println("连接服务器成功"); &#125; channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new NettyClient(10086, "localhost"); &#125;&#125; 123456789101112131415161718192021222324package network.tcp;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;public class NettyProblemServerHandler extends ChannelHandlerAdapter &#123; private int counter; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req); System.out.println("-----start------\n"+ body + "\n------end------"); String content = "receive" + ++counter; ByteBuf resp = Unpooled.copiedBuffer(content.getBytes()); ctx.writeAndFlush(resp); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package network.tcp;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;import java.io.UnsupportedEncodingException;public class NettyProblemClientHandler extends ChannelHandlerAdapter &#123; private int counter; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; byte[] req = ("我是一条测试消息，快来读我吧，啦啦啦").getBytes(); for (int i = 0; i &lt; 100; i++) &#123; ByteBuf message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req); System.out.println(body + " count:" + ++counter + "----end----\n"); &#125; private String getMessage(ByteBuf buf) &#123; byte[] con = new byte[buf.readableBytes()]; buf.readBytes(con); try &#123; return new String(con, "UTF8"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 实验结果 server client]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[稳定性建设]]></title>
    <url>%2F2020%2F12%2F24%2F%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[Site Reliability Engineer]]></content>
  </entry>
  <entry>
    <title><![CDATA[分布式事务基础]]></title>
    <url>%2F2020%2F11%2F11%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[单数据源的一致性依靠单机事务保证，多数据源的一致性由分布式事务保证。 1. 原理1.1 事务1.1.1 原子性1.1.2 一致性1.1.3 隔离性1.1.4 持久性1.2 两将军问题In computing, the Two Generals’ Problem[1] is a thought experiment meant to illustrate the pitfalls and design challenges of attempting to coordinate an action by communicating over an unreliable link. In the experiment, two generals are only able to communicate with one another by sending a messenger through enemy territory. The experiment asks how they might reach an agreement on the time to launch an attack, while knowing that any messenger they send could be captured. It is related to the more general Byzantine Generals Problem and appears often in introductory classes about computer networking (particularly with regard to the Transmission Control Protocol, where it shows that TCP can’t guarantee state consistency between endpoints and why this is the case), though it applies to any type of two-party communication where failures of communication are possible. A key concept in epistemic logic, this problem highlights the importance of common knowledge). Some authors also refer to this as the Two Generals’ Paradox, the Two Armies Problem, or the Coordinated Attack Problem.[1][2] The Two Generals’ Problem was the first computer communication problem to be proved to be unsolvable. An important consequence of this proof is that generalizations like the Byzantine Generals problem are also unsolvable in the face of arbitrary communication failures, thus providing a base of realistic expectations for any distributed consistency protocols. 1.3 CAPCAP原则又称CAP定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 In theoretical computer science, the CAP theorem, also named Brewer’s theorem after computer scientist Eric Brewer), states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees. 1.4 BASEBASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语font&gt;的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 1.5 XA，JXAXA协议包含两阶段提交（2PC）和三阶段提交（3PC）两种实现 2. 解决方案2.1 两阶段提交(2PC)两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。 2.1.1 过程2.1.1.1 准备首先协调者询问参与者事务是否执行成功，参与者发回事务执行结果。 2.1.1.2 提交或者回滚然后如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。(标黄的部分可能会失败) 2.1.2 具体实现2.1.3 协调者如何来通信的？2.1.4 存在的问题2.1.4.1 同步阻塞所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。 2.1.4.2 单点问题协调者存在挂掉的风险。 2.1.4.3 数据不一致如果在第二阶段，因为网络问题，出现只有部分commit的情况，但是此时并没有回滚。 2.2 补偿事务(TCC)2.2.1 过程TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假入 Bob 要向 Smith 转账，思路大概是： 我们有一个本地方法，里面依次调用 首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。 在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 2.2.2 存在的问题2.2.2.1 数据一致性问题2.3 MQ事务引用资料1.Wikipedia《两将军问题，拜占庭将军问题》https://en.wikipedia.org/wiki/Two_Generals'_Problem ↩]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发经验积累]]></title>
    <url>%2F2020%2F10%2F24%2F%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[开发经验积累。一些套路、一些开发尝试、一些开发规约。 统一返回数据格式项目中我们会将响应封装成json返回，一般我们会将所有接口的数据格式统一， 使前端(iOS Android, Web)对数据的操作更一致、轻松。 一般情况下，统一返回数据格式没有固定的格式，只要能描述清楚返回的数据状态以及要返回的具体数据就可以。但是一般会包含状态码、返回消息、数据这几部分内容 例如，我们的系统要求返回的基本数据格式如下： 列表： 1234567891011121314&#123; "success": true, "code": 20000, "message": "成功", "data": &#123; "items": [ &#123; "id": "1", "name": "刘德华", "intro": "毕业于师范大学数学系，热爱教育事业，执教数学思维6年有余" &#125; ] &#125;&#125; 分页： 123456789101112131415&#123; "success": true, "code": 20000, "message": "成功", "data": &#123; "total": 17, "rows": [ &#123; "id": "1", "name": "刘德华", "intro": "毕业于师范大学数学系，热爱教育事业，执教数学思维6年有余" &#125; ] &#125;&#125; 没有返回数据： 123456&#123; "success": true, "code": 20000, "message": "成功", "data": &#123;&#125;&#125; 失败： 123456&#123; "success": false, "code": 20001, "message": "失败", "data": &#123;&#125;&#125; 因此，我们定义统一结果 123456&#123; "success": 布尔, //响应是否成功 "code": 数字, //响应码 "message": 字符串, //返回消息 "data": HashMap //返回数据，放在键值对中&#125; 创建统一结果返回类1、在common模块下创建子模块common-utils2、创建接口定义返回码创建接口 ResultCode.java 12345678package com.atguigu.commonutils;public interface ResultCode &#123; public static Integer SUCCESS = 20000; public static Integer ERROR = 20001;&#125; 3、创建结果类创建类 R.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Datapublic class R &#123; @ApiModelProperty(value = "是否成功") private Boolean success; @ApiModelProperty(value = "返回码") private Integer code; @ApiModelProperty(value = "返回消息") private String message; @ApiModelProperty(value = "返回数据") private Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(); private R()&#123;&#125; public static R ok()&#123; R r = new R(); r.setSuccess(true); r.setCode(ResultCode.SUCCESS); r.setMessage("成功"); return r; &#125; public static R error()&#123; R r = new R(); r.setSuccess(false); r.setCode(ResultCode.ERROR); r.setMessage("失败"); return r; &#125; public R success(Boolean success)&#123; this.setSuccess(success); return this; &#125; public R message(String message)&#123; this.setMessage(message); return this; &#125; public R code(Integer code)&#123; this.setCode(code); return this; &#125; public R data(String key, Object value)&#123; this.data.put(key, value); return this; &#125; public R data(Map&lt;String, Object&gt; map)&#123; this.setData(map); return this; &#125;&#125; 统一返回结果使用1、在service模块中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;xxxxx&lt;/groupId&gt; &lt;artifactId&gt;common_utils&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 2、修改Controller中的返回结果列表 123456@ApiOperation(value = &quot;所有讲师列表&quot;)@GetMappingpublic R list()&#123; List&lt;Teacher&gt; list = teacherService.list(null); return R.ok().data(&quot;items&quot;, list);&#125; 删除 12345678@ApiOperation(value = "根据ID删除讲师")@DeleteMapping("&#123;id&#125;")public R removeById( @ApiParam(name = "id", value = "讲师ID", required = true) @PathVariable String id)&#123; teacherService.removeById(id); return R.ok();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>开发经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F10%2F18%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在实际的例子中来学习设计模式。 UML 名称 解释 图片 泛化 泛化是一种一般与特殊、一般与具体之间关系的描述，具体描述建立在一般描述的基础之上，并对其进行了扩展。在java中用来表示继承的关系。 实现 实现是一种类与接口的关系，表示类是接口所有特征和行为的实现，在程序中一般通过类实现接口来描述 依赖 是一种使用的关系，即一个类的实现需要另一个类的协助。java中，方法参数需要传入另一个类的对象，就表示依赖这个类。表示方法：虚线箭头，类A指向类B。 关联 表示类与类之间的联接,它使一个类知道另一个类的属性和方法，这种关系比依赖更强、不存在依赖关系的偶然性、关系也不是临时性的，一般是长期性的。 聚合 java中一个类的全局变量引用了另一个类，就表示关联了这个类聚合关联关系的一种特例，是强的关联关系。聚合是整体和个体之间的关系，即has-a的关系，整体与个体可以具有各自的生命周期，部分可以属于多个整体对象，也可以为多个整体对象共享。程序中聚合和关联关系是一致的，只能从语义级别来区分； 组合 组合也是关联关系的一种特例。组合是一种整体与部分的关系，即contains-a的关系，比聚合更强。部分与整体的生命周期一致，整体的生命周期结束也就意味着部分的生命周期结束，组合关系不能共享。程序中组合和关联关系是一致的，只能从语义级别来区分。 构造者模式思想不直接产生想要的对象，让客户端在builder对象上调用类似setter的方法，来设置每个相关的可选参数，最后客户端调用无参的build方法来生成通常不可变的对象。 Demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.mao.rong;public class BuilderTest &#123; private int paramA; private int paramB; private int paramC; private int paramD; private int paramE; public static class Builder &#123; private int paramA; private int paramB; private int paramC = 0; private int paramD = 0; private int paramE = 0; public Builder(int paramA, int paramB) &#123; this.paramA=paramA; this.paramB=paramB; &#125; public Builder paramC(int val)&#123; paramC = val; return this; &#125; public Builder paramD(int val)&#123; paramD = val; return this; &#125; public Builder paramE(int val)&#123; paramE = val; return this; &#125; // build方法没有参数，是静态类的成员方法，调用build方法生成对象 public BuilderTest build() &#123; return new BuilderTest(this); &#125; &#125; // 有参的构造器 private BuilderTest(Builder builder)&#123; paramA = builder.paramA; paramB = builder.paramB; paramC = builder.paramC; paramD = builder.paramD; paramE = builder.paramE; &#125;&#125;// 使用package com.mao.rong;public class Main &#123; public static void main(String[] args) &#123; BuilderTest builderTest = new BuilderTest.Builder(1,2) .paramC(3) .paramD(4) .paramE(5) .build(); &#125;&#125; 举例在构建请求参数的时候，可以按照以上的方法设置具名的操作。当然想实现具名的请求参数构建，可以写为以下的方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Datapublic class R &#123; @ApiModelProperty(value = "是否成功") private Boolean success; @ApiModelProperty(value = "返回码") private Integer code; @ApiModelProperty(value = "返回消息") private String message; @ApiModelProperty(value = "返回数据") private Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(); private R()&#123;&#125; public static R ok()&#123; R r = new R(); r.setSuccess(true); r.setCode(ResultCode.SUCCESS); r.setMessage("成功"); return r; &#125; public static R error()&#123; R r = new R(); r.setSuccess(false); r.setCode(ResultCode.ERROR); r.setMessage("失败"); return r; &#125; public R success(Boolean success)&#123; this.setSuccess(success); return this; &#125; public R message(String message)&#123; this.setMessage(message); return this; &#125; public R code(Integer code)&#123; this.setCode(code); return this; &#125; public R data(String key, Object value)&#123; this.data.put(key, value); return this; &#125; public R data(Map&lt;String, Object&gt; map)&#123; this.setData(map); return this; &#125;&#125; 观察者模式举例: SpringListener]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[直播技术研究]]></title>
    <url>%2F2020%2F09%2F12%2F%E7%9B%B4%E6%92%AD%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[直播技术积累。]]></content>
      <categories>
        <category>音视频</category>
      </categories>
      <tags>
        <tag>音视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统设计之圈人]]></title>
    <url>%2F2020%2F08%2F29%2F%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B9%8B%E5%9C%88%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统设计之秒杀]]></title>
    <url>%2F2020%2F08%2F29%2F%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B9%8B%E7%A7%92%E6%9D%80%2F</url>
    <content type="text"><![CDATA[秒杀的关键要考虑两个方面：（1）不能够超卖，超卖（库存变成负数、优惠券变成负数）必然带来损失，是不允许的；（2）需要考虑性能！ 1. 分布式锁为了防止超卖，需要加锁，Java本身的内存锁在分布式的环境下并不能发挥加锁的功能，也就是说需要实现分布式锁。常见的方法有基于Redis的分布式锁和基于Zookeeper的分布式锁。 1.1 基于Redis分布式锁1.1.1 Redis分布式锁原理https://xiaomi-info.github.io/ Redis 锁主要利用 Redis 的 setnx 命令。 加锁命令：SETNX key value，当键不存在时，对键进行设置操作并返回成功，否则返回失败。KEY 是锁的唯一标识，一般按业务来决定命名。 解锁命令：DEL key，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。 锁超时：EXPIRE key timeout, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。 则加锁解锁伪代码如下： 12345678if (setnx(key, 1) == 1)&#123; expire(key, 30) try &#123; //TODO 业务逻辑 &#125; finally &#123; del(key) &#125;&#125; 以上存在以下问题： SETNX 和 EXPIRE 非原子性 锁错误解除 超时解锁导致并发 不可重入 1.1.2 Redis分布式锁实现1.1.3 Redis主从模式存在的问题1.2 基于Zookeeper分布式锁1.2.1 Zookeeper原理及应用场景1.2.2 Zookeeper常用的命令1.2.3 Zookeeper分布式锁原理1.2.4 Zookeeper分布式锁实现2. 秒杀性能2.1 秒杀前，页面访问压力大？解决方案：页面静态化，CDN+Redis+Ngnix多级缓存 2.2 秒杀时，下单过于集中，作弊软件刷单？解决方案：前端加答题环节,把URL动态化，就连写代码的人都不知道，你就通过MD5之类的加密算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过 2.3 秒杀时，下单对系统冲击大，影响其他正常功能？解决方案：独立的秒杀系统 2.4 秒杀时，快速精准减库存？解决方案：基于缓存如Redis实现快速精准减库存。 我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到Redis中去，让整个流程都在Redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。但是用了Redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。Lua脚本是类似Redis事务，有一定的原子性，不会被其他命令插队，可以完成一些Redis事务性的操作。这点是关键。知道原理了，我们就写一个脚本把判断库存扣减库存的操作都写在一个脚本丢给Redis去做，那到0了后面的都Return False了是吧，一个失败了你修改一个开关，直接挡住所有的请求，然后再做后面的事情嘛。 2.5 秒杀后，快速过滤没有抢到的下单请求？解决方案：库存减完后，快速通知Ngnix，过滤下单请求。 2.6 秒杀后，下单模块压力大？解决方案：下单请求写入RocketMq，下单后使用RocketMq通知下游服务，完成下单。 2.8 限流&amp;降级&amp;熔断&amp;隔离]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡算法]]></title>
    <url>%2F2020%2F08%2F29%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 负载均衡是实现高可用的一项重要技术，我的理解是一种将请求“均衡”分配到不同的机器上的技术，目的是提升系统整体的负载能力。从单机网站到分布式网站，解决了大型网站访问量大，并发量高，海量数据的问题。然而每个部署的独立业务面临单点问题和访问统一入口问题，需要通过负载均衡技术来实现流量分发。 ​ 最直接的分类方式可以将负载均衡分为基于硬件的负载均衡和基于软件的负载均衡；根据处理的对象不同，可以将其分为基于内容的负载均衡和基于请求的负载均衡；根据算法本身的处理方式可以分为静态负载均衡算法和动态负载均衡算法；根据网络协议可以分为四层负载均衡算法和七层负载均衡算法。 ​ 下面首先对常见的负载均衡算法介绍；然后对不同算法的应用场景进行比较，分析几个负载均衡相关的业务案例。 0.基本概念0.1 名词 解释 备注 LVS Linux Virtual Server 意即Linux虚拟服务器，是一个虚拟的服务器集群系统。 VIP 虚拟IP RIP 真实IP RS 真实服务器 A记录 一个域名对应一个ip CNAME 一个域名对应另外一个域名 1.负载均衡算法1.1. 基于请求的负载均衡算法1.1.1 简单轮询1.1.2加权轮询算法 参考[1] 原理：轮询调度算法就是以轮询的方式依次将请求调度到不同的服务器，即每次调度执行i = (i + 1) mod n，选出第 i 台服务器。加权轮询调度算法可以解决服务器间性能不一的情况，它用相应的权值表示服务器的处理性能，按权值的高低和轮询方式分配请求到各服务器。权值高的服务器先收到连接，权值高的服务器比权值低的服务器处理更多的连接，相同权值的服务器处理相同数目的连接数。 优势：算法简洁。它无需记录当前所有连接的状态，所以它是一种无状态调度。 劣势：不适用于请求服务时间变化比较大，或者每个请求所消耗的时间不一致的情况，此时轮询调度算法容易导致服务器间的负载不平衡。 适用场景：每个请求所占用的后端服务器时间基本相同，常用于短连接服务，例如 HTTP 等服务。 用户推荐：用户可知每个请求所占用后端时间基本相同或相差较小时，如已知后端服务器处理的都是同类型或者相似类型的请求时，推荐选择加权轮询的方式，因为该实现方式消耗小，无需遍历，效率较高。 1.1.3加权最小连接数算法 参考[1] 原理：在实际情况中，客户端的每一次请求服务在服务器停留的时间可能会有较大的差异，随着工作时间的延伸，如果采用简单的轮询算法，每一台服务器上的连接进程数目可能会产生极大的不同，这样实际上并没有达到真正的负载均衡。最小连接调度是一种动态调度算法，它通过服务器当前所活跃的连接数来估计服务器的负载情况。最小连接调度是一种动态调度算法，它通过服务器当前所活跃的连接数来估计服务器的负载情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1; 当连接中止或超时，其连接数减一。加权最小连接数算法是在最小连接数调度算法的基础上，根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。 假设各台 机器 的权值依次为 wi,当前连接数依次为 ci,依次计算 ci/wi,值最小的 机器 作为下一个分配的 机器 如果存在ci/wi相同的 机器，这些 机器 再使用加权轮询的方式调度 优势：此种均衡算法适合处理长时的请求服务，如 FTP 等应用。 劣势：相较于加权轮询算法，加权最小连接数算法需要保存服务器现有的连接数目，它是一种有状态调度。 适用场景：每个请求所占用的后端时间相差较大的场景，常用于长连接服务。 用户推荐：如果用户需要处理不同的请求，且请求所占用后端时间相差较大，如 2 ms 和 2s 这种数量级的差距时，推荐使用加权最小连接数算法实现负载均衡。 1.1.4 随机策略1.1.5 动态负载均衡1.2. 基于数据的负载均衡算法1.2.1. hash​ 如根据请求uri进行负载均衡，在Ngnix配置如下： 12345upstream backend &#123; hash $uri; server 192.168.61.9080 weight=1; server 192.168.62.9081 weight=2;&#125; 1.2.2. 一致性hash算法upstream ngnix_local_server { hash $consistent_key consistent; server 192.168.61.9080 weight=1; server 192.168.62.9081 weight=2; } 1.2.3. 基于关键字1.3. 全局负载均衡​ 全局负载均衡技术将用户的访问指向离用户最近的工作正常的流媒体服务器上, 带来的益处包括 increased reliability 和 reductions in latency。GSLB一般是通过CDN（Content Delivery Network）来实现的，那CDN又是如何来实现呢？CDN是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。 ​ 主流的实现技术如下。 DNS：GSLB会替代最终的DNS的服务器从而实现自己的解析策略，返回给用户最合适的IP(列表)。代表性的实现如F5。 HTTP redirection：使用HTTP重定向将内容转发到不同位置。 IP Route：更改IP首部实现使用跳转，并利用IP tunneling技术实现只对请求负载均衡(响应直接返回)。 统一调度服务层：客户端SDK+调度服务完成GSLB设备的功能。使用的策略可以是当前用户所在的区域，或者是userId等信息。 2. 均衡算法选取实例 2.1 轮询之场景1​ 若用户首次接触云服务，且建站时间不长，网站负载较低，则建议购买相同配置的 机器，因此 机器 都是无差别的接入层服务器。在此场景下，用户可以将 机器 的权重设置为相同的值，采用加权轮询的方式进行流量分发。 2.2 最小连接数之场景2 设有2台配置相同（CPU 和 内存）的 机器，由于性能一致，用户可以将 机器 权重都设置为10。设现在每台 机器 与客户端端建立了50个 TCP 连接，此时新增一台 机器。在此场景下，推荐用户使用最小连接数的均衡方式，这样能快速的提升新加入 机器 的负载，降低另外2台 机器 的压力。 2.3 加权轮询之场景3 ​ 用户有4台服务器，用于承载简单的静态网站访问，且4台服务器的计算能力的比例为 6：3：2：1（按CPU、内存换算）。在此场景下，用户可以依次将 机器 权重比例设置为60，30，20，10，由于静态网站访问大多数是短连接请求，因此可以采用加权轮询的均衡方式，让 SLB 按 机器 的性能比例分配请求。 2.4 加权轮询之场景4​ 某用户有12台 机器 用于承担海量的 WEB 访问请求，且不希望多购置 机器 增加支出。 某台 机器 经常会因为负载过高，导致服务器重启。在此场景下，建议用户根据 机器 的性能设置相应的权重，给负载过高的 机器 设置较小的权值。除此之外，可以采用最小连接数的负载均衡方式，将请求分配到活跃连接数较少的 机器 上，从而解决某台 机器 负载过高的问题。 2.5 加权轮询之场景5 ​ 某用户有3台 机器 用于处理若干长连接请求，且这3太服务器的计算能力比例为4：2：1（按CPU、内存换算）。 此时性能最好的服务器处理请求较多，用户不希望过载此服务器，希望能够将新的请求分配到空闲服务器上。在此场景下，可以采用加权最小连接数的均衡方式，并适当降低繁忙服务器的权重，便于 SLB 将请求分配到活跃数较少的 机器 上，实现负载均衡。 3. 负载均衡实战3.1. Ngnix3.1.1. 七层负载均衡​ 根据端口+应用层协议(如HTTP协议的主机名，URL)转发报文到上游服务器[2]。 3.1.2. 四层负载均衡​ 根据端口将报文转发报文到上游服务器[2]。 3.2. Dubbo3.3. Haproxy​ 在之前的博客里讲述了如何使用docker搭建一个软件应用栈，使用到的配置文件如下： 123456789101112131415161718192021222324252627282930313233global log 127.0.0.1 local0 maxconn 4096 chroot /usr/local/sbin daemon nbproc 4 pidfile /usr/local/sbin/haproxy.piddefaults log 127.0.0.1 local3 mode http option dontlognull option redispatch retries 2 maxconn 2000 balance roundrobin timeout connect 5000ms timeout client 50000ms timeout server 50000mslisten status bind 0.0.0.0:6301 stats enable stats uri /haproxy-stats server APP1 APP1:8001 check inter 2000 rise 2 fall 5 server APP2 APP2:8002 check inter 2000 rise 2 fall 5listen admin_status bind 0.0.0.0:32795 mode http stats uri /haproxy stats realm Global\ statistics stats auth admin:admin 参考资料1.https://docs.ksyun.com/documents/1145 ↩2.《亿级流量网站架设核心技术---张开涛》p.19 ↩]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Our Adventure]]></title>
    <url>%2F2020%2F08%2F19%2FOur-Adventure%2F</url>
    <content type="text"><![CDATA[Welcome to my blog, enter password to read. Incorrect Password! No content to display! U2FsdGVkX18ttZy8rAoBN1S7ERVV4+WjWll2AeqS27+BUtn3xqutqVUJFx1Z1lrOab861P8B4pRaxTFjMFHERNLlBxh6yNnUJrkG3upx8PuE4v8Hpj/1coLptqHIvISFYO5DV9C27ciAaK6o9TTRNrkOkh2O5ueN+DUiya9FbdBntZg4zMr+Kbl+trl7/LIBfj9iiVic8M7FcrVy6TmmYqStHGiQlsY7OUSaC/DHoqg8mIDqHXVZDSMXLxB7icKvCVIR/N4uw4sFZ7rhVVQ5Y1XxKFSwVUz6h+I02U2JW8s+o+ZPQF8gUFi0y/1Nn14ZlCC+vZ5kQVfv23ICs3mMv0d11BbYH9TxXKMJnNfP9crAUDfrvoA3Hz6F76CVRg5R3DH/JsPcX3w4vfdRwIQK4NEANjhM0WyunphFo78kUPEqo2HGLI/X//+uQ3ek8Ae1hqUEspkSEMMF5Nyi3zc3J4ykQNdhZRfxghcHYQEeId9spPmOUl4VTAaTZfWYRT8mRVurlO2GkNURSWnuII4yKgWoiOGK7/NSQyMdCoouwRrWOxJuUa45JnIqes1wX5nohZrVc9pfJSPe/hHkcSODy3ZVliJbLfoYj/tK8E6guQUWy9Uf13ir6ysvhhbkaWcv1PLbmNPZgeSwV5OmFgV06B9b+oEsx627Z/Ls8eXVIq0a0rvNtCf6but1difwpLK3gU6vSjFc7MLS/ijyq/KadWFRoDlcoDfE5g/KMLThjRkxsfJB1026TEKM0c6GV+Bn0lHon9Sd2WZDr3tz6TtF9yO13rU8WNl4SM17D5ct5CPbBabOanZh1iaD6r/dBpR2n7WIN4080vxbg0apPOnhKwMZw7MD/3HDg59pTZmr/tMJYCDg+prhu5q8WmrnMTrP6vHP7vS+WxXucdVpUAflsCPLMHPpetTc7N43E+NHDAN8xSNBzsQP+WU3tEa6D6vsEAw97HyuM/z0y/JFsu+wZKwmEWMgXpdJhXHqDPAcNSF4cu//ptZfhLMrQYvgzXO9l/7GVR+tO+Wr5cV5BHGRKD8qoE+6w1juOvMHTS69J5x0+LelJxWuTtxfXsEcjgJkstjW3AGY0CpcEcS6wqBvsxUt4NxviR2BnBJyHaivFLe9uGWOmUl3P2878WeEFeNoIlMtH+OhupaYaqpmop5iEzBVTdbx7hgyIl+y6cXJX4/T3fybCMSd3pvQ0AJ5+E8purTFxEFWWmQrQ0h9RPKkPYAI8rIjlj7ZV1zTgADehSYfQLtgip7NteMNSaqzJYcX72z53+hfsFsaxiQCxYNOxXrIWfaCGZDIlcNAXT+IZ3/1cX0plrmgkDLPzzmYsXZooc19JdYEq0vdibk3ULqzvf5DwOJvez9A+g4y1HROennunRBziq9/pN7TzkJJFImIuFCzS55asxLmfp4pFNfpBG9mXw+WFsuM4hn9c6wkCobXQ8qHZE51MlZefpwldWuMn789cNvurrDAIrNmYku5L0izkTeXjgNXbazaABqPT8zrIGjFhySIA8t8zK1vPN6VNaF6kodObBLJMZAsa+Cz435J8Bw9jc8lq6NFmEGIanKZO46xts2tLSgQ17p4r23HBfsosG8tmlakzYdD7igSIMfioniEmK/aZg3uTbed6FqmkDYl0yqLV2X9b97sZ1pabFNXZ9VoDv5vUW/4f9Ug03ScGiT/+lh6SqHQEueP0cNIvM7vYB96iSJY+9LmxOFLW9Djh95K1yn+jNrOgJ5crAXeegqv/t+3ET/Mduabe047hSbEkNpfQrbNkmO2wJxslUza82SD0hmRTaQupUVs5txgbqYJo7Qp7LoASJza10aX3WyZDMR8xPVV6wkZDJD7hzPKpVzbK1H7U0doQscpYFGk+B45YLBwqST26/X6qfe0yj98vK68w31YAzeEFKuKFkOOrCv9oWZUTMIeazuY3H+1lLvjSK46nEyXmplI169O+EkgkfK8bctjc/K4OS4GGpK9L++mrctgawdbmfmoBMd6I9LxYmSPrHFQ4MjQwi+G4G6CXbmAXJ9y+DcHyzMIY8BMUKMdnro0Ig5CWF/HBCkM57KGtdsfML9oQdEfcGboLZrKjaDzEybYz4kj+YrVol3i8PFR2m4BY52ubCDaf3xfl7+RFZAMA5hLjyZgy2V227CO4NzBFxjweEggq1EQ19reKRW4I2RvmCqc+v+0l6pa9r6nt3gXTF/gvdPrpN9R8TfDhIUzu7FzjQXuly2JD3vOiharl19saCuC0pwaa9ZZ/FgULMvkyR8zfPDE0QKpj22ELI4wujng1WRgDHGx7DgPKkFNJLjl6I3eZ5+r1lTp7DmHun2nV02xRGsBJF4/k2VjgE7sEeJwM5tu6y5jcOi5u8h2O5r9sAry8bNj3e/YVGn6ifKRvuslXjs3vtYi12oRHoOl0ygmrmKu27q3wuL667ZZf5LCWPN7Q/WfilzF8YhP9Gz3uxQzZXEtmVJ66jQCc3PTDwfrWcSez6VBc0oKkEsnO8g/dn+xMN180oxjaOrd3iR1Gpt8tnNfzCZugJ4Em/P912HlmJuhPRnOLAvAHwtqoROGZo4kJh4Yd/slc99tefIuxxpsDfJC/Yz57AVw44aaiuu3G9cMF2jhuWRrvImhqUxA3Tj1c8CYbPiuM2q0ynx1Ldl2WrP2jnX5wL0iGIPPCjn66pnTdiWtXKXYdiUp6Iwu2ErMBcoUaplhYC11Dn2qrxkBY1yBruN4mSa1XCe7RB3u8FWmY0F1f6+WOx7g9sg1/VEC+nfhE77srd54KELcUw6vr8GjwZnEdgGwSiIvL2EHl8CzTc50MTxMMngmyryA8HaX6TrRmePk+y2J294oFvU58LPi1QocIqlCXOodCbtMhCMqhJdAshjgQdBw7X+2NDrlmG9XlJsGXmKMUZ6Ar8nfXbN9hN4bLP44CYrP3tdK5yxGp+I3UpWfuGpM918fkwu/tLA7KeOSfvFwcw6+gg8053RHQkST3XKUzv7r0XgqwEAdlPLGjen3NNFW22zyklDePteEyPhH3G5hodwwF2pRcob/jSuQWqwLX74dBYvphW029XPFPUYLjRxqgC9gl4A6MBDR4peWmvW46jca9RrCTxUvHaS+OkgYc+i0MI4T+JZQyU0EFLbf2Q/l4iM6twwMWUojKHnHh2GtCDU4m62z6vVj1gTQmqpCsYUMMTtyWeLHSa66hzOt0Oy/TaMc0TO6JRF7tleHPyUCni5Lo+X/9TkxGMYVxM9VKLraUu7O61HGqKCTdFUFAJoeeZTh+G3gmg64Qqvbb9IURKk56v/tvcmI2XY8D+Kg9Ib3dqKzyBRnYMo8VwDQ54rqVEoa6FvTn073fyQJjQm92RVXyi8HG4Ky6/R3UC9mdLqdok+b/8cIPkBvmbIeT16zdUAjk2pyK0wtSdlj1+8T3RbPf776lGAVs9Xnw9zOGxH/su3pdYtklACToA7rPaAgVJsmsT11lBzW9VOBoX2pqF4u88r/OC4OE/lOEIEPVyZQ2DCyVkq3iRthvZ7Yjbx/5DU7re86rHej+VSm/dLy7YaKCdh6IpRBwIQ4siLoB5gQAj+qTMQRD9XUW7kRXLpdAHdALFuSaJCeGj7gq0qFT8txNRYk322cxWoCWm/b4bTTBtNhxu5X7lsiOWyt0X7+zJfSU5x2pzznaUeW0A8hM5F2ZhbjD5Kzg1MaiBMsM6VUSM41WYRLcXhvaLaMBNlvFylWYFF6xgcBVsOFNrxWsNU/Cj9OPM59fZNJUW6R7oh8mWBkI06Al+ioHCJruVit1TQZypzTiZAjgMBQ92TWxAajmyF75UXQlt5bvqt9zgiCAEBAgrLlf+iU0uZL4GUsBcOuQnhYQgoytIxSCOTzxYAMD8oA8HmMXNZyazwAk9ExfBOwxTaC97AK/GZxgOzTNzG4NMEheM0YpdKP2Xn2KRPoH76hIK47wROIcFdmqwudS+MLgEEc9Dn0o3KpTwL8fCbb2YnhhKkv01rinIikhKXUaItVBw6/w3ID9sQYFEOjOdqYrhWEw8vQWRV4P0DeHpVJ547zO1kRb+YKnGhcxbEo1puRV/gvVkwtpHHAKQpgInYC1ZcP4mYRS8t6j+YlGIQw095cYPrim2NqIe+WY29JR9pKzZ53Vr8vi8XD1tOEFy29hQX8wgcdXbnl3KLxkC9dKollZDm84fP9OZnPe+J3zy2zM/Bxp3hR+CXxP++61LoqYVDJ9OKux0By1YqLVGjKy9HweTlrAV/iX30pdjahGnyt6cXEG0EXgzqIdUzYSxbJevuH0DsLG+bfaZ/J1lVJH5jZ3sFX4iSVhNj+FheewNvXEI8qECFJZbavGKBVjehnZrOoCttVK9dUhhPGy3QjKnVqaDqPZoUlTiIk+9zgalIFqJ12rq7kWVztvaQAeS5mjse73BUkWEyYpJ9Zapi1lMrmkOBUQ/sIiJhVR1SnAtyKO8fqMzpX6B9m/ts0HO2VZkRDIkKirriKFP6Ug/uo71poOoyBhKnAVvVzJEH3rlelrFPcRw8oBvoDMaa0c+8adKuFQ02no0cmZz2N4usj4BywA2ZKlcl64LroxUnKqZQVHuvZHYNcW9mW6bN6rO+AwRaR1jlCxrYxRnBmudCnLA4L7OjahfHYWxIyBNvk1YrAYqH+Yx1Jg14t0Me1vRiv0QMkcTEtj5r3KEBwVQgaZpLqmtanUhk4io42WvNA21Dl07mtlKIpniHyOz7rBaURvsufe8D6W3zhRcxtVp7J1bpWMdEj+34mD/9dzQ+ltOGJpBC65WxBrzYSTcIqjyCvZOjyZWTuGzxU62D5wuAI7ZeivJtbS6uunb0y14nZ+gleSJxAHugQIHfreTQvMzwxcgbxEi+C84OH0SnUytiHbN+s8oWs4IyJcVnAe1Y0ShRYF/Zpxz5X39w46c389roD7JSwdjSmbkkXJsUsE42SMbdNzycU7xcOWla6Qo7aNVR1m/wuUSONwNmQjNQMgAdeFUdbjaRhP0RuMzWKwVsVF0/mzIJa//2XJfWlO9RKUHDolm0B4BOMpz6Y1/bpXi5ZS7Yuf6CfiDtJjRGuR+1F3Nl8Spw1aNqV42Jlrnk9x3jCBwVvFxIJJ6Rpk+beqDpy8IKJHwc7nuHUC0+EbElbHCOdAyAUTTx3xCkp0em+AOMJn+wIdnJP4wGRa9yNDAp2TbUZPTtyg2RL/FuJ2H0BpLZeGNB47SV5f9L1y1D9NsriKINl34+cwaLi5U2gcBZ7lQnKuLsQQ7PNvWle1ZmKVZhXX4bE4r7i4cfDGL+AXo0nhPsObqUomRU/LkMK2tfq0p22m0R0m8PfAPSnQ/OzuG+h/O+JNYMfnFIMokJRtLJJGWv4FVQvOdV38TMwk9P7t3bVFE/FnDGQNlrMjrhDF46zXKQWcjFrIHXh/Xd7BC8uv/jlr6MdCvCGyfATm/Zsu3I8y7OD+pvTnJ18wUvG80VYrv940sF/6PulbrfENCgSQ8L9KL5+r/oFTGR/NcIfWm/5V5SVytvaaxfgSmzn1pi5wL3MhnBT245LCFGkLLAu/mBuvC/ue2pRBJp+2HdofXL26TlA/mlEgiesVEo08hXMJ9Y3nyP8sqtY5H32qHygWvWd56E2sHGy4dz206RpD3KZsoy8OBkvmNetWccGFIZVnf12QrRo3oh8xSZHpq2g3UnWpYYenPJMxaBmF7HUjJD9wdSYGt5Uty5IZDZkShQdGY/z9S8dIGVimoYJwMVoI2CVLxTQ4Sb6+8jPMd3UDHMAca4S5Kicd/CWDQN+4wEI2DHCDHwLv4oy/OEi4jH3LYNuGf+fI3ejqMAV3//aOsMJJbiOc3EttS+zntNZ0FauTHYxHPVnZIru5FTXMQ/LBnYEObHEv4uMHJgQlRUvlzMcPQKbwIsQ06g+gY0X7s9awW/HujPRw83qLntWMNuvq9HSPtspUumyTT4lBXJIWeJxd1TToQ99IrOvZ4IQLzKXyGqwrZCpSfGE4bFwcgsa634f3lm2SBA1OwqQypN6hLxvlS93F+54io+D8bvhqhmocsypEyh0VkM2D55c7F/++KA0QE5x/AyRBKlhCmUykcczAWL4jAlQcE2OWwrfBv7ukm6q4QEzykjOKZTlWcGNm/aPwR873ED+pbNWAjeC8yxDW8168m7Tqzk0pGhM+kN60x5p/u6ZV/x7PjfLuVPOiMEi4tF95pdbKrRX7Q62qJWfZ8MfkRlciq8kghQ0UgH+H4Y5F4L/bKWc8DKZdYLnvHdeRhcRrr6htCI8Zh7aspm0LyWmXow/sc4Lt92w3XIfl2kDuc00/aNUB18qL14mkxtmjlsm5WQD5ZIIvhNLPdXPyg80TdbPLtgb3QWfIwvtGvM4kLulxif9UwMlDFLI8AReFRIeSquHx+b0nPIchBQZu2r2sh7gk6s8iAtfXpiY2cRGv6ntzYpAtUZviZhgOFEBs7RlSm3lVjFOJAvNNEUWSium02SRkEsZoJpSrZwjtsGSRUqi2gCzyiuf9BrBqJFlnv9b5aEIb1b6J1ta9MmaFfEG8P8Hx5KT8ibgy6AKQrWSWZ3oM5YTDkv0dv44sOBTvFgvjFiERCz41v2a24PakEsxI8PIN53GIIz3gP2kNCZdeJdyBNBrq7x4TUX7Fl6tYvPFlTmpm2plAoWX8dMktzcHEoHulxG91518DNUsS8RwIm8fjnNF+n5MPD+6FfBbNIbiy/M6n9ZHP37xvnsl8tbnZOfHH8xfrfBthkM6Jw0Z4f6qMPXOZ9X2U29ebphbF9KePvXFNrvascHnHAF0mpn9fr0c3RQmCOWRp4U1Hri7ZpnFaD1GgpQZKWRDgzOLJSnghuE5cYr5e5mXJ2DcRw0qRv9vKghRn8PSiHIll3+lpGnKZqHf5Cfso03krizbaLBNv+4/i+OvNzr5w+WQsnJtrxh4XmvwPNIYo6NasDT+73w0k9hx32BKpao6R993JYFzXfnXOJZO8tCuwmRqwz2VRjr7S4rtwhLwRqCZ/XqeBgqCnMqi2xVDLW1teywpFwasQ/WVyXH6+JAZQbVzCoXlSxDkhOC3TvDDs2uSM7oSBbvyAtm7acvXeQiiHlUi2GAtjkzoK3OrhvhTTKT5vIReY1z6zASaqpnkUfhRBLhYGkUFuhap8RKYTCU9s3N9uKF/IQ6AC8693uU39QBh+z3DNEyhNIAZOUYYWCVoyJzM3RoET9uqCiJVy6GrMUALoxJFmOk6jUbzinckiOj9Rvj3MMqM8bRUbSxJZmRqmwzD6zMYNA9YCAOgWRw9IhqgBlTtodo7/vr9E6iHy3Pc8G5k1VNk4ZW3iUsvQL6v4LLPjaV8FJl1Hd1PUwNAKy33maRrv9OfYcR+keircgGvSdJxmiS+rhk5t10VMrxNjMPwjpMuW/ifmp3VAUqwDZEA4PuJUru3kHDiiU3sJX5dHowIebJSK1F5T/vm+OcXAwIZUWq0GGiBxA0wk6eWo2DwvuYaTlqDc/ZnyaoC4NYoAKw81gTIrJ6mznag/J07i2l7H/+IAAaNc3uz02PeOPeiCavoaPxiPmK1fQCKA0SGoTaZxxc0AQKkq8buaKk/aOObY1vlPRO7h5tCk5v0XsbnYW73OSr7jC/vA8KWjP7af7mBxzW7UVzdG8cGY6tvdARkPj0G6BJLT0q6BpK52LHVUMc=]]></content>
      <categories>
        <category>平&amp;茂</category>
      </categories>
      <tags>
        <tag>平&amp;茂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[粤语学习]]></title>
    <url>%2F2020%2F08%2F09%2F%E7%B2%A4%E8%AF%AD%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[粤语学习之旅。 粤语拼音声母1b p m f d t n l g k h w j ng gw kw z c s 韵母1ik u ui un ung ut uk yu yun yut m ng 音调1fan1 fan2 fan3 fan3 fan4 fan6 fan7 fat7 fatt8 fat9 词语积累 序号 粤语 中文 录音 1 你我他 2 大家 3 好 4 jat7 ji2 saam1 sei3 ng5 一二三四五 5 luk9 tsat7 baat8 gau2 sap9 六七八九十 6 7 8 9 10 句子积累 序号 粤语 中文 录音 1 2 3 4 5 6 7 8 9 10]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>粤语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长沙之行]]></title>
    <url>%2F2020%2F07%2F29%2F%E9%95%BF%E6%B2%99%E4%B9%8B%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[2020年8月1日—-2020年8月2日，长沙之行。 Plan Travelling 平平 岳麓山]]></content>
      <categories>
        <category>在路上</category>
      </categories>
      <tags>
        <tag>旅行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter远程环境搭建]]></title>
    <url>%2F2020%2F07%2F25%2Fjupyter%E8%BF%9C%E7%A8%8B%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[搭建远程jupyter notebook，用于代码片段备份，学习碎片日常总结。并在服务器上配置crontab定时任务，推送任务到GitHub。 一、配置远程jupyter（一）生成jupyter notebook配置文件1jupyter notebook --generate-config 记住生成配置文件的目录，一般是在/root/.jupyter 中 （二）生成密文密码 打开ipython3 12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: #输入密码Verify password: #确认密码Out[2]: '###此处为你的密文密码###########' （三）修改配置文件vim /root/.jupyter/jupyter_notebook_config.py插入 1234c.NotebookApp.ip='*'c.NotebookApp.password = u'sha1:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 （四）启动jupyter在服务器终端输入: nohup jupyter notebook —allow-root &amp; （五）实现远程访问由于本人在开启jupyter时，给的端口是8889，所以需要给阿里云服务器添加安全规则，开放8889端口 终端输入：服务器ip地址:8889 此时，就可以开心的访问服务器端的jupyter以上参考原文链接：https://blog.csdn.net/web_9705/article/details/80421044 二、配置定时任务（一）添加crontab任务 不要使用crontab -e来添加任务，直接用vim编辑好了使用crontab 文件名添加，使用crontab -l验证。 （二）编写推送脚本 （三）推送结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[高性能and高可用思想]]></title>
    <url>%2F2020%2F05%2F16%2F%E9%AB%98%E6%80%A7%E8%83%BDand%E9%AB%98%E5%8F%AF%E7%94%A8%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[读张开涛———《亿级流量-网站架构核心技术》所得。 1. 高并发1.1 缓存技术1.1.1 应用级缓存1.1.2 HTTP缓存1.1.3 多级缓存1.1.4 连接池1.2 队列技术2. 高可用2.1 限流技术2.1.1 限流算法2.1.1.1 令牌桶算法 按照固定的速率添加令牌—————&gt;限制的是平均流入速率，允许一定程度的突发流量。 2.1.1.2 漏斗算法 按照固定的速率处理请求————-&gt;常量流出速率，平滑突发流入流量。 2.1.1.3 计数器（粗暴）2.1.2 应用级限流2.1.2.1 限流总并发、连接、请求数2.1.2.2 限流总资源数2.1.2.3 限流某一个接口的总并发、请求数2.1.2.4 限流某一个接口的时间窗请求数2.1.2.5 平滑限流某一个接口的请求数2.1.3 单机限流2.1.3.1 Guava限流 Guava RateLimiter———-限制时间窗口内的凭据速率 平滑突发限流（SmoothBurst） 平滑预热限流（SmoothWarningUp） 主要方法：RateLimiter.create()和limiter.acquire() 2.1.4 分布式限流2.1.3.1 Ngnix+Lua2.1.3.2 Redis+Lua​ 思路上就很粗暴！！！比如当前限流为10000QPS/s，直接将当前秒作为key，每一个请求到达的时候都将这个key自增，当一个请求将其自增到10000后，就拒绝访问！这里一个关键的技巧就是将当前的时间的秒作为key！具体的实现见张开涛的《亿级流量网络架构核心技术》p75—-“分布式限流”。 2.1.5 接入层限流2.1.6 节流在特定的时间窗口内对于重复的相同事件最多只处理一次，或者希望限制多个连续相同的事件最小执行时间间隔，可以使用节流（Throttle）实现，防止多个相同的事件连续执行。 2.2 负载均衡和反向代理2.3 隔离技术2.4 降级技术2.5 超时和重试2.6 回滚2.7 压测和预案]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>高性能高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020年6月入职前的规划]]></title>
    <url>%2F2020%2F04%2F27%2F2020%E5%B9%B46%E6%9C%88%E5%85%A5%E8%81%8C%E5%89%8D%E7%9A%84%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[《资治通鉴·周纪一》中有一段话，说“夫事未有不生于微而成于著。圣人之虑远，故能谨其微而治之；众人之识近，故必待其著而后救之。治其微，则用力寡而功多；救其著，则竭力而不能及也。”今年六月即将进入职场，因为没有职场经历，较为惶恐。为了更快适应新的环境和生活方式并对自己未来的职业发展提供指导，特在这学生生涯即将谢幕之际对自己的发展做出规划。]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读-结构思考力by李忠秋-所得]]></title>
    <url>%2F2020%2F04%2F27%2F%E8%AF%BB-%E7%BB%93%E6%9E%84%E6%80%9D%E8%80%83%E5%8A%9Bby%E6%9D%8E%E5%BF%A0%E7%A7%8B-%E6%89%80%E5%BE%97%2F</url>
    <content type="text"><![CDATA[《机构思考力》读书笔记]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>智库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国历代纪年表]]></title>
    <url>%2F2020%2F03%2F28%2F%E4%B8%AD%E5%9B%BD%E5%8E%86%E4%BB%A3%E7%BA%AA%E5%B9%B4%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[纪年表主要数据源自软件“LINGOES灵格斯”，侵删。 史纪时代(王朝国号 帝王 庙号) 年代年号 旧石器时代 170万年前—1万年前 新石器时代 1万年前—4千年前 三皇 伏羲氏，居三皇之一，列五帝之首，以木德王建 都于宛丘（河南淮阳）。太昊即是人们对伏羲的赞词。据说伏羲仰则观象于天，俯则观法于地，旁观鸟兽之文与地之宜，近取诸身，远取诸物，始画八卦，是世界混沌初开，华胥氏踩了雷神的足印生出的儿子。春秋时已建伏羲陵墓。 女娲氏三皇之一，是抟土作人的人类始祖；传说女娲炼七彩石补天，并造就了人类。是母系社会神话的反映。 燧人氏是三皇之一，传说中发明火的第一人，故人们把他称作“火祖”。 同时又传说燧人氏诞生于商丘，其一生多活动于商丘一带。 五帝 黄帝轩辕氏 公元前2600—公元前2100 颛顼高阳氏 帝喾高辛氏 唐尧 虞舜 夏 禹 公元前2070—公元前1600 启 太康 仲康 相 少康 杼 槐 芒 泄 不降 扃 廑 孔甲 皋 发 癸(桀) 商 商前期 汤 公元前1600—公元前1300 太丁 外丙 仲壬 太甲 沃丁 太庚 小甲 雍己 太戊 仲丁 外壬 和亶甲 祖乙 祖辛 沃甲 祖丁 南庚 阳甲 盘庚 商后期 盘庚 公元前1300—公元前1046 小辛 小乙 武丁 祖庚 祖甲 廪辛 康丁 武乙 文丁 帝乙 帝辛 (纣) 周 西周 武王(姬发) 公元前1046—公元前771年 成王(诵) 康王(钊) 昭王(瑕) 穆王(满) 共王(繄扈) 懿王(囏 孝王(辟方) 夷王(燮 厉王(胡) 共和 庚申前841 宣王(静) 甲戌前827 幽王(宫湦) 庚申前781 东周 平王(姬宜臼) 辛末前770 公元前770—前256 桓王(林) 壬戊前719 庄王(佗) 乙酉前696 釐王(胡齐) 庚子前681 惠王(阆) 乙巳前676 襄王(郑) 庚午前651 顷王(壬臣) 癸卯前618 匡王(班) 己酉前612 定王(瑜) 乙卯前606 简王(夷) 丙子前585 灵王(泄心) 庚寅前571 景王(贵) 丁巳前544 悼王(猛) 辛巳前520 敬王(匄) 壬午前519 元王(仁) 丙寅前475 贞定王(介) 癸酉前468 哀王(去疾) 庚子前441 思王(叔) 庚子前441 考王(嵬) 辛丑前440 威烈王(午) 丙辰前425 安王(骄) 庚辰前401 烈王(喜) 丙午前375 显王(扁) 癸丑前368 慎靓王(定) 辛丑前320 赧王(延) 丁未前314 春秋 公元前770—前476 战国 公元前475—前221 秦 昭襄王(嬴则，又名稷) 乙卯前306 公元前221—前206 孝文王(柱) 辛亥前250 庄襄王(子楚) 壬子前249 始皇帝(政) 乙卯前246 二世皇帝(胡亥) 壬辰前209 汉 西汉 高帝(刘邦) 乙未前206 -- 公元前206—公元23 惠帝(盈) 丁未前194 -- 高后(吕雉) 甲寅前187 -- 文帝(刘恒) 壬戌前179 -- 戊寅前163 后元 景帝(启) 乙酉前156 壬辰前149 中元 戊戌前143 后元 武帝(彻) 辛丑前l40 建元 丁未前134 元光 癸丑前128 元朔 己未前122 元狩 乙丑前116 元鼎 辛未前110 元封 丁丑前104 太初 辛巳前100 天汉 乙酉前96 太始 己丑前92 征和 癸巳前88 后元 昭帝(弗陵) 乙未前86 始元 辛丑前80 元凤 丁未前74 元平 宣帝(询) 戊申前73 本始 壬子前69 地节 丙辰前65 元康 庚申前61 神爵 甲子前57 五凤 戊辰前53 甘露 壬申前49 黄龙 元帝(奭) 癸酉前48 初元 戊寅前43 永光 癸未前38 建昭 戊子前33 竟宁 成帝(骜) 己丑前32 建始 癸巳前28 河平 丁酉前24 阳朔 辛丑前20 鸿嘉 乙巳前16 永始 己酉前12 元延 癸丑前8 绥和 哀帝(刘欣) 乙卯前6 建平 己未前2 元寿 平帝(衍) 辛酉公元1年 元始 孺子婴(王莽摄政) 丙寅公元6 居摄 戊辰公元8 初始 [新]王莽始 己巳公元9 建国 甲戌公元14 天凤 庚辰公元20 地皇 更始帝(刘玄) 癸未公元23 更始 东汉 光武帝(刘秀) 乙酉公元25 建武 公元25—公元220 丙辰公元56 建武中元 明帝(庄) 戊午公元58 永平 章帝(炟) 丙子公元76 建初 甲申公元84 元和 丁亥公元87 章和 和帝(肇) 己丑89 永元 乙巳105 元兴 殇帝(隆) 丙午106 延平 安帝(祜) 丁未107 永初 甲寅114 元初 庚申120 永宁 辛酉12l 建光 壬戌122 延光 顺帝(保) 丙寅126 永建 壬申132 阳嘉 丙子136 永和 壬午142 汉安 甲申144 建康 冲帝(炳) 乙酉145 永憙(嘉) 质帝(缵) 丙戌146 本初 桓帝(志) 丁亥147 建和 庚寅150 和平 辛卯151 元嘉 癸巳153 永兴 乙未155 永寿 戊戌158 延熹 丁未167 永康 灵帝(宏) 戊申168 建宁 壬子172 熹平 戊午178 光和 甲子184 中平 献帝(协) 庚午190 初平 甲戌194 兴平 丙子196 建安 庚子220 延康 三国 魏 文帝(曹丕) 庚子220 黄初 公元220—公元265 明帝(叡) 丁未227 太和 癸丑233 青龙 丁巳237 景初 齐王(芳) 庚申240 正始 己巳249 嘉平 高贵乡公(髦) 甲戌254 正元 丙子256 甘露 元帝(奂)(陈留王) 庚辰260 景元 甲申264 咸熙 蜀 昭烈帝(刘备) 辛丑221 章武 公元221—公元263 后主(禅) 癸卯223 建兴 戊午238 延熙 戊寅258 景耀 癸未263 炎兴 吴 大帝(孙权) 壬寅222 黄武 公元222—公元280 己酉229 黄龙 壬子232 嘉禾 戊午238 赤乌 辛未251 太元 壬申252 神凤 会稽王(亮) 壬申252 建兴 甲戌254 五凤 丙子256 太平 景帝(休) 戊寅258 永安 乌程侯(皓) 甲申264 元兴 乙酉265 甘露 丙戌266 宝鼎 己丑269 建衡 壬辰272 凤凰 乙未275 天册 丙申276 天玺 丁酉277 天纪 晋 西晋 武帝(司马炎) 乙酉265 泰始 公元265—公元316 乙未275 咸宁 庚子280 太康 庚戌290 太熙 惠帝(司马衷) 庚戌290 永熙 辛亥291 永平 辛亥291 元康 庚申300 永康 辛酉301 永宁 壬戌302 太安 甲子304 永安 甲子304 建武 甲子304 永安 甲子304 永兴 丙寅306 光熙 怀帝(炽) 丁卯307 永嘉 愍帝(邺) 癸酉313 建兴 东晋 元帝(司马睿) 丁丑317 建武 公元317—公元420 戊寅318 大兴 壬午322 永昌 明帝(绍) 壬午322闰 永昌 癸未323 太宁 成帝(衍) 乙酉325闰 太宁 丙戌326 咸和 乙未335 咸康 康帝(岳) 癸卯343 建元 穆帝(聃) 乙巳345 永和 丁巳357 升平 哀帝(丕) 壬戌362 隆和 癸亥363 兴宁 海西公(奕) 丙寅366 太和 简文帝(昱) 辛末371 咸安 孝武帝(曜) 癸酉373 宁康 丙子376 太元 安帝(德宗) 丁酉397 隆安 壬寅402 元兴 乙巳405 义熙 恭帝(德文) 己未419 元熙 十六国 公元304—公元439 南北朝 宋 武帝(刘裕) 庚申420 永初 420—479 少帝(义符) 癸亥423 景平 文帝(义隆) 甲子424 元嘉 孝武帝(骏) 甲午454 孝建 丁酉457 大明 前废帝(子业) 乙巳465 永光 乙巳465 景和 明帝(彧) 乙巳465 泰始 壬子472 泰豫 后废帝(昱)(苍梧王) 癸丑473 元徽 顺帝(凖) 丁巳477 昇明 齐 高帝(萧道成) 己未479 建元 479—502 武帝(赜) 癸亥483 永明 鬱林王(昭业) 甲戌494 隆昌 海陵王(昭文) 甲戌494 延兴 明帝(鸾) 甲戌494 建武 戊寅498 永泰 东昏侯(宝卷) 己卯499 永元 和帝(宝融) 辛巳501 中兴 梁 武帝(萧衍) 壬午502 天监 502—557 庚子520 普通 丁未527 大通 己酉529 中大通 乙卯535 大同 丙寅546 中大同 丁卯547 太清 简文帝(纲) 庚午550 大宝 元帝(绎) 壬申552 承圣 敬帝(方智) 乙亥555 绍泰 丙子556 太平 陈 武帝(陈霸先) 丁丑557 永定 557—589 文帝(蒨) 庚辰560 天嘉 丙戌566 天康 废帝(伯宗)(临海王) 丁亥567 光大 宣帝(顼) 己丑569 太建 后主(叔宝) 癸卯583 至德 丁未587 祯明 北魏 道武帝(拓跋登珪) 丙戌386 登国 386—534 丙申396 皇始 戊戌398 天兴 甲辰404 天赐 明元帝(嗣) 己酉409 永兴 甲寅414 神瑞 丙辰416 泰常 太武帝(焘) 甲子424 始光 戊辰428 神(上鹿下加) 壬申432 延和 乙亥435 太延 庚辰440 太平真君 辛卯451 正平 南安王(拓跋余) 壬辰452 永(承)平 文成帝(濬) 壬辰452 兴安 甲午454 兴光 乙未455 太安 庚子460 和平 献文帝(弘) 丙午466 天安 丁未467 皇兴 孝文帝(元宏) 辛亥471 延兴 丙辰476 承明 丁巳477 太和 宣武帝(恪) 庚辰500 景明 甲申504 正始 戊子508 永平 壬辰512 延昌 孝明帝(诩) 丙申516 熙平 戊戌518 神龟 庚子520 正光 乙巳525 孝昌 戊申528 武泰 孝庄帝(子攸) 戊申528 建义 戊申528 永安 长广王(晔) 庚戌530 建明 节闵帝(恭) 辛亥531 普泰 安定王(朗) 辛亥531 中兴 孝武帝(脩) 壬子532 太昌 壬子532 永兴 壬子532 永熙 东魏 孝静帝(元善见) 甲寅534 天平 534—550 戊午538 元象 己未539 兴和 癸亥543 武定 北齐 文宣帝(高洋) 庚午550 天保 550—577 废帝(殷) 庚辰560 乾明 孝昭帝(演) 庚辰560 皇建 武成帝(湛) 辛巳561 太宁 壬午562 河清 后主(纬) 乙酉565 天统 庚寅570 武平 丙申576 隆化 幼主(恒) 丁酉577 承光 西魏 文帝(元宝炬) 乙卯535 大统 537—557 废帝(钦) 壬申552 -- 恭帝(廓) 甲戌554 -- 北周 孝闵帝(宇文觉) 丁丑557 -- 557—581 明帝(毓) 丁丑557 -- 己卯559 武成 武帝(邕) 辛巳56l 保定 丙戌566 天和 壬辰572 建德 戊戌578 宣政 宣帝(赟) 己亥579 大成 静帝(阐) 己亥579 大象 辛丑581 大定 隋 文帝(杨坚) 辛丑581 开皇 581—618 辛酉601 仁寿 炀帝(广) 乙丑605 大业 恭帝(侑) 丁丑617 义宁 唐 高祖(李渊) 戊寅618 武得 618—907 太宗(世民) 丁亥627 贞观 高宗(治) 庚戌650 永徽 丙辰656 显庆 辛酉661 龙朔 甲子664 麟德 丙寅666 乾封 戊辰668 总章 庚午670 咸亨 甲戌674 上元 丙子676 仪凤 己卯679 调露 庚辰680 永隆 辛巳681 开耀 壬午682 永淳 癸未683 弘道 中宗(显又名哲) 甲申684 嗣圣 睿宗(旦) 甲申684 文明 武后(武曌) 甲申684 光宅 乙酉685 垂拱 己丑689 永昌 庚寅690 载初 武后改国号为周 庚寅690 天授 壬辰692 如意 壬辰692 长寿 甲午694 延载 乙未695 证圣 乙未695 天册万岁 丙申(腊)696 万岁登封 丙申696 万岁通天 丁酉697 神功 戊戌698 圣历 庚子700 久视 辛丑701 大足 辛丑701 长安 中宗李显又名哲复唐号 乙巳705 神龙 丁未707 景龙 睿宗(旦) 庚戌710 景云 壬子712 延和 玄宗(隆基) 壬子712 先天 癸丑713 开元 壬午742 天宝 肃宗(亨) 丙申756 至德 戊戌758 乾元 庚子(闰)760 上元 辛丑761 -- 代宗(豫) 壬寅762 宝应 癸卯763 广德 乙巳765 永泰 丙午766 大历 德宗(适) 庚申780 建中 甲子784 兴元 乙丑785 贞元 顺宗(诵) 乙酉805 永贞 宪宗(纯) 丙戌806 元和 穆宗(恒) 辛丑821 长庆 敬宗(湛) 乙巳825 宝历 文宗(昂) 丙午826 宝历 丁未827 大(太)和 丙辰836 开成 武宗(炎) 辛酉841 会昌 宣宗(忱) 丁卯847 大中 懿宗(漼) 己卯859 大中 庚辰860 咸通 僖宗(儇) 癸巳873 咸通 甲午874 乾符 庚子880 广明 辛丑881 中和 乙巳885 光启 戊申888 文德 昭宗(晔) 己酉889 龙纪 庚戌890 大顺 壬子892 景福 甲寅894 乾宁 戊午898 光化 辛酉9O1 天复 甲子(闰)904 天祐 哀帝(柷) 甲子904 天祐 五代十国 后梁 太祖(朱晃又名温、全忠) 丁卯907 开平 907—923 辛未911 乾化 末帝(瑱) 癸酉913 乾化 乙亥915 贞明 辛巳92l 龙德 后唐 庄宗(李存勖) 癸未923 同光 923—936 明宗(亶) 丙戌926 天成 庚寅930 长兴 闵帝(从厚) 甲午934 应顺 末帝(从珂) 甲午934 清泰 后晋 高祖(石敬瑭) 丙申936 天福 936—946 出帝(重贵) 壬寅942 天福 甲辰944 开运 后汉 高祖(刘暠，本名知远) 丁未947 天福 947—950 戊申948 乾祐 隐帝(承祐) 戊申948 乾祐 后周 太祖(郭威) 辛亥951 广顺 951—960 甲寅954 显德 世宗(柴荣) 甲寅954 显德 恭帝(宗训) 己未959 显德 十国 902—979 宋 北宋 太祖(赵匡胤) 庚申960 建隆 960—1127 癸亥963 乾德 戊辰968 开宝 太宗(炅，名匡义，光义) 丙子976 太平兴国 甲申984 雍熙 戊子988 端拱 庚寅990 淳化 乙未995 至道 真宗(恒) 戊戌998 咸平 甲辰1004 景德 戊申1008 大中祥符 丁巳1017 天禧 壬戊1022 乾兴 仁宗(祯) 癸亥1023 天圣 壬申1032 明道 甲戌1034 景祐 戊寅1038 宝元 庚辰1040 康定 辛巳1041 庆历 己丑1049 皇祐 甲午1054 至和 丙申1056 嘉祐 英宗(曙) 甲辰1064 治平 神宗(顼) 戊申1068 熙宁 戊午1078 元丰 哲宗(煦) 丙寅1086 元祐 甲戌1094 绍圣 戊寅1098 元符 徽宗(佶) 辛巳1101 建中靖国 壬午1102 崇宁 丁亥1107 大观 辛卯1111 政和 戊戌1118 重和 己亥1119 宣和 钦宗(桓) 丙午1126 靖康 南宋 高宗(赵构) 丁未1127 建炎 1127—1297 辛亥1131 绍兴 孝宗(昚) 癸未1183 隆兴 乙酉1165 乾道 甲午1174 淳熙 光宗(惇) 庚戌1190 绍熙 宁宗(扩) 乙卯1195 庆元 辛酉1201 嘉泰 乙丑1205 开禧 戊辰1208 嘉定 理宗(昀) 乙酉1225 宝庆 戊子1228 绍定 甲午1234 端平 丁酉1237 嘉熙 辛丑1241 淳祐 癸丑1253 宝祐 己未1259 开庆 庚申1260 景定 度宗(赵禥) 乙丑1265 咸淳 恭帝(繁显去掉右边页) 乙亥1275 德祐 端宗(昰) 丙子1276 景炎 帝昺(昺) 戊寅1278 祥兴 辽 太祖(耶律阿保机) 丁卯907 -- 907—1125 丙子916 神册 壬午922 天赞 丙戌926 天显 太宗(德光) 丁亥927 天显 戊戌938 会同 丁未947 大同 世宗(阮) 丁未947 天禄 穆宗(璟) 辛亥951 应历 景宗(贤) 己巳969 保宁 己卯979 乾亨 圣宗(隆绪) 壬午982 乾亨 癸未983 统和 壬子1012 开泰 辛酉1021 太平 兴宗(宗真) 辛未1031 景福 壬申1032 重熙 道宗(洪基) 乙未1055 清宁 乙巳1065 咸雍 乙卯1075 大(太)康 乙丑1085 大安, 乙亥1095 寿昌(隆) 天祚帝(延禧) 辛巳1101 乾统 辛卯1111 天庆 辛丑1121 保大 西夏 景宗李元昊 1032 显道 1038—1227 开运 广运 大庆 天授 礼法 延祚 毅宗李谅祚 1049 延嗣 宁国 天祐 垂圣 福圣 承道 奢单都 拱化 惠宗李秉常 1067 乾道 天赐 礼盛 国庆 大安 天安 礼定 崇宗李乾顺 1086 天仪 治平 天祐 民安 永安 贞观 雍宁 元德 正德 大德 仁宗李仁孝 1140 大庆 人庆 天盛 乾祐 桓宗李纯祐 1193 天庆 襄宗李安全 1206 应天 皇建 神宗李遵顼 1211 光定 献宗李德旺 1223 乾定 末主李眈 1226 金 太祖(完颜旻本名阿骨打) 乙未1115 收国 1115—1234 丁酉1117 天辅 太宗(晟) 癸卯1123 天会 熙宗(亶) 乙卯1135 天会 戊午1138 天眷 辛酉1141 皇统 海陵王(亮) 己巳1149 天德 癸酉1153 贞元 丙子1156 正隆 世宗(完颜雍) 辛巳1161 大定 章宗(璟) 庚戌1190 明昌 丙辰1196 承安 辛酉1201 泰和 卫绍王(永济) 己巳1209 大安 壬申1212 崇庆 癸酉1213 至宁 宣宗(珣) 癸酉1213 贞祐 丁丑1217 兴定 壬午1222 元光 哀宗(守绪) 甲申1224 正大 壬辰1232 开兴 壬辰1232 天兴 元 太祖(孛儿只斤铁木真) 丙寅1206 -- 1279—1368 拖雷(监国) 戊子1228 -- 太宗(窝阔台) 己丑1229 -- 乃马真后(称制) 壬寅1242 -- 定宗(贵由) 丙午1246 -- 海迷失后(称制) 己酉1249 -- 宪宗(蒙哥) 辛亥1251 -- 世祖(忽必烈) 庚申1260 中统 甲子1264 至元 成宗(铁穆耳) 乙未1295 元贞 丁酉1297 大德 武宗(海山) 戊申1308 至大 仁宗(爱育黎拔力八达) 壬子1312 皇庆 甲寅1314 延祐 英宗(硕德八剌) 辛酉1321 至治 泰定帝(也孙铁木儿) 甲子1324 泰定 戊辰1328 致和 天顺帝(阿速吉八) 戊辰1328 天顺 文宗(图帖睦尔) 戊辰1328 天历 明宗(和世'左王右束') 己巳1329 -- 庚午1330 至顺 宁宗(懿璘质班) 壬申1332 至顺 顺帝(妥懽帖睦尔) 癸酉1333 至顺 癸酉1333 元统 乙亥1335 (后)至元 辛巳1341 至正 明 太祖(朱元璋) 戊申1368 洪武 1368—1644 惠帝(允炆) 己卯1399 建文 成祖(棣) 癸未1403 永乐 仁宗(高炽) 乙巳1425 洪熙 宣宗(瞻基) 丙午1426 宣德 英宗(祁镇) 丙辰1436 正统 代宗(祁钰)(景帝) 庚午1450 景泰 英宗(祁镇) 丁丑1457 天顺 宪宗(见深) 乙酉1465 成化 孝宗(祐樘) 戊申1488 弘治 武宗(厚照) 丙寅1506 正德 世宗(厚熜) 壬午1522 嘉靖 穆宗(载垕) 丁卯1567 隆庆 神宗(翊钧) 癸酉1573 万历 光宗(常洛) 庚申1620 泰昌 熹宗(由校) 辛酉1621 天启 思宗(由检) 戊辰1628 崇祯 清 太祖(爱新觉罗努尔哈赤) 丙辰1616 天命 1644—1911 太宗(皇太极) 丁卯1627 天聪 丙子1636 崇德 世祖(福临) 甲申1644 顺治 圣祖(玄烨) 壬寅1662 康熙 世宗(胤禛) 癸卯1723 雍正 高宗(弘历) 丙辰1736 乾隆 仁宗(颙琰) 丙辰1796 嘉庆 宣宗(旻宁) 辛巳1821 道光 文宗(奕詝) 辛亥1851 咸丰 穆宗(载淳) 壬戌1862 同治 德宗(载湉) 乙亥1875 光绪 溥仪 己酉1909 宣统]]></content>
      <categories>
        <category>那些年</category>
      </categories>
      <tags>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[规划工具]]></title>
    <url>%2F2020%2F03%2F07%2F%E8%A7%84%E5%88%92%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[SWOT、PDCA、6W2H、SMART、WBS、时间管理、二八原则· 一、SWOT分析法 Strengths：优势 Weaknesses：劣势 Opportunities：机会 Threats：威胁 意义：帮您清晰地把握全局，分析自己在资源方面的优势与劣势，把握环境提供的机会，防范可能存在的风险与威胁，对我们的成功有非常重要的意义。 二、PDCA循环规则 Plan：制定目标与计划； Do：任务展开，组织实施； Check：对过程中的关键点和最终结果进行检查； Action：纠正偏差，对成果进行标准化，并确定新的目标，制定下一轮计划。 意义：每一项工作，都是一个pdca循环，都需要计划、实施、检查结果，并进一步进行改进，同时进入下一个循环，只有在日积月累的渐进改善中，才可能会有质的飞跃，才可能取得完善每一项工作，完善自己的人生。 三、6W2H法 What：工作的内容和达成的目标； Why：做这项工作的原因； Who：参加这项工作的具体人员，以及负责人； When：在什么时间、什么时间段进行工作； Where：工作发生的地点 ； Which：哪一种方法或途径； How：用什么方法进行； How much：需要多少成本？ 意义：做任何工作都应该从6W2H来思考，这有助于我们的思路的条理化，杜绝盲目性。我们的汇报也应该用6W2H，能节约写报告及看报告的时间。 四、SMART原则 Specific 具体的； Measurable 可测量的； Attainable 可达到的； Relevant 相关的； Time based 时间的； 意义：人们在制定工作目标或者任务目标时，考虑一下目标与计划是不是SMART化的。只有具备SMART化的计划才是具有良好可实施性的，也才能指导保证计划得以实现。 特别注明： 有的又如此解释此原则： ——S代表具体 (Specific) ，指绩效考核要切中特定的工作指标，不能笼统； ——M代表可度量 (Measurable) ，指绩效指标是数量化或者行为化的，验证这些绩效指标的数据或者信息是可以获得的； ——A代表可实现 (Attainable) ，指绩效指标在付出努力的情况下可以实现，避免设立过高或过低的目标； ——R代表现实性 (realistic) ，指绩效指标是实实在在的，可以证明和观察； ——T代表有时限 (time bound) ，注重完成绩效指标的特定期限。 五、时间管理-重要与紧急 B、重要不紧急准备工作预防措施价值观的澄清计划人际关系的建立真正的再创造增进自己的能力 A、重要且紧急紧急状况迫切的问题限期完成的工作你不做其他人也不能做 D、不重要不紧急忙碌琐碎的事广告函件电话逃避性活动等待时间 C、紧急不重要造成干扰的事、电话、信件、报告会议许多迫在眉捷的急事符合别人期望的事 优先顺序=重要性*紧迫性在进行时间安排时，应权衡各种事情的优先顺序，要学会“弹钢琴”。 对工作要有前瞻能力，防患于未然，如果总是在忙于救火，那将使我们的工作永远处理被动之中。 六、任务分解法[WBS]即Work Breakdown Structure， 如何进行WBS分解：目标→任务→工作→活动 WBS分解的原则： 将主体目标逐步细化分解，最底层的任务活动可直接分派到个人去完成；每个任务原则上要求分解到不能再细分为止。 WBS分解的方法： 至上而下与至下而上的充分沟通； 一对一个别交流； 小组讨论。 WBS分解的标准： 分解后的活动结构清晰； 逻辑上形成一个大的活动； 集成了所有的关键因素包含临时的里程碑和监控点； 所有活动全部定义清楚。 意义：学会分解任务，只有将任务分解得足够细，您才能心里有数，您才能有条不紊地工作，您才能统筹安排您的时间表。 七、二八原则巴列特定律：“总结果的80%是由总消耗时间中的20%所形成的。” 按事情的“重要程度”编排事务优先次序的准则是建立在“重要的少数与琐碎的多数”的原理的基础上。 举例说明： 80%的销售额是源自20%的顾客； 80%的电话是来自20%的朋友； 80%的总产量来自20%的产品； 80%的财富集中在20%的人手中； 这启示我们：在工作中要善于抓主要矛盾，善于从纷繁复杂的工作中理出头绪，把资源用在最重要、最紧迫的事情上。 来自 https://zhuanlan.zhihu.com/p/348457630]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ExtractFlowWithPython]]></title>
    <url>%2F2020%2F03%2F06%2FExtractFlowWithPython%2F</url>
    <content type="text"><![CDATA[使用Python+Scapy从pcap文件中提取流，输出pcap文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166#!/usr/bin/env python#encoding=utf-8"""@author: TianMao@contact: tianmao1994@yahoo.com@file: split-pcap.py@time: 19-11-27 下午6:38@desc: 功能：按照TCP流切分pcap文件 参考：https://github.com/mao-tool/packet-analysis 环境：Scapy 使用：python split-pcap.py test.pcap 输出：test.pcap_220.194.64.35-443_192.168.137.22-56458_split.pcap 问题：当前输出为splitcap文件的一般？疑似这里处理的是双向流？"""import sysimport reimport glob# This is needed to suppress a really irrating warning message when scapy# is importedimport logginglogging.getLogger("scapy.runtime").setLevel(logging.ERROR)try: from scapy.all import*except ImportError: print "scapy is not installed. See comments for installation suggestions" exit ()# argument processing, require just the file name. If a second argument# is provided make sure its an integerif len (sys.argv) &lt; 2 or len (sys.argv) &gt; 3: print "Usage is: split-pcap.py file-name [packet-count]" print "Try\n grep -A 20 Usage: " + sys.argv[0] + \ " | head -20\nfor details" exit ()if len (sys.argv) == 3: inputFileString = sys.argv [1] try: inputTotalPackets = int (sys.argv [2]) except ValueError: print "The second argument must be an integer &lt;" + \ sys.argv [2] + "&gt; does appear to be an integer" exit ()else: inputFileString = sys.argv [1] inputTotalPackets = 0# 保存文件夹out_dir = "../../../../data/1/raw_2/"# try opening the file.try: pcapIn = PcapReader (inputFileString)except IOError: print "It doesn't look like " + inputFileString + " exists" exit()except NameError: print "It doesn't look like " + inputFileString + \ " is a file that can be processed." print "Note that this script cannot process pcapng files. Review the " print "usage details for ideas on how to convert from pcapng to pcap" exit ()# Extract out just the the file name. Note that I assume the the ".*/" match# is greedy and will match until the last "/" character in the string. If# the match fails there are no "/" characters so the whole string must be the# name.x = re.search ("^.*/(.*$)", inputFileString)try: prefix = x.group(1) + "_"except: prefix = inputFileString + "_"# Look for prefix*_split.pcap files. If you find them print a# warning and exit.t = len (glob (prefix + "*_split.pcap"))if t &gt; 0: print "There are already " + str (t) + " files with the name " + \ prefix + "*_split.pcap." print "Delete or rename them or change to a different directory to" print "avoid adding duplicate packets into the " + prefix + \ "*_split.pcap trace files." exit ()# 判断是否存在当前文件的文件夹if not os.path.exists(out_dir + inputFileString): os.makedirs(out_dir + inputFileString)pcapOutName = ""oldPcapOutName = ""packetCount = 0donePercentage = 0;oldDonePercentage = -1# Loop for each packet in the filefor aPkt in pcapIn:# count the packets read packetCount = packetCount + 1# If the packet contains a TCP header extract out the IP addresses and# port numbers if TCP in aPkt: ipSrc = aPkt[IP].src tcpSport = aPkt[TCP].sport ipDst = aPkt[IP].dst tcpDport = aPkt[TCP].dport# put things in some sort of cannonical order. It doesn't really matter# what the order is as long as packets going in either direction get the# same order. if ipSrc &gt; ipDst: pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap" elif ipSrc &lt; ipDst: pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap" elif tcpSport &gt; tcpDport: pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap" else: pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"# If the current packet should be written to a different file from the last# packet, close the current output file and open the new file for append# save the name of the newly opened file so we can compare it for the next# packet. if pcapOutName != oldPcapOutName: if oldPcapOutName != "": pcapOut.close() if type(aPkt) == scapy.layers.l2.Ether: lkType = 1 elif type (aPkt) == scapy.layers.l2.CookedLinux: lkType = 113 else: print "Unknown link type: " type (aPkt) print " -- exiting" exit # 修改文件路劲 pcapOutName = out_dir+inputFileString+"/"+pcapOutName pcapOut = PcapWriter (pcapOutName, linktype=lkType, append=True) oldPcapOutName = pcapOutName# write the packet pcapOut.write (aPkt)# Write the progress information, either percentages if we had a packet-count# argument or just the packet count. if inputTotalPackets &gt; 0: donePercentage = packetCount * 100 / inputTotalPackets if donePercentage &gt; oldDonePercentage: print "Percenage done: ", donePercentage oldDonePercentage = donePercentage else: print packetCount]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java相关代码分析]]></title>
    <url>%2F2019%2F12%2F08%2FJava%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[分析Java相关的代码片段、细节等。 Java，Spring，MySQL中的时间问题JavaJava8之前 java.util.Date Date如果不格式化，打印出的日期可读性差 使用SimpleDateFormat对时间进行格式化，但SimpleDateFormat是线程不安全的 案例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;/** * 关于java.util.Date、java.sql.Timestamp和String之间的互相转换的方法 */public class DateUtil &#123; /** * 将String字符串转换为java.util.Date格式日期 * * @param strDate * 表示日期的字符串 * @param dateFormat * 传入字符串的日期表示格式（如："yyyy-MM-dd HH:mm:ss"） * @return java.util.Date类型日期对象（如果转换失败则返回null） */ public static java.util.Date strToUtilDate(String strDate, String dateFormat) &#123; SimpleDateFormat sf = new SimpleDateFormat(dateFormat); java.util.Date date = null; try &#123; date = sf.parse(strDate); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return date; &#125; /** * 将String字符串转换为java.sql.Timestamp格式日期,用于数据库保存 * * @param strDate * 表示日期的字符串 * @param dateFormat * 传入字符串的日期表示格式（如："yyyy-MM-dd HH:mm:ss"） * @return java.sql.Timestamp类型日期对象（如果转换失败则返回null） */ public static java.sql.Timestamp strToSqlDate(String strDate, String dateFormat) &#123; SimpleDateFormat sf = new SimpleDateFormat(dateFormat); java.util.Date date = null; try &#123; date = sf.parse(strDate); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; java.sql.Timestamp dateSQL = new java.sql.Timestamp(date.getTime()); return dateSQL; &#125; /** * 将java.util.Date对象转化为String字符串 * * @param date * 要格式的java.util.Date对象 * @param strFormat * 输出的String字符串格式的限定（如："yyyy-MM-dd HH:mm:ss"） * @return 表示日期的字符串 */ public static String dateToStr(java.util.Date date, String strFormat) &#123; SimpleDateFormat sf = new SimpleDateFormat(strFormat); String str = sf.format(date); return str; &#125; /** * 将java.sql.Timestamp对象转化为String字符串 * * @param time * 要格式的java.sql.Timestamp对象 * @param strFormat * 输出的String字符串格式的限定（如："yyyy-MM-dd HH:mm:ss"） * @return 表示日期的字符串 */ public static String dateToStr(java.sql.Timestamp time, String strFormat) &#123; DateFormat df = new SimpleDateFormat(strFormat); String str = df.format(time); return str; &#125; /** * 将java.sql.Timestamp对象转化为java.util.Date对象 * * @param time * 要转化的java.sql.Timestamp对象 * @return 转化后的java.util.Date对象 */ public static java.util.Date timeToDate(java.sql.Timestamp time) &#123; return time; &#125; /** * 将java.util.Date对象转化为java.sql.Timestamp对象 * * @param date * 要转化的java.util.Date对象 * @return 转化后的java.sql.Timestamp对象 */ public static java.sql.Timestamp dateToTime(java.util.Date date) &#123; String strDate = dateToStr(date, "yyyy-MM-dd HH:mm:ss SSS"); return strToSqlDate(strDate, "yyyy-MM-dd HH:mm:ss SSS"); &#125; /** * 返回表示系统当前时间的java.util.Date对象 * @return 返回表示系统当前时间的java.util.Date对象 */ public static java.util.Date nowDate()&#123; return new java.util.Date(); &#125; /** * 返回表示系统当前时间的java.sql.Timestamp对象 * @return 返回表示系统当前时间的java.sql.Timestamp对象 */ public static java.sql.Timestamp nowTime()&#123; return dateToTime(new java.util.Date()); &#125;&#125; Java8及其之后(重点关注表示范围) java.time.LocalDate Modifier and Type Field and Description static LocalDate MAX The maximum supported LocalDate, ‘+999999999-12-31’. static LocalDate MIN The minimum supported LocalDate, ‘-999999999-01-01’. java.time.LocalTime Modifier and Type Field and Description static LocalTime MAX The maximum supported LocalTime, ‘23:59:59.999999999’. static LocalTime MIDNIGHT The time of midnight at the start of the day, ‘00:00’. static LocalTime MIN The minimum supported LocalTime, ‘00:00’. static LocalTime NOON The time of noon in the middle of the day, ‘12:00’. java.time.LocalDateTime Modifier and Type Field and Description static LocalDateTime MAX The maximum supported LocalDateTime, ‘+999999999-12-31T23:59:59.999999999’. static LocalDateTime MIN The minimum supported LocalDateTime, ‘-999999999-01-01T00:00:00’. java.time.Instant Modifier and Type Field and Description static Instant EPOCH Constant for the 1970-01-01T00:00:00Z epoch instant. static Instant MAX The maximum supported Instant, ‘1000000000-12-31T23:59:59.999999999Z’. static Instant MIN The minimum supported Instant, ‘-1000000000-01-01T00:00Z’. 案例 1//案例 SpringMySQL支持的数据类型 TIME（不常用）(hhh:mm:ss) MySQL retrieves and displays TIME values in ‘hh:mm:ss’ format (or ‘hhh:mm:ss’ format for large hours values). TIME values may range from ‘-838:59:59’ to ‘838:59:59’. DATE（常用，精度到天，不保存时区信息） The supported range is &#39;1000-01-01&#39; to &#39;9999-12-31&#39; DATETIME（常用，支持到了微秒级别，不保存时区信息） The DATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in &#39;YYYY-MM-DD hh:mm:ss&#39; format. The supported range is &#39;1000-01-01 00:00:00&#39; to &#39;9999-12-31 23:59:59&#39;. TIMESTAMP（常用，支持到了微秒级别，保存了时区信息） The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of &#39;1970-01-01 00:00:01&#39; UTC to &#39;2038-01-19 03:14:07&#39; UTC. 自动更新 123456789CREATE TABLE t1 ( ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, dt DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);CREATE TABLE t2 ( dt1 DATETIME ON UPDATE CURRENT_TIMESTAMP, -- default NULL dt2 DATETIME NOT NULL ON UPDATE CURRENT_TIMESTAMP -- default 0); YEAR（不常用）(1901 to 2155, or 0000) Java包 java.sql.Date java.lang.Object java.util.Date java.sql.Date java.sql.Timestamp（和MySQL中的范围有区别） Constructor and Description Timestamp(int year, int month, int date,int hour,int minute,int second,int nano) Deprecated. instead use the constructor Timestamp(long millis) Timestamp(long time) Constructs a Timestamp object using a milliseconds time value. 参考资料 MySQL官方文档 问题 精度（尤其是在使用MySQL时候作为判断条件时） 线程安全性 时区 使用mongoDB存储SpringBoot日志统一日志框架在系统开发的过程中,会使用到不同的技术,不同的技术会使用不同的日志框架.为了更好地处理日志信息,首先需要将日志框架进行统一. 为了将其他的日志框架装换为slf4j,只需要在pom.xml进行如下配置: 1234567891011121314151617&lt;!--统一日志框架: Slf4j+logback--&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;/dependency&gt; Spring Boot连接mongoDBmongoDB安装和使用 安装—-使用docker 新建数据库 use admin mongoDB的连接mongoDB的连接和其他数据库的连接存在一定的差异,主要是体现在mongoDB为每一个数据库设置了用户和密码,在建立建立连接通常采用一下方式. 12//spring.data.mongodb.uri=mongodb://用户名:密码t@ip:27017/数据库MongoClientURI mongoClientURI=new MongoClientURI(mongoUrl); 将日志信息写入mogoDB重写logback.xml12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;!-- use Spring default values --&gt; &lt;include resource="org/springframework/boot/logging/logback/defaults.xml"/&gt; &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="MONGODB" class="com.mao.api.util.MongoAppender"&gt; &lt;collectionName&gt;logging&lt;/collectionName&gt; &lt;/appender&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="CONSOLE"/&gt; &lt;appender-ref ref="MONGODB"/&gt; &lt;/root&gt;&lt;/configuration&gt; 定义Template12345678910111213141516171819202122232425262728293031323334package com.mao.api.core.config;import com.mongodb.MongoClientURI;import org.springframework.beans.factory.annotation.Value;import org.springframework.data.mongodb.MongoDbFactory;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.core.SimpleMongoDbFactory;import org.springframework.data.mongodb.core.convert.DefaultDbRefResolver;import org.springframework.data.mongodb.core.convert.DefaultMongoTypeMapper;import org.springframework.data.mongodb.core.convert.MappingMongoConverter;import org.springframework.data.mongodb.core.mapping.MongoMappingContext;/** * @Classname MongoConfig * @Description TODO * @Date 19-5-23 下午4:46 * @Created by mao&lt;tianmao818@qq.com&gt; *///@Configurationpublic class MongoConfig &#123; @Value("$&#123;spring.data.mongodb.uri&#125;") private String mongoUrl; public MongoTemplate mongoTemplate() &#123; MongoClientURI mongoClientURI=new MongoClientURI(mongoUrl); MongoDbFactory mongoDbFactory = new SimpleMongoDbFactory(mongoClientURI); DefaultDbRefResolver dbRefResolver = new DefaultDbRefResolver(mongoDbFactory); MappingMongoConverter converter = new MappingMongoConverter(dbRefResolver, new MongoMappingContext()); converter.setTypeMapper(new DefaultMongoTypeMapper(null)); return new MongoTemplate(mongoDbFactory, converter); &#125;&#125; 定义日志Appender(重写append,start,stop,setApplicationContext) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.mao.api.util;/** * @Classname MongoAppender * @Description TODO * @Date 19-5-23 下午4:49 * @Created by mao&lt;tianmao818@qq.com&gt; */import ch.qos.logback.classic.spi.ILoggingEvent;import ch.qos.logback.core.UnsynchronizedAppenderBase;import org.slf4j.LoggerFactory;import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.stereotype.Component;@Componentpublic class MongoAppender extends UnsynchronizedAppenderBase&lt;ILoggingEvent&gt; implements ApplicationContextAware&#123; private static MongoTemplate mongoTemplate; private String collectionName; @Override protected void append(ILoggingEvent event) &#123; if (!started) &#123; return; &#125; //日志存储内容 LogEntity log = new LogEntity(); log.threadName = event.getThreadName(); log.level = event.getLevel().levelStr; log.formattedMessage = event.getFormattedMessage(); log.loggerName = event.getLoggerName(); log.timestamp = event.getTimeStamp(); //使用模板保存日志 mongoTemplate.save(log, collectionName); &#125; @Override public void start() &#123; super.start(); &#125; @Override public void stop() &#123; super.stop(); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; if (applicationContext.getAutowireCapableBeanFactory().getBean(MongoTemplate.class) != null) &#123; mongoTemplate = applicationContext.getAutowireCapableBeanFactory().getBean(MongoTemplate.class); LoggerFactory.getLogger(this.getClass()).info("[ApplicationContext] Autowire MongoTemplate, MongoAppender is ready."); &#125; &#125; private class LogEntity &#123; String threadName; String level; String formattedMessage; String loggerName; Long timestamp; &#125; public String getCollectionName() &#123; return collectionName; &#125; public void setCollectionName(String collectionName) &#123; this.collectionName = collectionName; &#125;&#125; 效果查看collections 查看日志细节 使用递归的方法生成菜单摘抄于：https://www.cnblogs.com/lucky-pin/p/10740037.html 递归生成一个如图的菜单，编写两个类数据模型Menu、和创建树形的MenuTree。通过以下过程实现： 首先从菜单数据中获取所有根节点。 为根节点建立次级子树并拼接上。 递归为子节点建立次级子树并接上，直至为末端节点拼接上空的“树”。 首先，编写数据模型Menu。每条菜单有自己的id、父节点parentId、菜单名称text、菜单还拥有次级菜单children。 123456789101112131415161718import java.util.List;public class Menu &#123; private String id; private String parentId; private String text; private String url; private String yxbz; private List&lt;Menu&gt; children; public Menu(String id,String parentId,String text,String url,String yxbz) &#123; this.id=id; this.parentId=parentId; this.text=text; this.url=url; this.yxbz=yxbz; &#125; /*省略get\set*/ &#125; 创建树形结构的类MenuTree。方法getRootNode获取所有根节点，方法builTree将根节点汇总创建树形结构，buildChilTree为节点建立次级树并拼接上当前树，递归调用buildChilTree不断为当前树开枝散叶直至找不到新的子树。完成递归，获取树形结构。 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.ArrayList;import java.util.List;public class MenuTree &#123; private List&lt;Menu&gt; menuList = new ArrayList&lt;Menu&gt;(); public MenuTree(List&lt;Menu&gt; menuList) &#123; this.menuList=menuList; &#125; //建立树形结构 public List&lt;Menu&gt; builTree()&#123; List&lt;Menu&gt; treeMenus =new ArrayList&lt;Menu&gt;(); for(Menu menuNode : getRootNode()) &#123; menuNode=buildChilTree(menuNode); treeMenus.add(menuNode); &#125; return treeMenus; &#125; //递归，建立子树形结构 private Menu buildChilTree(Menu pNode)&#123; List&lt;Menu&gt; chilMenus =new ArrayList&lt;Menu&gt;(); for(Menu menuNode : menuList) &#123; if(menuNode.getParentId().equals(pNode.getId())) &#123; chilMenus.add(buildChilTree(menuNode)); &#125; &#125; pNode.setChildren(chilMenus); return pNode; &#125; //获取根节点 private List&lt;Menu&gt; getRootNode() &#123; List&lt;Menu&gt; rootMenuLists =new ArrayList&lt;Menu&gt;(); for(Menu menuNode : menuList) &#123; if(menuNode.getParentId().equals("0")) &#123; rootMenuLists.add(menuNode); &#125; &#125; return rootMenuLists; &#125;&#125; 最后，插入一些数据试试效果。得到的json就可以生成图一菜单了。 123456789101112131415161718192021222324import java.util.ArrayList;import java.util.List;import com.alibaba.fastjson.JSON;public class Hello &#123; public static void main(String []args) &#123; List&lt;Menu&gt; menuList= new ArrayList&lt;Menu&gt;(); /*插入一些数据*/ menuList.add(new Menu("GN001D000","0","系统管理","/admin","Y")); menuList.add(new Menu("GN001D100","GN001D000","权限管理","/admin","Y")); menuList.add(new Menu("GN001D110","GN001D100","密码修改","/admin","Y")); menuList.add(new Menu("GN001D120","GN001D100","新加用户","/admin","Y")); menuList.add(new Menu("GN001D200","GN001D000","系统监控","/admin","Y")); menuList.add(new Menu("GN001D210","GN001D200","在线用户","/admin","Y")); menuList.add(new Menu("GN002D000","0","订阅区","/admin","Y")); menuList.add(new Menu("GN003D000","0","未知领域","/admin","Y")); /*让我们创建树*/ MenuTree menuTree =new MenuTree(menuList); menuList=menuTree.builTree(); /*转为json看看效果*/ String jsonOutput= JSON.toJSONString(menuList); System.out.println(jsonOutput); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>代码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ML和DL训练场]]></title>
    <url>%2F2019%2F12%2F08%2FML%E5%92%8CDL%E8%AE%AD%E7%BB%83%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[记录机器学习深度学习算法 算法研究XGBoost概述在这里,我们首先来看一看XGBoost训练完成之后的具体结果.在sklearn实现的xgboost中我们可以将每一棵树的结果绘制出来,具体的代码参考这里 . 不惜篇幅画了这三张图,依次分别是tree_0,tree_1,tree_99.每一棵树的建立都是依赖于前一棵树建立起来的. 对于当前一棵树的建立:主要的过程是在寻找使用哪一个特征以及这一个特征的哪一个取值来进行分割当前节点上所有的样本(注意并非全部样本,样本在父节点上都已经分割过多次的).当前的节点会计算出一个权重值,这个权重$w$将会被用作更新$\hat{y_i}^{(t-1)}$,$\hat{y_i}^{(t-1)}$被用来计算损失loss,通过loss又来计算一阶导数,二阶导数,通过一阶导数以及二阶倒数又来更新新的$w$,最后的叶子节点将会计算出一个权重值保存. 串行执行是如何实现的呢?答案很容易理解,建立完成一棵树以后,在建立下一棵树的时候,是需要利用已经建立好的树来计算$\hat{y_i}^{(t-1)}$的,这个值又被用来计算残差$y_i-\hat{y_i}^{(t-1)}$,残差是loss的一个组成部分,同时是求解导数所必须的(求导很简单,因为算是函数都是那么集中,如均方误差等,导数有解析解,套上数据就行了).我们发现,新的树的建立依赖于已经建立好的树来参与计算. 需要注意,一个特征在分割的时候并不是只使用一次,在xgboost中,每一个节点的分割都是遍历了所有的特征,使用贪心算法来进行分割的时候,还遍历了排好序的所有的取值,不过在近似处理的时候是对排好序的数据进行分桶后处理,算法的细节稍后讨论. warning: math下面来谈论一下xgboost的数学细节,对于论文中出现的公式来进行逐个分析: Equ 1 :串行添加函数,当前的结果是所有函数计算结果的累加.\hat{y_i}=\phi(x_i)=\sum_{k=1}^{K}f_k(x_i),f_k \in F Equ 2 :损失是两部分构成的,一部分是当前的残差,另外的一部分是限制添加的函数的复杂度的.L(\phi)=\sum_{i}l(\hat{y_i},y_i)+\sum_{k}\Omega(f_k)where $\Omega(f)=\gamma T+\frac{1}{2}\lambda | w |^2$ 当前的预测值可以被拆分成为两部分,一部分是前面所有函数的计算结果的累加,另外一部分是当前函数的计算结果. L(\phi)=\sum_{i}l(y_i,\hat{y_i}^{t-1}+f_t(x_i))+\sum_{k}\Omega(f_k)将$L(\phi)$使用泰勒公式展开,得到(就是新加入的函数$f_t(x)$是增量): L^{(t)}\simeq \sum_{i=1}^{n}[l(y_i,\hat{y}^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t)where $gi=\partial{\hat{y}^{t-1}}l(yi,\hat{y}^{t-1})$(一阶导数),$h_i=\partial^2{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$(二阶倒数). Equ 3将常数项省略掉后得到的以下式子:L^{(t)}\simeq \sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t) Equ 4L^{(t)}\simeq \sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)]+\gamma T+\frac{1}{2}\lambda \sum_{j=1}^{T}{\| w \|^2}=\sum_{j=1}^{T}[(\sum_{i \in I_j}g_i)w_j+\frac{1}{2}(\sum_{i \in I_j}h_i+\lambda)w_j^2]+\gamma T Equ 5对于上式进行求导,命导数等于0,得到优化后的$w^*$w^*=-\frac{\sum_{i \in I_j}g_i}{\sum_{i \in I_j}h_i+\lambda} Equ 6将$w^*$带入计算L^{(t)}(q)=-\frac{1}{2}\sum_{j=1}^{T} \frac{(\sum_{i \in I_j}g_i)^2}{\sum_{i\in I_j}h_i+\lambda}+\gamma T Equ 7L_{split}=\frac{1}{2}[\frac{(\sum_{i \in I_R}g_i)^2}{\sum_{i\in I_R}h_i+\lambda}+\frac{(\sum_{i \in I_L}g_i)^2}{\sum_{i\in I_L}h_i+\lambda}-\frac{(\sum_{i \in I}g_i)^2}{\sum_{i\in I}h_i+\lambda}]-\gamma XGBoost相关的参数ensemble之初音女神通过初音案例分析ensemble原理 初音1234import matplotlibimport numpy as npfrom PIL import Imagefrom matplotlib.pyplot import imshow 123456789101112131415miku = pd.read_csv("../data/miku")miku = np.array(miku.values)miku_grayscale = miku[:, 2]miku_grayscale = miku_grayscale.reshape((500, 500))miku_grayscale = miku_grayscale.transpose()image = Image.fromarray(miku_grayscale*255)print(image.size)imshow(image)# image.show()# np.random.shuffle(miku)miku_data = miku[:, 0:2]miku_target = miku[:, 2] 1(500, 500) Decision Tree12345678910from sklearn.tree import DecisionTreeClassifierclf = DecisionTreeClassifier()clf.fit(miku_data, miku_target)predict = clf1.predict(miku_data)predict = predict.reshape((500, 500))predict = predict.transpose()image_pre = Image.fromarray(predict*255)# image_pre.show()imshow(image_pre) Bagging123456789101112131415# baggingfrom sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifier# clf = BaggingClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100)clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100)clf.fit(miku_data, miku_target)predict = clf.predict(miku_data)predict = predict.reshape((500, 500))predict = predict.transpose()image_pre = Image.fromarray(predict*255)# image_pre.show()imshow(image_pre) Boosting123456789101112131415# adaboostfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifierclf1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=10)clf1.fit(miku_data, miku_target)predict1 = clf1.predict(miku_data)predict1 = predict1.reshape((500, 500))predict1 = predict1.transpose()image_pre1 = Image.fromarray(predict1*255)# image_pre.show()imshow(image_pre1) 1234567clf2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100)clf2.fit(miku_data, miku_target)predict2 = clf2.predict(miku_data)predict2 = predict2.reshape((500, 500))predict2 = predict2.transpose()image_pre2 = Image.fromarray(predict2*255)imshow(image_pre2) GBDT12345678910from sklearn.ensemble import GradientBoostingClassifierclf3 = GradientBoostingClassifier(max_depth=10,n_estimators=100)clf3.fit(miku_data, miku_target)predict4 = clf3.predict(miku_data)predict4 = predict4.reshape((500, 500))predict4 = predict4.transpose()image_pre4 = Image.fromarray(predict4*255)# image_pre.show()imshow(image_pre4) XGBoost123456789101112131415161718import xgboost as xgbparam = &#123;'max_depth':5, 'eta':1, 'silent':1, 'objective':'binary:logistic', 'n_estimators':10, &#125;num_rounds = 500 # 迭代次数xgb_train=xgb.DMatrix(miku_data,label=miku_target)xgb_test=xgb.DMatrix(miku_data)model = xgb.train(param, xgb_train, num_rounds)predict3=model.predict(xgb_test)predict3 = predict3.reshape((500, 500))predict3 = predict3.transpose()image_pre3 = Image.fromarray(predict3*255)imshow(image_pre3) 方向研究迁移学习(Transfer Learning)Transfer Learning Definition:Ability of a system to recognize and apply knowledge and skills learned in previous domains/tasks to novel domains/tasks. 相关研究工作 2014年Bengio等人[1]研究深度学习中各个layer特征的可迁移性（或者说通用性） 戴文渊等人提出的TrAdaBoost算法就是典型的基于实例的迁移。 DANN (Domain-Adversarial Neural Network)[5]将近两年流行的对抗网络思想引入到迁移学习中 增量学习(Incremental learning)增量学习作为机器学习的一种方法，现阶段得到广泛的关注。在其中，输入数据不断被用于扩展现有模型的知识，即进一步训练模型，它代表了一种动态的学习的技术。对于满足以下条件的学习方法可以定义为增量学习方法： 可以学习新的信息中的有用信息 不需要访问已经用于训练分类器的原始数据 对已经学习的知识具有记忆功能 在面对新数据中包含的新类别时，可以有效地进行处理 增量学习的分类 序号 类别 备注 1 Sample Incremental Learning 2 Class Incremental Learning 3 Feature Incremental Learning 相关研究工作 《End-to-End Incremental Learning》Although deep learning approaches have stood out in recent years due to their state-of-the-art results, they continue to suffer from catastrophic forgetting, a dramatic decrease in overall performance when training with new classes added incrementally. This is due to current neural network architectures requiring the entire dataset, consisting of all the samples from the old as well as the new classes, to update the model -a requirement that becomes easily unsustainable as the number of classes grows. We address this issue with our approach to learn deep neural networks incrementally, using new data and only a small exemplar set corresponding to samples from the old classes. This is based on a loss composed of a distillation measure to retain the knowledge acquired from the old classes, and a cross-entropy loss to learn the new classes. Our incremental training is achieved while keeping the entire framework end-to-end, i.e., learning the data representation and the classifier jointly, unlike recent methods with no such guarantees. We evaluate our method extensively on the CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and show state-of-the-art performance.[6] 引用1.How transferable are features in deep neural networks（NIPS2014 Bengio et al.） ↩2.Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks（CVPR2014 Oquab.et al.） ↩3.Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach（ICML2011 Glorot. Bengio.et al.） ↩4.Marginalized denoising autoencoders for domain adaptation (ICML2012 Chen et al.) ↩5.Domain-Adversarial Training of Neural Networks（JMLR2016 Ganin.et al.） ↩6.Castro, F. M., Marín-Jiménez, M. J., Guil, N., Schmid, C., &amp; Alahari, K. (2018). End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 233-248). ↩]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL相关]]></title>
    <url>%2F2019%2F12%2F01%2FMySQL%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[MySQL相关知识总结积累MySQL相关原理和操作。 建库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177DROP DATABASE IF EXISTS sampledb;CREATE DATABASE sampledb DEFAULT CHARACTER SET utf8;USE sampledb;/*==============================================================*//* DBMS name: MySQL 5.0 *//* Created on: 2016-10-3 0:34:13 *//*==============================================================*/drop table if exists t_board;drop table if exists t_board_manager;drop table if exists t_post;drop table if exists t_topic;drop table if exists t_user;drop table if exists t_login_log;CREATE TABLE `t_board` ( `board_id` int(11) NOT NULL auto_increment COMMENT '论坛版块ID', `board_name` varchar(150) NOT NULL default '' COMMENT '论坛版块名', `board_desc` varchar(255) default NULL COMMENT '论坛版块描述', `topic_num` int(11) NOT NULL default '0' COMMENT '帖子数目', PRIMARY KEY (`board_id`), KEY `AK_Board_NAME` (`board_name`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8;## Dumping data for table t_board#INSERT INTO `t_board` VALUES (1,'SpringMVC','Spring 框架提供了构建 Web 应用程序的全功能 MVC 模块\r\n',8);INSERT INTO `t_board` VALUES (2,'\r\nSpring Boot','简化新Spring应用的初始搭建以及开发过程，让我们一起来深入这个领域吧',0);INSERT INTO `t_board` VALUES (3,'Spring 事务管理','本板块将讨论 Spring 庞杂而强大的事务功能,包括编程式事务和声明式事务 ',0);INSERT INTO `t_board` VALUES (4,' IOC和AOP ','IOC和AOP讨论板块',3);INSERT INTO `t_board` VALUES (7,'dddddddddddd','ddddddddddddddddddddddddddddddd',0);INSERT INTO `t_board` VALUES (8,'SpringMVC','SpringMVC经验~~',0);## Source for table t_board_manager#CREATE TABLE `t_board_manager` ( `board_id` int(11) NOT NULL, `user_id` int(11) NOT NULL, PRIMARY KEY (`board_id`,`user_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='论坛管理员';## Dumping data for table t_board_manager#INSERT INTO `t_board_manager` VALUES (1,1);INSERT INTO `t_board_manager` VALUES (1,2);INSERT INTO `t_board_manager` VALUES (5,2);INSERT INTO `t_board_manager` VALUES (5,3);## Source for table t_login_log#CREATE TABLE `t_login_log` ( `login_log_id` int(11) NOT NULL auto_increment, `user_id` int(11) default NULL, `ip` varchar(30) NOT NULL default '', `login_datetime` varchar(30) NOT NULL, PRIMARY KEY (`login_log_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;## Dumping data for table t_login_log### Source for table t_post#CREATE TABLE `t_post` ( `post_id` int(11) NOT NULL auto_increment COMMENT '帖子ID', `board_id` int(11) NOT NULL default '0' COMMENT '论坛ID', `topic_id` int(11) NOT NULL default '0' COMMENT '话题ID', `user_id` int(11) NOT NULL default '0' COMMENT '发表者ID', `post_type` tinyint(4) NOT NULL default '2' COMMENT '帖子类型 1:主帖子 2:回复帖子', `post_title` varchar(50) NOT NULL COMMENT '帖子标题', `post_text` text NOT NULL COMMENT '帖子内容', `create_time` date NOT NULL COMMENT '创建时间', PRIMARY KEY (`post_id`), KEY `IDX_POST_TOPIC_ID` (`topic_id`)) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=utf8 COMMENT='帖子';## Dumping data for table t_post#INSERT INTO `t_post` VALUES (1,1,1,1,1,'SpringMVC','Spring Web MVC是一种基于Java的实现了Web MVC设计模式的请求驱动类型的轻量级Web框架\r\n','2016-03-07');INSERT INTO `t_post` VALUES (2,1,2,1,1,'配置\r\详解','谁能告诉我SpringMVC的详细配置呢','2016-03-07');INSERT INTO `t_post` VALUES (3,1,3,1,1,'test3','http://localhost/forum/boardManage.do?method=addTopicPage&amp;boardId=1','2016-03-16');INSERT INTO `t_post` VALUES (4,1,4,1,1,'test5','http://localhost/forum/boardManage.do?method=addTopicPage&amp;boardId=1','2016-03-16');INSERT INTO `t_post` VALUES (5,4,5,1,1,'AOP背后的故事','AOP背后的故事\r\n','2016-04-16');INSERT INTO `t_post` VALUES (6,0,5,1,2,'铁杵磨成针','铁杵磨成针............\r\n','2016-04-16');INSERT INTO `t_post` VALUES (7,4,6,1,1,'IOC的原理一','IOC的原理深入讲解\r\n','2016-04-16');INSERT INTO `t_post` VALUES (8,4,7,1,1,'IOC的原理二','IOC的原理深入讲解','2016-04-16');INSERT INTO `t_post` VALUES (14,5,14,1,1,'d','dddddddddddddd','2016-04-12');INSERT INTO `t_post` VALUES (15,5,15,1,1,'dad','sdfffffffffffffffffff','2016-04-12');INSERT INTO `t_post` VALUES (20,1,20,1,1,'测试。。。。','测试。。。。','2016-04-17');INSERT INTO `t_post` VALUES (21,1,21,1,1,'测试。。。。','测试。。。。','2016-04-17');INSERT INTO `t_post` VALUES (22,1,22,1,1,'SpringMVC集成','SpringMVC集成！！','2016-04-18');INSERT INTO `t_post` VALUES (23,1,23,1,1,'SpringMVC集成','SpringMVC集成！！','2016-04-18');INSERT INTO `t_post` VALUES (24,1,1,1,2,'SpringMVC集成','SpringMVC集成！！','2016-04-18');## Source for table t_topic#CREATE TABLE `t_topic` ( `topic_id` int(11) NOT NULL auto_increment COMMENT '帖子ID', `board_id` int(11) NOT NULL COMMENT '所属论坛', `topic_title` varchar(100) NOT NULL default '' COMMENT '帖子标题', `user_id` int(11) NOT NULL default '0' COMMENT '发表用户', `create_time` date NOT NULL COMMENT '发表时间', `last_post` date NOT NULL COMMENT '最后回复时间', `topic_views` int(11) NOT NULL default '1' COMMENT '浏览数', `topic_replies` int(11) NOT NULL default '0' COMMENT '回复数', `digest` int(11) NOT NULL COMMENT '0:不是精华话题 1:是精华话题', PRIMARY KEY (`topic_id`), KEY `IDX_TOPIC_USER_ID` (`user_id`), KEY `IDX_TOPIC_TITLE` (`topic_title`)) ENGINE=InnoDB AUTO_INCREMENT=24 DEFAULT CHARSET=utf8 COMMENT='话题';## Dumping data for table t_topic#INSERT INTO `t_topic` VALUES (1,1,'test',1,'2016-03-07','2016-04-18',0,1,1);INSERT INTO `t_topic` VALUES (2,1,'test2',1,'2016-03-07','2016-03-07',0,0,1);INSERT INTO `t_topic` VALUES (3,1,'test3',1,'2016-03-16','2016-03-16',0,0,0);INSERT INTO `t_topic` VALUES (4,1,'test5',1,'2016-03-16','2016-03-16',0,0,0);INSERT INTO `t_topic` VALUES (5,4,'AOP背后的故事',1,'2016-03-16','2016-03-16',0,1,1);INSERT INTO `t_topic` VALUES (6,4,'IOC的原理一',1,'2016-03-16','2016-03-16',0,0,0);INSERT INTO `t_topic` VALUES (7,4,'IOC的原理二',1,'2016-03-16','2016-03-16',0,0,0);INSERT INTO `t_topic` VALUES (14,5,'d',1,'2016-04-12','2016-04-12',0,0,0);INSERT INTO `t_topic` VALUES (15,5,'dad',1,'2016-04-12','2016-04-12',0,0,0);INSERT INTO `t_topic` VALUES (20,1,'测试。。。。',1,'2016-04-17','2016-04-17',0,0,0);INSERT INTO `t_topic` VALUES (21,1,'测试。。。。',1,'2016-04-17','2016-04-17',0,0,0);INSERT INTO `t_topic` VALUES (22,1,'SpringMVC集成',1,'2016-04-18','2016-04-18',0,0,0);INSERT INTO `t_topic` VALUES (23,1,'SpringMVC集成',1,'2016-04-18','2016-04-18',0,0,0);## Source for table t_user#CREATE TABLE `t_user` ( `user_id` int(11) NOT NULL auto_increment COMMENT '用户Id', `user_name` varchar(30) NOT NULL COMMENT '用户名', `password` varchar(30) NOT NULL default '' COMMENT '密码', `user_type` tinyint(4) NOT NULL default '1' COMMENT '1:普通用户 2:管理员', `locked` tinyint(4) NOT NULL default '0' COMMENT '0:未锁定 1:锁定', `credit` int(11) default NULL COMMENT '积分', `last_visit` datetime default NULL COMMENT '最后登陆时间', `last_ip` varchar(20) default NULL COMMENT '最后登陆IP', PRIMARY KEY (`user_id`), KEY `AK_AK_USER_USER_NAME` (`user_name`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;## Dumping data for table t_user#INSERT INTO `t_user` VALUES (1,'tom','1234',2,0,203,NULL,NULL);INSERT INTO `t_user` VALUES (2,'john','1234',2,1,10,NULL,NULL);INSERT INTO `t_user` VALUES (3,'ggg','123123',1,0,110,NULL,NULL); MySQL按行导入text123mysql --local-infile=1 -u root -p use peoples_daily;load data local infile "/home/mao/Downloads/source_BIO_2014_cropus.csv" into table news_table(news_text);]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据相关]]></title>
    <url>%2F2019%2F11%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[重点参考： https://github.com/CheckChe0803/BigData-Interview Hadoop Hive Spark Flink HBase Kafka Zookeeper 文档 文档 文档 文档 文档 文档 文档 HadoopHDFS架构？[2]HDFS-1 架构图 HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode. NameNode NameNode 负责管理整个分布式系统的元数据，主要包括： 目录树结构； 文件到数据库 Block 的映射关系； Block 副本及其存储位置等管理数据； DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。 这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。 fsimage：是内存命名空间元数据在外存的镜像文件； editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。 这两个文件相结合可以构造完整的内存数据。 **Secondary NameNode** Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。 **DataNode** 负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。 HDFS-2 架构图和上面一样 HDFS High Availability 属性 （1）Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； （2）ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； （3）Zookeeper 集群：为主备切换控制器提供主备选举支持； （4）共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 （5）DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 HDFS比较（主要体现在jdk版本，HA等） 比较 特性 HDFS 1 HDFS 2 HADOOP，HDFS，YARN，MAPREDUCE HDFS 3 Move to JDK8+ HDFS相关的例子？ 常用命令 123查看文件系统的基本信息和统计信息：hdfs dfsadmin -report建立文件夹: hadoop fs -mkdir /user/tpc-h1G上传文件:hadoop fs -put *.tbl /user/tpc-h1G Yarn架构？（资源管理） The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). An application is either a single job or a DAG of jobs. 原理[1][3] Yarn架构图 ResourceManager（RM）RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成： 调度器 Scheduler 调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器 Applications Manager，ASM。 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。 NodeManager（NM）NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。 ApplicationMaster（AM）提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）； 将得到的任务进一步分配给内部的任务； 与 NM 通信以启动/停止任务； 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。 MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 ContainerContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 MapReduce过程？MapReduce分为两个阶段: Map 和 Ruduce. Map阶段: input. 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务 map 就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行 Partition. 需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition. spill 此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件. sort 在写入环形缓冲区前,对数据排序.格式排序 combine(可选). 在溢出文件之前,提前开始combine,相当于本地化的reduce操作 merge spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件. Reduce阶段: copy. 拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作. merge 合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件. 数据经过 内存-&gt;磁盘 , 磁盘-&gt;磁盘的过程. output merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束 reduce 开始执行reduce任务,最后结果保留在hdfs上. 案例[4]1.下表是一个不同年份的用电量，找出平均用电量最大的年份 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Avg 1979 23 23 2 43 24 25 26 26 26 26 25 26 25 1980 26 27 28 28 28 30 31 31 31 30 30 30 29 1981 31 32 32 32 33 34 35 36 36 34 34 34 34 1984 39 38 39 39 39 41 42 43 40 39 38 38 40 1985 38 39 39 39 39 41 41 41 00 40 39 39 45 分别实现Mapper和Reducer接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package hadoop; import java.util.*; import java.io.IOException; import java.io.IOException; import org.apache.hadoop.fs.Path; import org.apache.hadoop.conf.*; import org.apache.hadoop.io.*; import org.apache.hadoop.mapred.*; import org.apache.hadoop.util.*; public class ProcessUnits &#123; //Mapper class public static class E_EMapper extends MapReduceBase implements Mapper&lt;LongWritable ,/*Input key Type */ Text, /*Input value Type*/ Text, /*Output key Type*/ IntWritable&gt; /*Output value Type*/ &#123; //Map function public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; String line = value.toString(); String lasttoken = null; StringTokenizer s = new StringTokenizer(line,"\t"); String year = s.nextToken(); while(s.hasMoreTokens()) &#123; lasttoken = s.nextToken(); &#125; int avgprice = Integer.parseInt(lasttoken); output.collect(new Text(year), new IntWritable(avgprice)); &#125; &#125; //Reducer class public static class E_EReduce extends MapReduceBase implements Reducer&lt; Text, IntWritable, Text, IntWritable &gt; &#123; //Reduce function public void reduce( Text key, Iterator &lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; int maxavg = 30; int val = Integer.MIN_VALUE; while (values.hasNext()) &#123; if((val = values.next().get())&gt;maxavg) &#123; output.collect(key, new IntWritable(val)); &#125; &#125; &#125; &#125; //Main function public static void main(String args[])throws Exception &#123; JobConf conf = new JobConf(ProcessUnits.class); //任务：获取最大用电量 conf.setJobName("max_eletricityunits"); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(E_EMapper.class); conf.setCombinerClass(E_EReduce.class); conf.setReducerClass(E_EReduce.class); conf.setInputFormat(TextInputFormat.class); conf.setOutputFormat(TextOutputFormat.class); FileInputFormat.setInputPaths(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); JobClient.runJob(conf); &#125; &#125; 不建立单机环境，仅导包完成调试功能：运行和调试MapReduce程序只需要有相应的Hadoop依赖包就行，可以完全当成一个普通的JAVA程序。 2.排序：order by 3.去重：distinct 4.多表查询 5.倒排索引 (ps:spark经典案例[8]) Yarn 调度MapReduce？Yarn采用的双层调度框架，RM将资源分配给AM,AM再将资源进一步分配给Task,资源不够时会为TASK预留，直到资源充足。 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; hdfs写流程？ Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象； 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息； 通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block； DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点； DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功； 完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入； 调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。 hdfs读流程？ Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息； NameNode 返回存储的每个块的 DataNode 列表； Client 将连接到列表中最近的 DataNode； Client 开始从 DataNode 并行读取数据； 一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。 hdfs创建一个文件的流程？(类的调用过程) Apache Hadoop HDFS 2.9.1 API[5] 客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。 FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。 进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。 最后，RpcServer将RPC响应返回给客户端。 hadoop1.x 和hadoop 2.x 的区别？ 资源调度方式的改变 在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理. HA模式 在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode. HDFS Federation Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性 hadoop1.x： hadoop2.x： hadoop HA介绍？ HDFS High Availability[6] Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； Zookeeper 集群：为主备切换控制器提供主备选举支持； 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 hadoop的常用配置文件？在TPC-H的测评实验中，使用配置文件见github.com/maomao1994/TPC-H[7] hadoop-env.sh: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等； core-site.xml: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置； hdfs-site.xml: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置； mapred-site.xml：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置； yarn-site.xml ~/hadoop/etc/hadoop/slaves 小文件过多会有什么危害,如何避免？Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存. 每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间. 显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat实现小文件的合并 启动hadoop集群会分别启动哪些进程,各自的作用？ NameNode： 维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件 记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息 负责响应客户端的 数据块位置请求 。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取 接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息 SecondaryNameNode(非HA模式)： 实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(注意 并不是NameNode的备用节点) DataNode： 根据需要存取并检索数据块 定期向namenode发送其存储的数据块列表 ResourceManager： 负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。 NodeManager： 运行ResourceManager分配的任务，同时将任务进度向application master报告 JournalNode(HA下启用): 高可用情况下存放namenode的editlog文件 HIVE介绍The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. HIVE架构下面是hive的架构图[9] Unit Name Operation User Interface Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server). Meta Store Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping. HiveQL Process Engine HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it. Execution Engine The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce. HDFS or HBASE Hadoop distributed file system or HBASE are the data storage techniques to store data into file system. **下面hive和hadoop的交互图[9]** Step No. Operation 1 Execute QueryThe Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute. 2 Get PlanThe driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query. 3 Get MetadataThe compiler sends metadata request to Metastore (any database). 4 Send MetadataMetastore sends metadata as a response to the compiler. 5 Send PlanThe compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete. 6 Execute PlanThe driver sends the execute plan to the execution engine. 7 Execute JobInternally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job. 7.1 Metadata OpsMeanwhile in execution, the execution engine can execute metadata operations with Metastore. 8 Fetch ResultThe execution engine receives the results from Data nodes. 9 Send ResultsThe execution engine sends those resultant values to the driver. 10 Send ResultsThe driver sends the results to Hive Interfaces. hive的数据类型**All the data types in Hive are classified into four types, given as follows:[9]** Column Types Literals Null Values Complex Types hive的数据类型细分如下： 类型 细分 备注 Column Types Integral Types TINYINT，SMALLINT，INT，BIGINT String Types VARCHAR，CHAR Timestamp It supports traditional UNIX timestamp with optional nanosecond precision. It supports java.sql.Timestamp format “YYYY-MM-DD HH:MM:SS.fffffffff” and format “yyyy-mm-dd hh:mm:ss.ffffffffff”. Dates NaN. Decimals as same as Big Decimal format of JavaDECIMAL(precision, scale) Union Types UNIONTYPE\]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令]]></title>
    <url>%2F2019%2F11%2F24%2FLinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux命令收集linux相关命令,包含文件,网络,性能,编辑等. 文件相关建立软链接，快速启动软件（不用修改环境变量）1ss@ss:/usr/bin$ sudo ln -s /opt/pycharm/bin/pycharm.sh pycharm 查看文件行数123wc -l filename #就是查看文件里有多少行,wc -l *.csv ==&gt;列出所有csv文件行数wc -w filename #看文件里有多少个word。wc -L filename #文件里最长的那一行是多少个字 查看文件的前/后 n 行12head -n "文件名"tail -n "文件名" 用dd生成指定大小的文件12#生成5GB数据dd if=/dev/zero of=tmp bs=1G count=5 查看当前文件夹下文件的总个数1ls -l | grep "^-" | wc -l 重命名文件12# 将文件名中包含A的文件名全部替换为Brename &quot;s/A/B/&quot; * lsof1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 1.列出所有打开的文件:lsof备注: 如果不加任何参数，就会打开所有被打开的文件，建议加上一下参数来具体定位# 2. 查看谁正在使用某个文件lsof /filepath/file# 3.递归查看某个目录的文件信息lsof +D /filepath/filepath2/备注: 使用了+D，对应目录下的所有子目录和文件都会被列出# 4. 比使用+D选项，遍历查看某个目录的所有文件信息 的方法lsof | grep ‘/filepath/filepath2/’# 5. 列出某个用户打开的文件信息lsof -u username备注: -u 选项，u其实是user的缩写# 6. 列出某个程序所打开的文件信息lsof -c mysql备注: -c 选项将会列出所有以mysql开头的程序的文件，其实你也可以写成lsof | grep mysql,但是第一种方法明显比第二种方法要少打几个字符了# 7. 列出多个程序多打开的文件信息lsof -c mysql -c apache# 8. 列出某个用户以及某个程序所打开的文件信息lsof -u test -c mysql# 9. 列出除了某个用户外的被打开的文件信息lsof -u ^root备注：^这个符号在用户名之前，将会把是root用户打开的进程不让显示# 10. 通过某个进程号显示该进行打开的文件lsof -p 1# 11. 列出多个进程号对应的文件信息lsof -p 123,456,789# 12. 列出除了某个进程号，其他进程号所打开的文件信息lsof -p ^1# 13 . 列出所有的网络连接lsof -i# 14. 列出所有tcp 网络连接信息lsof -i tcp# 15. 列出所有udp网络连接信息lsof -i udp# 16. 列出谁在使用某个端口lsof -i :3306# 17. 列出谁在使用某个特定的udp端口lsof -i udp:55特定的tcp端口lsof -i tcp:80# 18. 列出某个用户的所有活跃的网络端口lsof -a -u test -i# 19. 列出所有网络文件系统lsof -N# 20.域名socket文件lsof -u# 21.某个用户组所打开的文件信息lsof -g 5555# 22. 根据文件描述列出对应的文件信息lsof -d description(like 2)# 23. 根据文件描述范围列出文件信息lsof -d 2-3 批量解压（路径中含有空格）12345678#/usr/bin env# 解压路径下有空格，将空格首先替换为“问好”，注意shell等号前后没有空格files=$(find ./files/ -type f | tr " " "?")for i in $&#123;files&#125;doecho "$i"unzip "$i"done 以上，为何问好就可以不用替换回空格呢？原因在于，在linux中问好号作为单个通配符使用是，可以匹配任何一个字符，也就是说不论是什么符号都可以匹配，所以不用替换回去！！！ 检索（递归）所有文件 网络相关连接远程服务器12ssh tm@172.16.18.24# 之后会提醒输入密码 远程上传文件，下载文件命令1234# 下载scp -r username@192.168.0.1:/home/username/remotefile.txt# 上传scp -r localfile.txt username@192.168.0.1:/home/username/ 开通ssh服务12345678910# 查看是否开启了ssh服务是否安装,使用命令：sudo ps -e |grep ssh# 先更新资源列表，使用命令：sudo apt-get update# 安装openssh-server，使用命令：sudo apt-get install openssh-server# 启动ssh命令service sshd start# 停止ssh命令service sshd stop ssh免密1234# 本地执行:ssh-keygen -t rsa# ssh-copy-id -i ~/.ssh/id_rsa.pub tm@172.16.18.39 tcpdump抓包指定网卡，指定端口，指定host,写到test.pcap 1tcpdump -i ens7f0 port 10080 and host 192.168.126.3 -w test.pcap Ubuntu 镜像使用帮助1234567891011121314# Ubuntu 的软件源配置文件是 /etc/apt/sources.list。将系统自带的该文件做个备份，将该文件替换为下面内容，即可使用 TUNA 的软件源镜像。# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 广播消息,给特定用户发消息12345# 特定用户whowrite syf pts/10# 广播wall "message" 性能相关后台运行相关123456fg、bg、jobs、&amp;、ctrl+z1. &amp; 最经常被用到这个用在一个命令的最后，可以把这个命令放到后台执行2. ctrl + z 可以将一个正在前台执行的命令放到后台，并且暂停3. jobs查看当前有多少在后台运行的命令4. fg将后台中的命令调至前台继续运行 如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) 杀掉所有的python进程（如：使用了多进程）1ps -ef | grep python | grep -v grep | awk '&#123;print $2&#125;' | xargs kill -9 编辑相关源码查看：主要使用vim快速查看函数的原型定义123456#（1）真对于系统函数，偶尔可以使用shift+K进行定位#（2）主要的方法是使用ctags工具来生成tags文件，方法如下sudo apt-get install ctags # 安装ctags软件ctags -R # 生成tags文件:set tags=绝对路径（tags文件）# 跳转方法：ctrl+]跳转到光标所在单词的tag，ctrl+T：跳回到原来的位置，有多个tag的时候使用g]键进行跳转。 替换命令123:s/foo/bar/g Change each 'foo' to 'bar' in the current line.:%s/foo/bar/g Change each 'foo' to 'bar' in all the lines.:5,12s/foo/bar/g Change each 'foo' to 'bar' for all lines from line 5 to line 12 (inclusive). sed 命令格式 sed [-nefri] ‘command’ 文件名 选项 -n -e（多条命令顺序执行，命令使用分号切割） -f -r -i（写入文件） 命令 a（append新增） c（行替换） d（delete删除） i（insert前面插入） p（print打印） s（字符串的替换） 举例 cat user.txt 12345ID Name Sex Age1 zhang M 192 wang G 203 cheng M 104 huahua M 100 在user.txt文件中；匹配带h的行 并且只显示1,3行 1234cat user.txt | grep h |sed -n &apos;1,3p&apos;1 zhang M 193 cheng M 104 huahua M 100 删除最后一行记录 1cat user.txt | grep h | sed &apos;$d&apos; 在user.txt中显示带h的行；并且从结果中删掉2,3行的记录；只看第一行记录 12cat user.txt | grep h |sed &apos;2,3d&apos;1 zhang M 19 在user.txt中查询出带h的行；并在第二行后面添加新的一行数据 1cat user.txt | grep h |sed &apos;2a5\thuang\tG\t40&apos; 在第二行插入2行数据的签名插入新增的数据： 1cat user.txt | grep h |sed &apos;2i hello\nword&apos; 把第二行数据；用命令c替换成 10 wanghua N 90 1cat user.txt | grep h |sed &apos;2c 10\twanghua\tN\t90&apos; 字符串的替换：s 123456789cat user.txt | grep h1 zhang M 193 cheng M 104 huahua M 100cat user.txt | grep h |sed &apos;2s/ch/wh/g&apos;1 zhang M 193 wheng M 104 huahua M 100 把第3行的数据里的wang 替换成heee 并写入到user.txt 1sed -i &apos;3s/wang/heee/g&apos; user.txt sed -e ‘s/zhang//g ; s/wang//g’ user.txt # -e允许多条命令顺序执行，用分号隔开，s前面不加数字表示所有行 奇数行，偶数行 123sed -n &apos;p;n&apos; a.txt 输出奇数行，n表示读入下一行文本（隔行）nextsed -n &apos;n;p&apos; a.txt 输出偶数行，n表示读入下一行文本（隔行）sed -n &apos;$=&apos; a.txt 输出文件的行数， wc -l返回行数及文件名 awk 功能 在当前路径下，递归遍历所有的文件，每个文件使用逗号分割，找出每一行第一列值为10的所有文件的记录的行号和文件名。 12345678#/usr/bin env# 通过find递归，得到所有的文件的完整路径files=$(find ./ -type f)for i in $filesdo # awk的-F选项指定分割符号，-v是指定的变量，可以在print中打印，'$1=="10"是指第一列中等于10的，print NR表示的是指示的行号，uniq指的是过滤掉重复的，&gt;&gt;out指的是追加到out文件 awk -F "," -v mao=$PWD '$1=="10"&#123;print NR,FILENAME&#125;' $i | uniq &gt;&gt;outdone 展示 文件以及文件内容(cat -n可以显示行号) 输出结果 awk简单命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java总结-3]]></title>
    <url>%2F2019%2F11%2F22%2FJava%E6%80%BB%E7%BB%93-3%2F</url>
    <content type="text"><![CDATA[Java总结-3 六、Spring1. IOC和AOP？首先声明：IoC &amp; AOP 不是 Spring 提出来的，它们在 Spring 之前其实已经存在了，只不过当时更加偏向于理论。Spring 在技术层次将这两个思想进行了很好的实现。 1.1 IOC？IoC （Inversion of control ）控制反转/反转控制。它是一种思想不是一个技术实现。描述的是：Java 开发领域对象的创建以及管理的问题。 例如：现有类 A 依赖于类 B 传统的开发方式 ：往往是在类 A 中手动通过 new 关键字来 new 一个 B 的对象出来 使用 IoC 思想的开发方式 ：不通过 new 关键字来创建对象，而是通过 IoC 容器(Spring 框架) 来帮助我们实例化对象。我们需要哪个对象，直接从 IoC 容器里面过去即可。 从以上两种开发方式的对比来看：我们 “丧失了一个权力” (创建、管理对象的权力)，从而也得到了一个好处（不用再考虑对象的创建、管理等一系列的事情） 为什么叫控制反转? 控制：指的是对象创建（实例化、管理）的权力，反转：控制权交给外部环境（Spring 框架、IoC 容器） 解决了什么问题？ IoC 的思想就是两方之间不互相依赖，由第三方容器来管理相关资源。这样有什么好处呢？首先对象之间的耦合度或者说依赖程度降低；然后资源变的容易管理；比如你用 Spring 容器提供的话很容易就可以实现一个单例。 1.2 AOP 什么是AOP？ AOP：Aspect oriented programming 面向切面编程，AOP 是 OOP（面向对象编程）的一种延续。切 ：指的是横切逻辑，原有业务逻辑代码不动，只能操作横切逻辑代码，所以面向横切逻辑；面 ：横切逻辑代码往往要影响的是很多个方法，每个方法如同一个点，多个点构成一个面。 AOP解决了什么问题？ 通过上面的分析可以发现，AOP 主要用来解决：在不改变原有业务逻辑的情况下，增强横切逻辑代码，根本上解耦合，避免横切逻辑代码重复。 1.3 AOP实现：JDK动态代理，CGLIB两种方式：一种是JDK动态代理，另一种是CGLib的方式。 1.3.1 JDK动态代理具体实现原理 通过实现InvocationHandlet接口创建自己的调用处理器； 通过为Proxy类指定ClassLoader对象和一组interface来创建动态代理； 通过反射机制获取动态代理类的构造函数，其唯一参数类型就是调用处理器接口类型； 通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数参入； JDK动态代理是面向接口的代理模式，如果被代理目标没有接口那么Spring也无能为力，Spring通过Java的反射机制生产被代理接口的新的匿名实现类，重写了其中AOP的增强方法。 1.3.2 CGLib动态代理CGLib是一个强大、高性能的Code生产类库，可以实现运行期动态扩展java类，Spring在运行期间通过 CGlib继承要被动态代理的类，重写父类的方法，实现AOP面向切面编程呢。 1.3.3 两者对比JDK动态代理是面向接口的。 CGLib动态代理是通过字节码底层继承要代理类来实现（如果被代理类被final关键字所修饰，那么抱歉会失败）。 1.3.4 性能关于两者之间的性能的话，JDK动态代理所创建的代理对象，在以前的JDK版本中，性能并不是很高，虽然在高版本中JDK动态代理对象的性能得到了很大的提升，但是他也并不是适用于所有的场景。 主要体现在如下的两个指标中： CGLib所创建的动态代理对象在实际运行时候的性能要比JDK动态代理高不少，有研究表明，大概要高10倍； 但是CGLib在创建对象的时候所花费的时间却比JDK动态代理要多很多，有研究表明，大概有8倍的差距； 因此，对于singleton的代理对象或者具有实例池的代理，因为无需频繁的创建代理对象，所以比较适合采用CGLib动态代理，反正，则比较适用JDK动态代理。 2.Spring事务管理？3.Spring启动的过程？4.Spring Bean的生命周期？graph TB st(( ))--通过getBean调用某一个Bean-->a(*调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation方法) a-->b(实例化) b-->c(*调用InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation方法) c-->d(*调用instantiationAwarebeanPostProcessor的postProcessPropertyValues方法) d-->e(设置属性值) e-->f(调用BeanNameAware的setBeanName方法) f-->g(调用BeanFactoryAware的setBeanFactory方法) g-->h(*调用BeanPostProcessor的postProcessBeforeInitialization方法) h-->i(调用InitializingBean的afterPropertiesSet方法) i-->j(通过init-method属性配置的初始化方法) j-->k(*调用BeanPostProcessor的postProcessAfterInitialization方法) k-->l(Spring缓存池中准备就绪的Bean) k-->m(将准备就绪的Bean交给调用者) l-->n(调用DisposableBean的destroy方法) n-->p(通过destroy-method属性配置的销毁方法) p-->pp(( )) ps：ApplicationContext中的Bean的生命周期 graph TB st(( ))--启动容器-->aa(调用BeanFactoryPostProcessor的postProcessBeanFactory方法对工厂定义信息进行处理) aa--通过getBean调用某一个Bean-->a(*调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation方法) a-->b(实例化) b-->c(*调用InstantiationAwareBeanPostProcessor的postprocessAfterInstantiation方法) c-->d(*调用instantiationAwarebeanPostProcessor的postProcessPropertyValues方法) d-->e(设置属性值) e-->f(调用BeanNameAware的setBeanName方法) f-->g(调用BeanFactoryAware的setBeanFactory方法) g-->gg(调用ApplicationContextAware的setApplicationnContext方法) gg-->h(*调用BeanPostProcessor的postProcessBeforeInitialization方法) h-->i(调用InitializingBean的afterPropertiesSet方法) i-->j(通过init-method属性配置的初始化方法) j-->k(*调用BeanPostProcessor的postProcessAfterInitialization方法) k-->l(Spring缓存池中准备就绪的Bean) k-->m(将准备就绪的Bean交给调用者) l-->n(调用DisposableBean的destroy方法) n-->p(通过destroy-method属性配置的销毁方法) p-->pp(结束) 5.Spring AOP和Spring Bean的生命周期有什么关系？ AOP本身是可以对一个pojo进行操作，比如希望在一个类中的方法执行前和执行后添加日志信息。 在Spring中，Spring AOP、IOC和AspectJ被整合在一起了。 Spring AOP使用了JDK动态代理和CGLIB动态代理。（PS：JDK动态代理只支持接口代理，不支持类代理） JDK动态代理主要涉及的类：Proxy和InvocationHandler CGLIB主要涉及到的接口：MethodInterpreter，Enhance，MethodProxy 对于增强，CGLIB手动建立的，AOP定义了5种增强类型：前置增强、后置增强、环绕增强、异常抛出增强、引介增强。 如何管理植入切面的增强？ 直接使用ProxyBean 通过Spring配置来定义：ProxyFactoryBean Spring提供了自动代理机制，让容器自动生成代理，避免上述繁琐的配置方式，在内部，Spring使用BeanPostProcessor自动完成这个工作。 6.接收参数的几种方法？7.@Bean和@Component的区别？（1）@Component注解表明一个类会作为组件类，并告知Spring要为这个类创建bean。 （2）@Bean注解告诉Spring这个方法将会返回一个对象，这个对象要注册为Spring应用上下文中的bean。通常方法体中包含了最终产生bean实例的逻辑。 两者的目的是一样的，都是注册bean到Spring容器中。 区别： @Component（@Controller、@Service、@Repository）通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中。 而@Bean注解通常是我们在标有该注解的方法中定义产生这个bean的逻辑。 @Component 作用于类，@Bean作用于方法。 总结： @Component和@Bean都是用来注册Bean并装配到Spring容器中，但是Bean比Component的自定义性更强。可以实现一些Component实现不了的自定义加载类。 8. Spring中的设计模式https://juejin.cn/post/6844903849849962509 （1）单例模式1234567891011121314151617181920212223242526272829/** Cache of singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);/** * Return the (raw) singleton object registered under the given name. * &lt;p&gt;Checks already instantiated singletons and also allows for an early * reference to a currently created singleton (resolving a circular reference). * @param beanName the name of the bean to look for * @param allowEarlyReference whether early references should be created or not * @return the registered singleton object, or &#123;@code null&#125; if none found */@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; （2）工厂模式Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 （3）观察者模式 Spring 的事件流程总结: ApplicationEvent，ApplicationListener，ApplicationEventPublisher。 graph TB st(( ))-->a(1. 定义一个事件, 实现一个继承自ApplicationEvent, 并且写相应的构造函数) a-->b(2. 定义一个事件监听者: 实现ApplicationListener接口, 重写onApplicationEvent方法) b-->c(3. 使用事件发布者发布消息: 可以通过ApplicationEventPublisher的publishEvent方法发布消息) 12345678910111213141516171819202122232425262728293031323334353637383940// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数public class DemoEvent extends ApplicationEvent&#123; private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message)&#123; super(source); this.message = message; &#125; public String getMessage() &#123; return message; &#125; // 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；@Componentpublic class DemoListener implements ApplicationListener&lt;DemoEvent&gt;&#123; //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) &#123; String msg = event.getMessage(); System.out.println("接收到的信息是："+msg); &#125;&#125;// 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。@Componentpublic class DemoPublisher &#123; @Autowired ApplicationContext applicationContext; public void publish(String message)&#123; //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); &#125;&#125; （4）代理模式（5）模板方法（6）适配器模式（7）装饰者模式9. Spring 的启动过程是怎样的？（1）SpringApplication.java注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Class that can be used to bootstrap and launch a Spring application from a Java main * method. By default class will perform the following steps to bootstrap your * application: * * &lt;ul&gt; * &lt;li&gt;Create an appropriate &#123;@link ApplicationContext&#125; instance (depending on your * classpath)&lt;/li&gt; * &lt;li&gt;Register a &#123;@link CommandLinePropertySource&#125; to expose command line arguments as * Spring properties&lt;/li&gt; * &lt;li&gt;Refresh the application context, loading all singleton beans&lt;/li&gt; * &lt;li&gt;Trigger any &#123;@link CommandLineRunner&#125; beans&lt;/li&gt; * &lt;/ul&gt; * * In most circumstances the static &#123;@link #run(Class, String[])&#125; method can be called * directly from your &#123;@literal main&#125; method to bootstrap your application: * * &lt;pre class="code"&gt; * &amp;#064;Configuration * &amp;#064;EnableAutoConfiguration * public class MyApplication &#123; * * // ... Bean definitions * * public static void main(String[] args) &#123; * SpringApplication.run(MyApplication.class, args); * &#125; * &#125; * &lt;/pre&gt; * * &lt;p&gt; * For more advanced configuration a &#123;@link SpringApplication&#125; instance can be created and * customized before being run: * * &lt;pre class="code"&gt; * public static void main(String[] args) &#123; * SpringApplication application = new SpringApplication(MyApplication.class); * // ... customize application settings here * application.run(args) * &#125; * &lt;/pre&gt; * * &#123;@link SpringApplication&#125;s can read beans from a variety of different sources. It is * generally recommended that a single &#123;@code @Configuration&#125; class is used to bootstrap * your application, however, you may also set &#123;@link #getSources() sources&#125; from: * &lt;ul&gt; * &lt;li&gt;The fully qualified class name to be loaded by * &#123;@link AnnotatedBeanDefinitionReader&#125;&lt;/li&gt; * &lt;li&gt;The location of an XML resource to be loaded by &#123;@link XmlBeanDefinitionReader&#125;, or * a groovy script to be loaded by &#123;@link GroovyBeanDefinitionReader&#125;&lt;/li&gt; * &lt;li&gt;The name of a package to be scanned by &#123;@link ClassPathBeanDefinitionScanner&#125;&lt;/li&gt; * &lt;/ul&gt; * * Configuration properties are also bound to the &#123;@link SpringApplication&#125;. This makes it * possible to set &#123;@link SpringApplication&#125; properties dynamically, like additional * sources ("spring.main.sources" - a CSV list) the flag to indicate a web environment * ("spring.main.web-application-type=none") or the flag to switch off the banner * ("spring.main.banner-mode=off"). * * @author Phillip Webb * @author Dave Syer * @author Andy Wilkinson * @author Christian Dupuis * @author Stephane Nicoll * @author Jeremy Rickard * @author Craig Burke * @author Michael Simons * @author Madhura Bhave * @author Brian Clozel * @author Ethan Rubinson * @since 1.0.0 * @see #run(Class, String[]) * @see #run(Class[], String[]) * @see #SpringApplication(Class...) */ 10.Spring的自动装配？11.ApplicationContext通常的实现是什么?12.AOP实现流程？（1）基于接口的增强以下是通过接口实现增强的使用，用起来很笨重。后面来讲基于@AspectJ的方式就简单很多！ graph TB A1(获取spring提供的代理工厂)-.->B1(设置目标代理) B1-.->C1(为代理目标设置增强) C1-.->D1(生成代理实例) 举一个例子，如何自行定义一个权限控制的注解？注解+AOP实现 先定义一个注解，比如叫@AuthCheck 在接口上添加@AuthCheck注解 对一个类添加@AspectJ注解，在这个类中通过@PointCut来扫描@AuthCheck，就是切点；然后定义一些增强，比如使用@Before搞一个前置增强，这个增强对切点进行处理，可以将切点的参数一并携带，进行一些逻辑处理！ 13.Springboot自动配置的流程？（1）refresh()以下refresh()定义了Spring容器在加载配置文件后的各项处理过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // （1）初始化Bean Factory ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // （2）调用工厂后处理器 invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. // （3）初始化Bean后处理器 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. // （4）初始化消息源 initMessageSource(); // Initialize event multicaster for this context. // （5）初始化应用上下文事件广播器 initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. // （6）初始化其他的Bean onRefresh(); // Check for listener beans and register them. // （7）注册事件监听器 registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. // （8）初始化所有的单实例Bean，使用懒加载模式的除外 finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. // （9）完成刷新并发布容器刷新事件 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; （2）spring从配置文件创建完整bean的流程graph TB st((配置文件))--ResourceLoader装载配置文件-->a(Resource) a--BeanDefinitoinReader解析配置信息-->b(BeanDefinitionRegistry加工前的BeanDefinition) b-->c(BeanDefinitionRegistry加工后的BeanDefinition) b-->d(PropertyEditorRegistry存放的自定义的PropertyEditor) c--InstantiationStrategy实例化Bean对象-->e(Bean实例未设置属性) d--BeanWrapper设置Bean属性-->f(Bean实例已设置属性) e--BeanWrapper设置Bean属性-->f f--BeanPostprocessror对Bean进行加工-->g(Bean实例准备完成) 1234567(1) ResourceLoader从存储介质加载Spring配置信息，并使用Resource表示这个配置文件资源。(2) BeanDefinitionReader读取Resource所指向的配置文件资源，然后解析配置文件。配置文件中的每一个&lt;bean&gt;解析成一个BeanDefinition对象，并且保存到BeanDefinitionRegistry中。(3) 容器扫描BeanDefinitonRegistry中的BeanDefinition，使用Java反射机制自动识别出Bean工厂后处理器（实现了BeanFactoryPostProcessor接口的Bean），然后调用这些Bean工厂后处理器对BeanDefinitionRegistry中的BeanDefinition进行加工处理。(4) Spring容器从BeanDefinitionRegistry中取出加工后的BeanDefinition，并且调用InstantiationStrategy着手进行Bean实例化工作。(5) 在实例化Bean时，Spring容器使用BeanWrapper对Bean进行封装。BeanWrapper提供了很多以Java反射机制操作bean的方法，它将结合该Bean的BeanDefinition以及容器中的属性编辑器，完成Bean的属性注入工作。(6) 利用容器中注册的Bean后处理器（实现BeanPostProcessor接口的Bean）对已经完成属性设置工作的Bean进行后续加工，直接装配出一个准备就绪的Bean。总结：先对BeanDefinition操作，然后对Bean操作。 （3）Spring boot自动配置举例：在引入MyBatis的starter后，MyBatis相关的Bean自动装配进入容器，如何仅仅通过starter来实现？ 1）SpringBoot启动的时候加载主配置类，开启了自动配置功能@EnableAutoConfiguration 2）@EnableAutoConfiguration 作用：利用EnableAutoConfigurationImportSelector给容器中导入一些组件可以查看selectImports()方法的内容； List configurations = getCandidateConfigurations(annotationMetadata, attributes); 获取候选的配置 12345&gt; SpringFactoriesLoader.loadFactoryNames()&gt; 扫描所有jar包类路径下 META‐INF/spring.factories&gt; 把扫描到的这些文件的内容包装成properties对象&gt; 从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中&gt; 将 类路径下 META-INF/spring.factories 里面配置的所有EnableAutoConfiguration的值加入到了容器中；每一个这样的 xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置； 3）、每一个自动配置类进行自动配置功能； 14.BeanFactory和FactoryBean的区别？ BeanFactory是工厂，我们通过getBean获取Bean。 FactoryBean是一个工厂类接口，可以用来定制Bean的逻辑，Spring提供了很多FactoryBean的实现类，隐藏了实例化一些复杂Bean的细节。 15.@Resource和@Autowired的作用的相同的么？ 这是一个sb的问题，没啥子实际区别！ 16.BeanFactory和ApplicationContext的区别？ 生命周期上有区别 对Bean的初始化有区别 17.在 Spring中如何注入一个java集合？18.MyBatis 相关（1）简单谈谈你对 Mybatis 的理解？（2）MyBatis 接口绑定的优点是什么？（3）实现 MyBatis 接口绑定分别有哪几种方式?（4）MyBatis 如何实现一对一关联关系？（5）MyBatis 如何实现一对多关联关系？（6）说说 MyBatis 动态 SQL 的具体使用步骤?（7）MyBatis 与 Hibernate 的区别是什么？（8）MyBatis 如何实现模糊查询?19.Nginx 反向代理实现高并发的具体步骤是什么？20.Nginx 搭建 Tomcat 集群的核心配置应该怎么写？21.bean之间的关系？（继承，依赖）22.配置外部数据源？context:property-placeholder，使用${var}23.SpEL的使用（1）SpEL的作用？24.bean的配置方式？（1）xml文件（2）注解（可以使用value修改名字）25.Spring事务管理（1）编程式事务管理（注释）（2）声明式事务管理（XML）26.Spring Boot事务管理27.Spring事务的传播行为28.Spring的事务隔离级别29. 注解30. Spring 问题？30.1 Spring 循环依赖？循环依赖：一个或多个对象实例之间存在直接或间接的依赖关系，这种依赖关系构成了构成一个环形调用。 循环依赖总结一下（假设A,B之间循环依赖）：一级缓存singletonObject，也就是常说的单例池，是个Map二级缓存earlySingletonObjects，也就是提前一点的单例池，哈哈，字面翻译,也是Map三级缓存singletonFactories，这个Map有点特殊，因为这个Map的value存放的是一个lambda表达式1、单例池不能存放原始对象，只能存放经过完整生命周期的对象，也就是java bean2、A，B在注入属性都会执行一个addSingletonFactory方法，这个方法里面三级缓存就出现了，三级缓存put了key为beanName，value为一个lambda表达式3、其实最容易绕晕的地方是，当B注入属性A的时候，执行populateBean注入一个bean的属性的时候会执行getSingleton这个方法，一定要记得！！populateBean方法体中没有直接调用getSingleton这个方法，但一定要记得，执行了这个方法4、getSingleton这个方法，会依次到一级缓存，二级缓存，三级缓存中get(beanName)，很显然当B注入A属性的时候，一级，二级里面都没有内容，只有三级有，这时会执行lambda表达式，lambda表达式的作用就是生成代理对象！！然后把生成的代理对象存入二级缓存，并返回这个代理对象，B就会得到这个代理对象A，B就会认为这个代理对象A就是A的最后的bean对象，因此也就完成了对A的属性注入这步操作，接着依次执行B后续的操作，最后就完成了B的生命周期，B就成功变成了bean对象，B也就完成了使命5、当B完成使命之后，A就会继续注入B，这时就会注入属性成功了，接下来开始执行AOP操作，因为上一步中A已经生成了代理对象A，也就是相当于完成了AOP，所以B就不执行AOP操作了，此时A就会执行最后一步操作，将代理对象A放入到单例池中去，这时A就会执行方法getSingleton，从二级缓存中获得了代理对象A，最后将其存入单例池，也就是一级缓存！好了，现在A和B都放入了单例池，圆满结束！！！！ 30.2 Spring为什么要有三级缓存？三级缓存的实现在代码中的 SingletonBeanRegistry 中,其中有以下几个核心属性： singletonObjects：一级缓存， 用于存放完全初始化好的 bean，从该缓存中取出的 bean 可以直接使用。 earlySingletonObjects：二级缓存，用于存放提前曝光的单例对象的cache，原始的 bean 对象（尚未填充属性）。 singletonFactories：三级缓存，用于存放 bean 工厂对象（ObjectFactory）。三级缓存中用到了 ObjectFactory，这里的 ObjectFactory 可以理解为三级缓存中 Bean 的代理对象，其 getObject() 方法描述了如何获取这个三级缓存的对象。设计成这样除了方便实现三级缓存解决循环依赖，另外也是方便 Spring 在 ObjectFactory 中做一些拓展。 singletonsCurrentlyInCreation：用于存放正在被创建的 Bean 对象。 二级和三级只有在产生循环依赖的时候用到。 31. Spring单例实现使用ConcurrentHashMap存放Bean 123/** Cache of singleton objects: bean name to bean instance. */ private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);synchronized (this.singletonObjects) 七、Spring Boot1.Spring boot重要的构成部分2.Spring Boot启动过程https://www.e-learn.cn/content/qita/2004793 graph TB A1(加载resources/META-INF/spring.factories中配置的ApplicationContextInitializer ApplicationListener)-.->B1(加载 resources/META-INF/spring.factories 中配置的 SpringApplicationRunListener) B1-.->C1(创建 StandardServletEnvironment 并完成初始配置) C1-.->D1(通过 EventPublishingRunListener发布ApplicationEnvironmentPreparedEvent事件) D1-.->E1(加载resources/META-INF/spring.factories中配置的EnvironmentPostProcessor,并触发其 postProcessEnvironment调用,完成Environment的后处理,主要用于加载属性源到 Environment中) E1-.->F1(将Environment绑定到SpringApplication 中) F1-.->G1(创建ApplicationContext:AnnotationConfigServletWebServerApplicationContext) G1-.->H1(加载 resources/META-INF/spring.factories 中配置的 SpringBootExceptionReporter'FailureAnalyzers') H1-.->I1(执行所有已加载的 ApplicationContextInitializer,附加自定义配置) I1-.->J1(通过EventPublishingRunListener发布 ApplicationContextInitializedEvent事件) J1-.->K1(创建 BeanDefinitionLoader, 将应用程序主类解析为 BeanDefinition 并注册到 DefaultListableBeanFactory中) K1-.->L1(通过 EventPublishingRunListener 发布 ApplicationPreparedEvent 事件) L1-.->M1(prepareRefresh:清除类路径扫描器缓存写入closed, active 标识,验证所有必须的属性是否都能解析) M1-.->N1(obtainFreshBeanFactory:写入 refreshed 标识, 获取 DefaultListableBeanFactory) N1-.->O1(prepareBeanFactory:写入 BeanPostProcessor写入可以忽略的依赖注入接口, 注册部分内部可解析的依赖接口等) O1-.->P1(postProcessBeanFactory:注册 WebApplicationScope, 注册部分内部可解析的依赖接口) P1-.->Q1(invokeBeanFactoryPostProcessors:执行SharedMetadataReaderFactoryContextInitializer$CachingMetadataReaderFactoryPostProcessor注册用于获取MetadataReader的org.springframework.boot.autoconfigure.internalCachingMetadataReaderFactory的BeanDefinition) Q1-.->R1(registerBeanPostProcessors:按照 PriorityOrdered, Ordered, 普通的顺序将 BeanPostProcessor添加到 BeanFactory 中) R1-.->S1(initMessageSource:注册 messageSource 单例'DelegatingMessageSource') S1-.->T1(注册applicationEventMulticaster单例'SimpleApplicationEventMulticaster', 用于广播 ApplicationEvent) T1-.->U1(onRefresh:设置主题, 创建内嵌服务器'WebServer', 获取 ServletContextInitializer DispatcherServletRegistrationBean初始化器并注册DispatcherServlet到 javax.servlet.ServletContext和org.apache.catalina.core.ApplicationContext中.注册 OrderedCharacterEncodingFilter,OrderedHiddenHttpMethodFilter, OrderedFormContentFilter, OrderedRequestContextFilter等过滤器, 也可注册自定义过滤器. 写入 ServletContextPropertySource, ServletConfigPropertySource到Environment中) U1-.->V1(registerListeners:注册内置的 ApplicationListener, 自定义的 ApplicationListener 到 ApplicationEventMulticaster中) V1-.->W1(finishBeanFactoryInitialization:冻结 DefaultListableBeanFactory 中注册的BeanDefinition 信息, 并实例化所有非延迟初始化的单例 bean.在所有 eager 单例初始化完成之后, 如果其实现了 SmartInitializingSingleton 接口, 则触发其 afterSingletonsInstantiated 调用完成后处理) W1-.->X1(注册 DefaultLifecycleProcessor,查找所有实现 SmartLifecycle 接口的 bean && 如果它是 isAutoStartup,则触发其start方法调用) X1-.->Y1(通过 EventPublishingRunListener 发布 ContextRefreshedEvent 事件) Y1-.->Z1(启动 WebServer) Z1-.->A2(通过 EventPublishingRunListener 发布 ServletWebServerInitializedEvent 事件) A2-.->B2(清理各种缓存) B2-.->C2(注册 Application 的 ShutdownHook) C2-.->D2(触发 SpringApplicationRunListener 的 started 通知) D2-.->E2(通过 EventPublishingRunListener 发布 ApplicationStartedEvent 事件) E2-.->F2(按照 Order 对 ApplicationRunner,CommandLineRunner 进行排序,并顺序执行) F2-.->G2(触发 SpringApplicationRunListener 的 running 通知) G2-.->H2(通过 EventPublishingRunListener 发布 ApplicationReadyEvent 事件) H2-.->I2(应用程序启动完成) 3. 自定义spring-boot-starter 写一个@Enable*的注解，@Import相关的@Configuration 写具体的实现 写@Configuration，创建Bean 八、RocketMq1. 事务机制（1）half机制2. Dledger(外部机制实现高可用)（1）Raft协议 Leader follower candidate（竞选状态） 3. 私信队列 一直没有消费———&gt;Retry队列———-&gt;死信队列————-&gt;加入另外的消费者消费 4. 怎么避免消息丢失？5. 消息堆积处理？6. 顺序消费？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[光影心得]]></title>
    <url>%2F2019%2F11%2F20%2F%E5%85%89%E5%BD%B1%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[足记简介：每拍一回照片，选一张最喜欢的分享，记录技巧和体会。 2019年11月09日（中国科学院大学玉泉路校区高能所） 照片参数 序号 总结 1 糊片儿了 2 曝光有一点不足 2019年11月26日（中国科学院大学玉泉路校区南门天桥） 照片参数 序号 总结 1 拍摄位置选的不理想，红绿灯太多，车流不稳定。 2 拍摄时间不太理想，不是车流量高峰。 2020年02月17日(湖北省恩施州巴东县) 己亥末，庚子春，楚大疫。居家很久，遇到几场大雪，难能平静的时刻。 2020年5月21日（湖北省巴东县野三关镇乾丰广场） ​ 晚上出来瞎逛，这家建始大饼还没有打烊，丈夫在做饼，妻子在一旁注视着。小店在楼梯下，很狭窄，门口挂了一只风筝。 2020年5月27日（湖北省巴东县野三关镇乾丰广场） ​ 沉寂了好久的街道开始有人活动起来，有人还戴着口罩，有人已经选择不戴。路人行色匆匆，道路在重新施工。 2020年5月28日（湖北省巴东县野三关镇喝二两老街） 枇杷和樱桃成熟的季节。 2020年8月1日（湖南省长沙市岳麓山顶） ​ 在长沙第一次见到平平，下午去爬岳麓山，我们边爬边没有边际地聊天。平平是一个精力充沛的女孩，在炎热的长沙爬了一下午竟然没有说累。在山顶我们看到了长沙地夜景，我给平平拍了下面这张剪影。Here just you and me。 技术单反相机解剖图 英文名 中文名 备注 DSLR（Digital Single Lens Reflex） 单反相机 Aperture 光圈 Lens 镜头 Shutter 快门 Exchangeable Image File（Exif） exif信息 HSL（Hue，Saturation，Lightness） 色相、饱和度、亮度 以下是相机的不同组成部分（材料信息，参数信息）： 感光 快门反光板五棱镜光圈对比手法 对比手法 实例 明暗对比 动静对比 虚实对比 冷暖对比 主次对比 大小对比 多少对比 高低对比 远近对比 曲直对比 疏迷对比 刚柔对比 构图方法（摘自宁思潇潇《摄影笔记》） 方法 实例 居中构图 三分构图 重复法 引导线 三角构图 历史人物 序号 人物 备注 代表作 1 滨田英明 2 宁思潇潇 3 Tim Walker 4 BILL GEKAS https://www.billgekas.com/ 5 W.EUGENE SMITH 尤金·史密斯 6 PALPH NIBSON 7 ANDREAS FEININGER 安德烈亚斯·费宁格 http://www.artnet.com/artists/andreas-feininger/ 8 MICHAEL KEN 9 PAUL STRAND 保罗·斯特兰德 10 HIROSHI SUGIMOTO 杉本博司 11 ANSEL ADAMS 安塞尔·伊士顿·亚当斯 12 EDWARD WESTON 爱德华·韦斯顿 13 14 网站 序号 网址 备注 1 https://www.format.com/magazine/galleries format网址，包含一些gallery和magzine 2 http://www.artnet.com/ 工具 序号 名称 使用 备注 1 sudo apt-get install exiftool exiftool 20191126.jpeg &gt; 20191126.jpeg.txt 查看exif信息 2 Snapseed 学习《摄影笔记》——-宁思潇潇 以下是摘录 摄影技术诞生于1839年8月19日 用摄影来表达 摄影和绘画、文字、音乐一样，都是一个载体，将创作者的思想或者意图传递给读者（观众、听众） 如果你的内心被一张照片触动到，那么这张照片一定做到了三件事。1.达到并发现某个场景。2.熟练操作相机。3.用自己的摄影技术将这个场景尽可能地进行表达。 摄影真正难的是：“达到并发现某一个场景” 好照片第一标准———一个好照片必须有一个鲜明的主题 好照片的第二个标准———照片必须将注意力引向摄影主题 好照片的第三个标准———尽量简洁 善于利用相机的一切功能，在具体的场景中，使用最便捷的拍摄模式，用更多的精力去观察取景，而不是调节相机。 永远不要忘记，取景，曝光，虚实，构图，就是摄影的一切。而这些都是为了明确主题，突出主体，简化画面二服务的 摄影内容远远大于摄影技术 摄影技术：取景、曝光、虚实、构图 李涛老师摄影后期经典课程学习方法 序号 方法 备注 1 取法乎上，寻师经典 2 务先大体，鉴比穷源 3 诚心正意，自证良知 发现—-联系—-重组 天下大事，必作于细 众生平等，皆有联系 情理之中，意料之外 观察力、想象力、创作力《摄影构图学》后期步骤 序号 步骤 备注 1 校准（镜头，畸变） 消除色差，启动镜头配置 2 准确的色彩还原（白平衡） 3 正确的黑白场 4 足够的清晰度 5 适当的饱和度 6 丰富合理的层次 7 局部调节 画笔、镜像、渐变 互补色 要有目的地做后期影调 高短 高中 高长 中短 中中 中长 低短 低中 低长]]></content>
      <categories>
        <category>摄影</category>
      </categories>
      <tags>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行业前瞻]]></title>
    <url>%2F2019%2F11%2F20%2F%E8%A1%8C%E4%B8%9A%E5%89%8D%E7%9E%BB%2F</url>
    <content type="text"><![CDATA[对于信息渠道的收集。 会议/期刊会议(CCF-A) 序号 方向 名称 备注 1 计算机体系结构/并行与分布计算/存储系统 PPoPP：ACM SIGPLAN Symposium on Principles &amp; Practice of Parallel ProgrammingFAST：Conference on File and Storage TechnologiesDAC：Design Automation ConferenceHPCA：High Performance Computer ArchitectureMICRO：IEEE/ACM International Symposium on MicroarchitectureSC：International Conference for High Performance Computing, Networking, Storage, and AnalysisASPLOS： International Conference on Architectural Support for Programming Languages and Operating SystemsISCA： International Symposium on Computer ArchitectureUSENIX： ATC USENIX Annul Technical Conference 2 计算机网络 SIGCOMM：ACM International Conference on Applications, Technologies, Architectures, and Protocols for Computer CommunicationMobiCom：ACM International Conference on Mobile Computing and NetworkingINFOCOM： IEEE International Conference on Computer CommunicationsNSDI：Symposium on Network System Design and Implementation 3 网络与信息安全 CCS： ACM Conference on Computer and Communications SecurityEUROCRYPT： European Cryptology ConferenceS&amp;P： IEEE Symposium on Security and PrivacyCRYPTO： International Cryptology ConferenceUSENIX： Security Usenix Security Symposium 4 软件工程/系统软件/程序设计语言 PLDI： ACM SIGPLAN Conference on Programming Language Design &amp; ImplementationPOPL： ACM SIGPLAN-SIGACT Symposium on Principles of Programming LanguagesFSE/ESEC： ACM SIGSOFT Symposium on the Foundation of Software Engineering/ European Software Engineering ConferenceSOSP： ACM Symposium on Operating Systems PrinciplesOOPSLA： Conference on Object-Oriented Programming Systems, Languages, and ApplicationsASE： International Conference on Automated Software EngineeringICSE： International Conference on Software EngineeringISSTA： International Symposium on Software Testing and AnalysisOSDI： USENIX Symposium on Operating Systems Design and Implementations 5 数据库／数据挖掘／内容检索 SIGMOD： ACM Conference on Management of DataSIGKDD： ACM Knowledge Discovery and Data MiningICDE： IEEE International Conference on Data EngineeringSIGIR： International Conference on Research on Development in Information RetrievalVLDB： International Conference on Very Large Data Bases 6 计算机科学理论 STOC： ACM Symposium on the Theory of ComputingSODA： ACM-SIAM Symposium on Discrete AlgorithmsCAV： Computer Aided VerificationFOCS： IEEE Annual Symposium on Foundations of Computer ScienceLICS： IEEE Symposium on Logic in Computer Science 7 计算机图形学与多媒体 ACM MM： ACM International Conference on MultimediaSIGGRAPH: ACM SIGGRAPH Annual ConferenceVR: IEEE Virtual RealityIEEE VIS: IEEE Visualization Conference 8 人工智能 AAAI: AAAI Conference on Artificial IntelligenceNeurIPS: Annual Conference on Neural Information Processing SystemsACL: Annual Meeting of the Association for Computational LinguisticCVPR: IEEE Conference on Computer Vision and Pattern RecognitionICCV: International Conference on Computer VisionICML: International Conference on Machine LearningIJCAI： International Joint Conference on Artificial Intelligence 9 人机交互与普适计算 CSCW： ACM Conference on Computer Supported Cooperative Work and Social ComputingCHI： ACM Conference on Human Factors in Computing SystemsUbiComp： ACM International Conference on Ubiquitous Computing 10 交叉/综合/新兴 WWW： International World Wide Web ConferencesRTSS： Real-Time Systems Symposium 期刊(CCF-A) 序号 刊物简称 刊物全称 出版社 网址 - - 计算机体系结构 并行与分布计算 存储系统 - - 1 TOCS ACM Transactions on Computer Systems ACM http://dblp.uni-trier.de/db/journals/tocs/ 2 TOS ACM Transactions on Storage ACM http://dblp.uni-trier.de/db/journals/tos/ 3 TCAD IEEE Transactions on Computer-Aided Design of Integrated Circuits And System IEEE http://dblp.uni-trier.de/db/journals/tcad/ 4 TC IEEE Transactions on Computers IEEE http://dblp.uni-trier.de/db/journals/tc/index.html 5 TPDS IEEE Transactions on Parallel and Distributed Systems IEEE http://dblp.uni-trier.de/db/journals/tpds/ - - 计算机网络 - - 1 JSAC IEEE Journal of Selected Areas in Communications IEEE http://dblp.uni-trier.de/db/journals/jsac/ 2 TMC IEEE Transactions on Mobile Computing IEEE http://dblp.uni-trier.de/db/journals/tmc/ 3 TON IEEE/ACM Transactions on Networking IEEE/ACM http://dblp.uni-trier.de/db/journals/ton/ - - 网络与信息安全 - - 1 TDSC IEEE Transactions on Dependable and Secure Computing IEEE http://dblp.uni-trier.de/db/journals/tdsc/ 2 TIFS IEEE Transactions on Information Forensics and Security IEEE http://dblp.uni-trier.de/db/journals/tifs/ 3 Journal of Cryptology Springer http://dblp.uni-trier.de/db/journals/joc/ - - 软件工程 系统软件 程序设计语言 - - 1 TOPLAS ACM Transactions on Programming Languages &amp; Systems ACM http://dblp.uni-trier.de/db/journals/toplas/ 2 TOSEM ACM Transactions on Software Engineering and Methodology ACM http://dblp.uni-trier.de/db/journals/tosem/ 3 TSE IEEE Transactions on Software Engineering IEEE http://dblp.uni-trier.de/db/journals/tse/ - - 数据库 数据挖掘 内容检索 - - 1 TODS ACM Transactions on Database Systems ACM http://dblp.uni-trier.de/db/journals/tods/ 2 TOIS ACM Transactions on Information Systems ACM http://dblp.uni-trier.de/db/journals/tois/ 3 TKDE IEEE Transactions on Knowledge and Data Engineering IEEE http://dblp.uni-trier.de/db/journals/tkde/ 4 VLDBJ The VLDB Journal Springer http://dblp.uni-trier.de/db/journals/vldb/ - - 计算机科学理论 - - 1 TIT IEEE Transactions on Information Theory IEEE http://dblp.uni-trier.de/db/journals/tit/ 2 IANDC Information and Computation Elsevier http://dblp.uni-trier.de/db/journals/iandc/ 3 SICOMP SIAM Journal on Computing SIAM http://dblp.uni-trier.de/db/journals/siamcomp/ - - 计算机图形学与多媒体 - - 1 TOG ACM Transactions on Graphics ACM http://dblp.uni-trier.de/db/journals/tog/ 2 TIP IEEE Transactions on Image Processing IEEE http://dblp.uni-trier.de/db/journals/tip/ 3 TVCG IEEE Transactions on Visualization and Computer Graphics IEEE http://dblp.uni-trier.de/db/journals/tvcg/ - - 人工智能 - - 1 AI Artificial Intelligence Elsevier http://dblp.uni-trier.de/db/journals/ai/ 2 TPAMI IEEE Trans on Pattern Analysis and Machine Intelligence IEEE http://dblp.uni-trier.de/db/journals/pami/ 3 IJCV International Journal of Computer Vision Springer http://dblp.uni-trier.de/db/journals/ijcv/ 4 JMLR Journal of Machine Learning Research MIT Press http://dblp.uni-trier.de/db/journals/jmlr/ - - 人机交互与普适计算 - - 1 TOCHI ACM Transactions on Computer-Human Interaction ACM http://dblp.uni-trier.de/db/journals/tochi/ 2 IJHCS International Journal of Human Computer Studies Elsevier http://dblp.uni-trier.de/db/journals/ijmms/ - - 交叉 综合 新兴 - - 1 JACM Journal of the ACM ACM http://dblp.uni-trier.de/db/journals/jacm/ 2 Proc. IEEE Proceedings of the IEEE IEEE http://dblp.uni-trier.de/db/journals/pieee/ 行业报告 https://resources.distilnetworks.com/analyst-reports]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>报告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java总结-1]]></title>
    <url>%2F2019%2F11%2F20%2FJava%E6%80%BB%E7%BB%93-1%2F</url>
    <content type="text"><![CDATA[Java总结-1 一、计算机网络1.OSI、TCP/IP、五层体系结构联系与区别？（1）三种比较？ （2）七层结构细节？ （3）每一层对应的设备？ 2.说一说TCP/IP协议簇？ 应用层（它是计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，完成用户希望在网络上完成的各种工作。） SNMP SMTP 传输层（向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。传输层的作用是向高层屏蔽下层数据通信的细节，即向用户透明地传送报文。） 网络层（通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。） ICMP IGMP RIP BGP OSPF IP 链路层（通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。） ARP，RARP 3.TCP三次握手的过程？ 握手过程可以由客户端调用socket开启，客户端发送SYN和Seq，closed状态变换为SYN_SEND状态，服务器端由LISTEN状态变换为SYN_RECV状态，服务端回送SYN+ACK，客户端接收，客户端状态变为Established，客户端发送ACK，服务端接收到ACK，状态变为Established，至此，TCP三次握手的过程就完成了。 （1）第一次握手：服务端确定（服务端可以接收数据，客户端可以发送数据） （2）第二次握手：客户端确定（服务端可以发送数据，服务端可以接收数据） （3）第三次握手：服务端确定（客户端可以接收数据） 以上，（1）（2）（3）是的双方确定彼此可以接收和发送数据。 4.TCP四次挥手的过程？ 关闭连接的过程可以由服务端和客户端的任何一方发起，发起的一方状态变化为： Established———&gt;FIN_WAIT_1———&gt;FIN_WAIT_2———&gt;TIME_WAIT———&gt;CLOSED; 被动关闭的一方的状态变化为： Establised———&gt;CLOSE_WAIT———&gt;LAST_ACK———&gt;CLOSED. 5.TCP在三次握手的过程中是如何超时重传的？ (1) 如果第一个包，A发送给B请求建立连接的报文(SYN)如果丢掉了，A会周期性的超时重传，直到B发出确认(SYN+ACK)；(2) 如果第二个包，B发送给A的确认报文(SYN+ACK)如果丢掉了，B会周期性的超时重传，直到A发出确认(ACK)；(3) 如果第三个包，A发送给B的确认报文(ACK)如果丢掉了， - A在发送完确认报文之后，单方面会进入ESTABLISHED的状态，B还是SYN_RCVD状态 - 如果此时双方都没有数据需要发送，B会周期性的超时发送(SYN+ACK)，直到收到A的确认报文(ACK)，此时B也进入ESTABLISHED状态，双方可以发送数据； - 如果A有数据发送，A发送的是(ACK+DATA)，B会在收到这个数据包的时候自动切换到ESTABLISHED状态，并接受数据(DATA)； - 如果这个时候B要发送数据，B是发送不了数据的，会周期性的超时重传(SYN+ACK)直到收到A的确认(ACK)B才能发送数据。 6.为什么要三次握手，四次挥手？（1）为什么要进行三次握手？ 三次握手的目的是为了建立可靠的通信信道，简单地来说就是双方确认自己与对方的发送和接收是正常的。 这是防止已失效的连接请求报文段突然又传送到了B而引发错误。 举例，客户端A向服务端B发送数据，受到网络状态的影响，可能A发送的数据B很久以后才收到（实际上A已经通过重传机制重新发送了），当这个阻塞的数据到来的时候，B就会误以为这是一个新的连接，则B将等待A，但是实际上A并没有发起新的请求，这就导致了资源的浪费。 （2）为什么要进行四次挥手？ TCP通信是一个双工通信，在结束连接的时候FIN和ACK是分开发送的，A向B发送FIN仅仅表示A不在发送数据，并不表示自己不在接收数据，同理，B向A发送FIN仅仅表示B不在发送数据，但是自己是可以接收数据的。 为什么要在发起端加上TIME_WAIT？是为了保证ACK丢失的时候可以重传。客户端发送第四次挥手中的报文后，再经过2MSL，可使本次TCP连接中的所有报文全部消失，不会出现在下一个TCP连接中。考虑丢包问题，如果第四挥手发送的报文在传输过程中丢失了，那么服务端没收到确认ack报文就会重发第三次挥手的报文。如果客户端发送完第四次挥手的确认报文后直接关闭，而这次报文又恰好丢失，则会造成服务端无法正常关闭。 7.在浏览器地址栏输入一个url到浏览器返回页面的过程？ 浏览器分析超链指向页面的 URL。 浏览器向 DNS 请求解析 www.tsinghua.edu.cn 的 IP 地址。 域名系统 DNS 解析出清华大学服务器的 IP 地址。 浏览器与服务器建立 TCP 连接 浏览器发出取文件命令：GET /chn/yxsz/index.htm。(HTTP) 服务器给出响应，把文件 index.htm 发给浏览器。 TCP 连接释放。 浏览器显示“清华大学院系设置”文件 index.htm 中的所有文本。 8.说一说在三次握手的时候可能存在的安全问题？当第二次握手后，服务端将会进入SYN_RECV状态（又叫做半连接状态），通过伪造客户端的地址，这个时候服务器端一直在等待客户端返回ACK，但是由于地址是伪造的，所以根本就无法收到ACK。当这种伪造的连接数量大的时候就会导致DDOS。 9.域名解析m.xyz.com需要查找y.abc.com的IP地址： 主机m.xyz.com向本地域名服务器进行递归查询。 主机向本地域名服务器查询时一般使用递归查询。 递归查询：就是如果本地域名服务器没有所需域名的IP地址，本地域名服务器就以客户的方式向其他根域名服务器继续查询，而不是主机自己进行查询。返回给客户的是解析好的ip。（查查查，一直查到了再返回） 本地域名服务器向其他根域名服务器进行查询的时一般使用迭代查询。 迭代查询： 当某个根域名服务器收到本地域名服务器的请求报文时，要么告诉它所需域名的IP地址，要么告诉它下一步应该向哪个服务器发起询问。然后让本地域名服务器自己去查询。（查不到，你去别的地方查询吧） 本地域名服务器迭代查询，先向一个根域名服务器查询。 根域名服务器告诉本地域名服务器，下一步应该向顶级域名服务器dns.com查询。 顶级域名服务器dns.com告诉本地域名服务器，下一步查找权限域名服务器：dns.adc.com。 本地域名服务器向权限域名服务器发起查询。权限域名服务器告诉本地服务器所需的IP地址，本地服务器在告诉给本地主机。 根：美国（10），日本（1），英国（1），瑞士（1） 顶级域名：com，org，edu，gov等 二级域名： 子域： 总共有四层，最大深度127层 DNS资源记录： SOA，每一个区在开始处都包含一个授权记录 NS资源记录，域名服务器记录 A资源记录， PTR资源记录， CNAME资源记录，别名记录 10.TCP是如何保证可靠传输的？(分编校丢流拥重超) （1）应用数据被TCP分割成为适合发送的数据块 （2）TCP将会给每一个包进行编号，接收方会对数据进行排序，将有序的数据传输给应用层。 序列号：TCP传输时将每个字节的数据都进行了编号，这就是序列号。 确认应答：TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文。这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。这也是TCP传输可靠性的保证之一。 （3）TCP将会保持首部和数据的校验和，目的是检查数据在传输的过程中是否被修改 （4）丢弃重复发送的数据 （5）流量控制：TCP连接的每一方都有一个固定的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能够容纳的数据，当接收方来不及处理的时候，能够提示发送端降低发送的速率，防止丢包。（TCP使用的是滑动窗口进行流量控制)，如果发送端发送的数据太快，接收端来不及接收就会出现丢包问题。为了解决这个问题，TCP协议利用了滑动窗口进行了流量控制。在TCP首部有一个16位字段大小的窗口，窗口的大小就是接收端接收数据缓冲区的剩余大小。接收端会在收到数据包后发送ACK报文时，将自己的窗口大小填入ACK中，发送方会根据ACK报文中的窗口大小进而控制发送速度。如果窗口大小为零，发送方会停止发送数据。 （6）拥塞控制（当网络阻塞的时候，减少数据的发送，拥塞控制就是防止过多的数据注入到网络中，这样使网络中的路由器或者链路不至于过载。）这里的发送方会维护一个拥塞窗口的状态变量，它和流量控制的滑动窗口是不一样的，滑动窗口是根据接收方数据缓冲区大小确定的，而拥塞窗口是根据网络的拥塞情况动态确定的，一般来说发送方真实的发送窗口为滑动窗口和拥塞窗口中的最小值。 （7）自动重传（为了实现可靠的传输，每发送完一个分组就会停止发送，等待对方确认，确认后再发送下一个分组。） （8）超时重传（当TCP发出一个分组后，它将启动一个定时器，等待目的端确认接收，如果不及时，将会重传。） 11.TCP和UDP之间的区别？(面头流速可有界) 区别 TCP UDP 面向连接 面向连接，TCP不提供广播和多播服务 面向无连接，UDP支持一对一、多对一、一对多、多对多的交互通信。 头部大小 头部至少为20个字节 头部为8个字节 流量控制 有流量控制 没有流量控制 速度 TCP速度较慢 UDP速度较快 可靠性 可靠传输 不可靠传输 有序 有序 无序 界 TCP有界，通过字节流传输 UDP无界，每一个包是单独传输的，发送方的UDP对应用程序交下来的报文添加首部后直接交付给IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。 适用场景 视频 文件传输 12.post和get的区别？ 区别 POST GET 可见性 数据在url中不可见 参数在url中可见 长度 没有长度限制 有长度限制 编码 application/x-www-form-urlencoded, multipart/form-data application/x-www-form-urlencoded 缓存 不支持 支持 安全性 相对安全 相对不安全 13.在TCP和UDP之上都有哪些应用层的协议？ TCP：HTTP，HTTPS，SMTP（简单邮件传输协议），POP3，SSH UDP：DNS，Telnet，SNMP（简单网络管理协议），IGMP（网络组管理协议）,RIP(路由信息协议)，DHCP（动态主机设置协议） 14.HTTPS握手的过程？ （1）客户端给出一个协议版本号、一个客户端生成的随机数（Client random）以及客户端支持的加密算法。（客户端发送了三件东西） （2）服务端确认双方使用的加密算法，并且给出数字证书，以及一个随机数（server random）。（服务端发送了两件东西） （服务端将自己的公钥发给数字证书认证机构，数字证书认证机构利用自己的私钥对服务器的公钥进行数字签名，并给服务器颁发公钥证书。） （3）客户端确认数字证书有效，然后生成一个新的随机数 ，并且使用数字证书中的公钥，加密这个随机数，将其发送给服务端。（客户端发送了一个非对称加密的随机数） （4）服务端使用自己的私钥，获取来自客户端的加密随机数（Premaster secret）。（服务端使用非对称加密算法进行解密） （5）客户端和服务端根据约定的加密方法，使用前面的三个随机数，生成对话密钥（session key），用来加密整个会话。（服务端使用对称密钥会话） 对称加密和非对称加密？ 》 DES、3DES（TripleDES）、AES、RC2、RC4、RC5和Blowfish等 》RSA、ECC（移动设备用）、Diffie-Hellman、El Gamal、DSA（数字签名用） 15.TCP头部，UDP头部比较？（1）TCP头部至少由20个字节构成（最长60个），如下图： （2）UDP头部由8个字节构成，如下图： 16.IP头部 17.HTTP请求，HTTP响应，字段？ （1）HTTP请求 请求行 方法，url，协议版本 请求首部字段 空行（这一个空行一定存在） 内容实体 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。 （2）HTTP响应 响应行 协议版本，响应状态码，原因短语 响应首部字段 空行 内容实体 （3）字段 通用头（通用头域包含请求和响应消息都支持的头域，通用头域包含缓存头部Cache-Control、Pragma及信息性头部Connection、Date、Transfer-Encoding、Update、Via） 名字 含义 Date Date头域表示消息发送的时间，服务器响应中要包含这个头部，因为缓存在评估响应的新鲜度时要用到，其时间的描述格式由RFC822定义。例如，Date:Mon,31 Dec 2001 04:25:57 GMT。Date描述的时间表示世界标准时，换算成本地时间，需要知道用户所在的时区。 Transfer-Encoding WEB 服务器表明自己对本响应消息体（不是消息体里面的对象）作了怎样的编码，比如是否分块（chunked），例如：Transfer-Encoding: chunked Pragma Pragma头域用来包含实现特定的指令，最常用的是Pragma:no-cache。在HTTP/1.1协议中，它的含义和Cache- Control:no-cache相同。 Connection Connection表示是否需要持久连接。 Cache-Control Cache-Control指定请求和响应遵循的缓存机制。在请求消息或响应消息中设置 Cache-Control并不会修改另一个消息处理过程中的缓存处理过程。请求时的缓存指令包括no-cache、no-store、max-age、max-stale、min-fresh、only-if-cached，响应消息中的指令包括public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age。 Upgrade 它可以指定另一种可能完全不同的协议，如HTTP/1.1客户端可以向服务器发送一条HTTP/1.0请求，其中包含值为“HTTP/1.1”的Update头部，这样客户端就可以测试一下服务器是否也使用HTTP/1.1了。 Via 列出从客户端到 OCS 或者相反方向的响应经过了哪些代理服务器，他们用什么协议（和版本）发送的请求。 HTTP请求头（请求头用于说明是谁或什么在发送请求、请求源于何处，或者客户端的喜好及能力。服务器可以根据请求头部给出的客户端信息，试着为客户端提供更好的响应。） 名字 含义 Accept 告诉WEB服务器自己接受什么介质类型，/ 表示任何类型，type/* 表示该类型下的所有子类型，type/sub-type。 Accept-Charset 浏览器告诉服务器自己能接收的字符集。 Accept-Encoding 浏览器申明自己接收的编码方法，通常指定压缩方法，是否支持压缩，支持什么压缩方法（gzip，deflate）。 Accept-Language 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等。 Authorization 当客户端接收到来自WEB服务器的 WWW-Authenticate 响应时，用该头部来回应自己的身份验证信息给WEB服务器。 If-Match 如果对象的 ETag 没有改变，其实也就意味著对象没有改变，才执行请求的动作，获取文档。 If-None-Match 如果对象的 ETag 改变了，其实也就意味著对象也改变了，才执行请求的动作，获取文档。 If-Modified-Since 如果请求的对象在该头部指定的时间之后修改了，才执行请求的动作（比如返回对象），否则返回代码304，告诉浏览器该对象没有修改。例如：If-Modified-Since：Thu, 10 Apr 2008 09:14:42 GMT If-Unmodified-Since 如果请求的对象在该头部指定的时间之后没修改过，才执行请求的动作（比如返回对象）。 If-Range 浏览器告诉 WEB 服务器，如果我请求的对象没有改变，就把我缺少的部分给我，如果对象改变了，就把整个对象给我。浏览器通过发送请求对象的ETag 或者自己所知道的最后修改时间给 WEB 服务器，让其判断对象是否改变了。总是跟 Range 头部一起使用。 Range 浏览器（比如 Flashget 多线程下载时）告诉 WEB 服务器自己想取对象的哪部分。例如：Range: bytes=1173546 Proxy-Authenticate 代理服务器响应浏览器，要求其提供代理身份验证信息。 Proxy-Authorization 浏览器响应代理服务器的身份验证请求，提供自己的身份信息。 Host 客户端指定自己想访问的WEB服务器的域名/IP 地址和端口号。如Host：rss.sina.com.cn Referer 浏览器向WEB 服务器表明自己是从哪个网页URL获得点击当前请求中的网址/URL，例如：Referer：http://www.jb51.net User-Agent 浏览器表明自己的身份（是哪种浏览器）。例如：User-Agent：Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN;rv:1.8.1.14) Gecko/20080404 Firefox/2.0.0.14 HTTP响应头（响应头向客户端提供一些额外信息，比如谁在发送响应、响应者的功能，甚至与响应相关的一些特殊指令。这些头部有助于客户端处理响应，并在将来发起更好的请求。） 名字 含义 Age 当代理服务器用自己缓存的实体去响应请求时，用该头部表明该实体从产生到现在经过多长时间了。 Server WEB 服务器表明自己是什么软件及版本等信息。例如：Server：Apache/2.0.61 (Unix) Accept-Ranges WEB服务器表明自己是否接受获取其某个实体的一部分（比如文件的一部分）的请求。bytes：表示接受，none：表示不接受。 Vary WEB服务器用该头部的内容告诉 Cache 服务器，在什么条件下才能用本响应所返回的对象响应后续的请求。假如源WEB服务器在接到第一个请求消息时，其响应消息的头部为：Content-Encoding:gzip; Vary: Content-Encoding，那么Cache服务器会分析后续请求消息的头部，检查其Accept-Encoding，是否跟先前响应的Vary头部值一致，即是否使用相同的内容编码方法，这样就可以防止Cache服务器用自己Cache里面压缩后的实体响应给不具备解压能力的浏览器。例如：Vary：Accept-Encoding。 HTTP实体头部（实体头部提供了有关实体及其内容的大量信息，从有关对象类型的信息，到能够对资源使用的各种有效的请求方法。总之，实体头部可以告知接收者它在对什么进行处理。请求消息和响应消息都可以包含实体信息，实体信息一般由实体头域和实体组成。实体头域包含关于实体的原信息，实体头包括信息性头部Allow、Location，内容头部Content-Base、Content-Encoding、Content-Language、Content-Length、Content-Location、Content-MD5、Content-Range、Content-Type，缓存头部Etag、Expires、Last-Modified、extension-header。） 名字 含义 Allow 服务器支持哪些请求方法（如GET、POST等）。 Location 表示客户应当到哪里去提取文档，用于将接收端定位到资源的位置（URL）上。Location通常不是直接设置的，而是通过HttpServletResponse的sendRedirect方法，该方法同时设置状态代码为302。 Content-Base 解析主体中的相对URL时使用的基础URL。 Content-Encoding WEB服务器表明自己使用了什么压缩方法（gzip，deflate）压缩响应中的对象。例如：Content-Encoding：gzip Content-Language WEB 服务器告诉浏览器理解主体时最适宜使用的自然语言。 Content-Length WEB服务器告诉浏览器自己响应的对象的长度或尺寸，例如：Content-Length: 26012 Content-Location 资源实际所处的位置。 Content-MD5 主体的MD5校验和。 Content-Range 实体头用于指定整个实体中的一部分的插入位置，他也指示了整个实体的长度。在服务器向客户返回一个部分响应，它必须描述响应覆盖的范围和整个实体长度。一般格式：Content-Range:bytes-unitSPfirst-byte-pos-last-byte-pos/entity-legth。例如，传送头500个字节次字段的形式：Content-Range:bytes0-499/1234如果一个http消息包含此节（例如，对范围请求的响应或对一系列范围的重叠请求），Content-Range表示传送的范围，Content-Length表示实际传送的字节数。 Content-Type WEB 服务器告诉浏览器自己响应的对象的类型。例如：Content-Type：application/xml Etag 就是一个对象（比如URL）的标志值，就一个对象而言，比如一个html文件，如果被修改了，其Etag也会别修改，所以，ETag的作用跟Last-Modified的作用差不多，主要供WEB服务器判断一个对象是否改变了。比如前一次请求某个html文件时，获得了其ETag，当这次又请求这个文件时，浏览器就会把先前获得ETag值发送给WEB服务器，然后WEB服务器会把这个ETag跟该文件的当前ETag进行对比，然后就知道这个文件有没有改变了。 Expires WEB服务器表明该实体将在什么时候过期，对于过期了的对象，只有在跟WEB服务器验证了其有效性后，才能用来响应客户请求。是 HTTP/1.0 的头部。例如：Expires：Sat, 23 May 2009 10:02:12 GMT Last-Modified WEB服务器认为对象的最后修改时间，比如文件的最后修改时间，动态页面的最后产生时间等等。例如：Last-Modified：Tue, 06 May 2008 02:42:43 GMT 18.HTTP1.0，HTTP1.1，HTTP2.0之间的区别？ （1）HTTP1.0： 无法复用连接 对头阻塞（head of line blocking）:对于同一个tcp连接，所有的http1.0请求放入队列中，只有前一个请求的响应收到了，然后才能发送下一个请求。可见，http1.0的队首组塞发生在客户端。 （2）HTTP1.1： 长连接（在头部加入了connection：keep-alive） 管道化（将请求队列移动到服务端队列）HTTP/1.1通过pipelining管道技术实现一次性发送多个请求，以期提高吞吐和性能,可见，http1.1的队首阻塞发生在服务器端。 缓存机制（引入了新的字段cache-control，支持断点重传） 增加了host字段（使得一个服务器可创建多个站点） (3）HTTP2.0： 二进制分帧 多路复用（消息由一个帧或者多个帧组成，可以乱序进行发送，之后使用帧的stream id进行重组，二进制分帧使得多路复用成为可能，多路复用实现真正的并发） 头部压缩，通信双方保存header filed表 服务器推送（不用客户端进行明确请求） （4）最新的HTTP版本？ https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Evolution_of_HTTP Post-HTTP/2 evolution HTTP/3 - HTTP over QUIC （5）在应用中如何配置HTTP版本？ 123456789101112131415161718&gt; # For HTTP, the proxy_http_version directive should be set to “1.1” and the “Connection” header field should be cleared:&gt; upstream http_backend &#123;&gt; server 127.0.0.1:8080;&gt; &gt; keepalive 16;&gt; &#125;&gt; &gt; server &#123;&gt; ...&gt; &gt; location /http/ &#123;&gt; proxy_pass http://http_backend;&gt; proxy_http_version 1.1;&gt; proxy_set_header Connection "";&gt; ...&gt; &#125;&gt; &#125;&gt; &gt; 新HTTP1.0与HTTP1.1区别： HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接、持续连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTP1.1与HTTP2.0的区别： 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。 header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。 服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能 19.cookie和session的区别？ 说先说一下为什么需要cookie和session？ （1）cookie数据存放在客户的浏览器上，session存放在服务器上。 （2）cookie不是安全的，别人可以分析存放在本地的cookie进行cookie欺骗。 （3）session会一定时间内存放在服务器上，当访问次数增多的时候，会影响性能。 （4）单个cookie保存的数据不会超过4K，很多浏览器限制一个站点的cookie数目不超过20个。 20.状态码？（1）概括： 类别 原因短语 1XX Informational（信息状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 （2）细节 状态码 状态码英文名称 中文描述 100 Continue 继续。客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 200 OK 请求成功。一般用于GET与POST请求 201 Created 已创建。成功请求并创建了新的资源 202 Accepted 已接受。已经接受请求，但未处理完成 203 Non-Authoritative Information 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 204 No Content 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 205 Reset Content 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 206 Partial Content 部分内容。服务器成功处理了部分GET请求 300 Multiple Choices 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 301 Moved Permanently 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 303 See Other 查看其它地址。与301类似。使用GET和POST请求查看 304 Not Modified 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 305 Use Proxy 使用代理。所请求的资源必须通过代理访问 306 Unused 已经被废弃的HTTP状态码 307 Temporary Redirect 临时重定向。与302类似。使用GET请求重定向 400 Bad Request 客户端请求的语法错误，服务器无法理解 401 Unauthorized 请求要求用户的身份认证 402 Payment Required 保留，将来使用 403 Forbidden 服务器理解请求客户端的请求，但是拒绝执行此请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面 405 Method Not Allowed 客户端请求中的方法被禁止 406 Not Acceptable 服务器无法根据客户端请求的内容特性完成请求 407 Proxy Authentication Required 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 408 Request Time-out 服务器等待客户端发送的请求时间过长，超时 409 Conflict 服务器完成客户端的 PUT 请求时可能返回此代码，服务器处理请求时发生了冲突 410 Gone 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 411 Length Required 服务器无法处理客户端发送的不带Content-Length的请求信息 412 Precondition Failed 客户端请求信息的先决条件错误 413 Request Entity Too Large 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 414 Request-URI Too Large 请求的URI过长（URI通常为网址），服务器无法处理 415 Unsupported Media Type 服务器无法处理请求附带的媒体格式 416 Requested range not satisfiable 客户端请求的范围无效 417 Expectation Failed 服务器无法满足Expect的请求头信息 500 Internal Server Error 服务器内部错误，无法完成请求 501 Not Implemented 服务器不支持请求的功能，无法完成请求 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 504 Gateway Time-out 充当网关或代理的服务器，未及时从远端服务器获取请求 505 HTTP Version not supported 服务器不支持请求的HTTP协议的版本，无法完成处理 1XX——表示通知信息，如请求收到了或正在进行处理 2XX——表明请求被正常处理了 200 OK：请求已正常处理。 204 No Content：请求处理成功，但没有任何资源可以返回给客户端，一般在只需要从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content：是对资源某一部分的请求，该状态码表示客户端进行了范围请求，而服务器成功执行了这部分的GET请求。响应报文中包含由Content-Range指定范围的实体内容。 3XX——表明浏览器需要执行某些特殊的处理以正确处理请求 301 Moved Permanently：资源的uri已更新，你也更新下你的书签引用吧。永久性重定向，请求的资源已经被分配了新的URI，以后应使用资源现在所指的URI。 302 Found：资源的URI已临时定位到其他位置了，姑且算你已经知道了这个情况了。临时性重定向。和301相似，但302代表的资源不是永久性移动，只是临时性性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。 303 See Other：资源的URI已更新，你是否能临时按新的URI访问。该状态码表示由于请求对应的资源存在着另一个URL，应使用GET方法定向获取请求的资源。303状态码和302状态码有着相同的功能，但303状态码明确表示客户端应当采用GET方法获取资源，这点与302状态码有区别。当301,302,303响应状态码返回时，几乎所有的浏览器都会把POST改成GET，并删除请求报文内的主体，之后请求会自动再次发送。 304 Not Modified：资源已找到，但未符合条件请求。该状态码表示客户端发送附带条件的请求时（采用GET方法的请求报文中包含If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since中任一首部）服务端允许请求访问资源，但因发生请求未满足条件的情况后，直接返回304。 307 Temporary Redirect：临时重定向。与302有相同的含义。 4XX——表明客户端是发生错误的原因所在。 400 Bad Request：服务器端无法理解客户端发送的请求，请求报文中可能存在语法错误。 401 Unauthorized：该状态码表示发送的请求需要有通过HTTP认证（BASIC认证，DIGEST认证）的认证信息。 403 Forbidden：不允许访问那个资源。该状态码表明对请求资源的访问被服务器拒绝了。（权限，未授权IP等） 404 Not Found：服务器上没有请求的资源。路径错误等。 5XX——服务器本身发生错误 500 Internal Server Error：貌似内部资源出故障了。该状态码表明服务器端在执行请求时发生了错误。也有可能是web应用存在bug或某些临时故障。 503 Service Unavailable：抱歉，我现在正在忙着。该状态码表明服务器暂时处于超负载或正在停机维护，现在无法处理请求。 21.TCP是如何实现面向连接的？面向连接和非面向连接的区别？（1）状态和序列号，以及错误校验。描述TCP和UDP头之间的差异！ 22.TCP的拥塞控制？（重传就可能导致拥塞）TCP通过慢启动、拥塞避免、快重传以及快恢复这四个算法来进行拥塞控制（使用滑动窗口进行流量控制）： 慢启动：一开始先设置一个比较小的拥塞窗口值cwnd（报文段的倍数），然后进行数据传输，每收到一个报文段的确认，我们就将cwnd+1，这样下来，cwnd总体上是乘以2^n的倍数增长。（慢启动非增长速度慢，只是增长的初始基数比较小） 拥塞避免： 因为慢启动算法的增长比较快，当cwnd = ssthresh（预先设置好的门限值）时，我们启动拥塞避免算法，窗口值开始线性增长。 随着拥塞避免算法的进行，网络出现超时的情况（这时判断为拥塞出现）。这时将cwnd降为一开始的值，重新进行慢开始-拥塞避免，并且此时的门限值设为出现拥塞时的cwnd的一半。 快重传： 快重传的目的是为了让发送方尽早知道某个报文段的丢失。如何知道呢？当我们重复收到某一个报文段的3次确认时，我们就可以判断，它的下一个报文段可能出现了丢失。这时我们启动快重传算法，立即重传丢失的报文段。 快恢复： 上面快重传算法的启动只是因为个别报文段的丢失，我们这时并不判断为网络拥塞，而是启动快恢复算法。我们将cwnd=ssthresh=当前cwnd的一半，并且开始拥塞避免算法。 当然，也有的快恢复算法是将当前拥塞窗口再增大3个报文段的值，因为既然收到了3个重复的ACK，则说明有三个分组已经离开了网络，不在占用网络资源而是停留在对方缓存当中，可以适当将窗口值增大。 23.TCP的流量控制？滑动窗口协议 24.重传算法？ SACK方法 为了解决快速重传的缺点，一种更好的SACK重传策略被提出 基于快速重传，同时在tcp头里加了一个SACK的东西 解决了什么问题：客户端应该发送哪些超时包的问题 25.路由算法？ （1）路由：找到任意两个节点之间开销最小的路径。 （2）距离向量，链路状态 距离向量：网络中没有任何一个节点知道整张表的信息，自己只知道它自己的路由表的内容。好处：所有的节点在没有任何集中授权的额情况下取得网络的一致视图。（RIP协议） 链路状态：每一个节点都有足够的信息构建完整的网络映象。（OSPF协议，开放最短路径优先），路由的计算采用迪杰特斯拉算法进行计算。 26.IP数据报格式？ 27.ABC类地址 A、B、C类IP地址的网络号字段分别是1、2、3个字节长，而在网络号的1-3位是类别位，分别是：0、10、110。 A、B、C类IP地址的主机号字段分别为3、2、1个字节。 A、B、C类IP地址是单播地址，D类IP地址（前四位为1110）为多播地址，E类IP地址（前四位1111）保留为以后使用。 A类地址的网络号中：全0和127是不指派的；主机号中：全0代表本主机所连接的单个网络地址，全1代表网络上的所有主机，也是不指派的。 B类IP地址网络号中：128.0.0.0不指派；主机号中：全0和全1也不指派。 C类IP地址网络号中：192.0.0.0不指派；主机号中：全0和全1也不指派。 28.端口号 应用程序 FTP TELNET SMTP DNS TFTP HTTP SNMP SNMP(trap) HTTPS 熟知端口号 21 23 25 53 69 80 161 162 443 29.滑动窗口（解决的是速率不匹配问题） 解决了什么问题：发送方和接收方速率不匹配时，保证可靠传输和包乱序的问题 机制：接收方根据目前缓冲区大小，通知发送方目前能接收的最大值。发送方根据接收方的处理能力来发送数据。通过这种协调机制，防止接收端处理不过来。 窗口大小：接收方发给发送端的这个值称为窗口大小 30.拥塞窗口（控制的是发送方） 解决什么问题：发送方发送速度过快，导致中转路由器拥堵的问题 机制：发送方增加一个拥塞窗口（cwnd），每次受到ack，窗口值加1。发送时，取拥塞窗口和接收方发来的窗口大小取最小值发送 起到发送方流量控制的作用 31.细节 MIME (Multipurpose Internet Mail Extensions) 是描述消息内容类型的因特网标准。 Request For Comments（RFC），是一系列以编号排定的文件。文件收集了有关互联网相关信息，以及UNIX和互联网社区的软件文件。 RIP使用UDP，OSPF使用IP,而BGP使用TCP。(R—-U,O—-I,B—-P) OSPF本身提供主从协商机制，可以保证可靠的传输，另外全网路由器保持着同样的一个LSDB（链路状态数据库），当拓扑发生变化时，需要携带的变更信息较少， 通过IP协议即可完成RIP协议采用UDP是因为RIP每周期需全网组播路由信息，路由信息数目较大，故使用UDP协议可以提高效率 BGP为边界网关协议，因携带的路由信息较多，且可能跨不同网络传送路由信息，为保证可靠性，需使用TCP协议，可兼顾容量和可靠性 32.HTTP和HTTPS的区别？ 证书机构： 阿里巴巴 (1) https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。(原来网易官网是http，而网易邮箱是https。) (2) http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 (3) http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 33.同源策略？ URL 结果 原因 http://store.company.com/dir2/other.html 成功 只有路径不同 http://store.company.com/dir/inner/another.html 成功 只有路径不同 https://store.company.com/secure.html 失败 不同协议 ( https和http ) http://store.company.com:81/dir/etc.html 失败 不同端口 ( http:// 80是默认的) http://news.company.com/dir/other.html 失败 不同域名 ( news和store ) 34.跨域问题？ 同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。 CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 1.你不能保证你登录了一个网站后，不再打开一个tab页面并访问另外的网站。 2.你不能保证你关闭浏览器了后，你本地的Cookie立刻过期，你上次的会话已经结束。（事实上，关闭浏览器不能结束一个会话，但大多数人都会错误的认为关闭浏览器就等于退出登录/结束会话了……） 3.上图中所谓的攻击网站，可能是一个存在其他漏洞的可信任的经常被人访问的网站。 CORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing） 35. websocket和SSE的区别？36. epoll epoll_create epoll_ctl epoll_wait 37. cacheline，MESI4kb 64bytes 二、操作系统（一）基础1.原码、补码、反码 原码：原码用第一位表示符号, 其余位表示值. 比如如果是8位二进制 补码 正数：正数的补码就是其本身 负数：负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1。 (即在反码的基础上+1) 为什么要用补码？ 原因很简单，如果使用补码表示负整数，那么ALU在做整数之间的操作时，就不用区分符号了，所有位都会参与运算，其上上面的例子中，符号位都参与了运算。 反码 正数：正数的反码是其本身 负数：负数的反码是在其原码的基础上, 符号位不变，其余各个位取反. 2.fork函数 int main(){fork()||fork();}共创建几个进程：3 fork()给子进程返回一个零值，而给父进程返回一个非零值； 在main这个主进程中，首先执行 fork() || fork(), 左边的fork()返回一个非零值，根据||的短路原则，前面的表达式为真时，后面的表达式不执行，故包含main的这个主进程创建了一个子进程， 由于子进程会复制父进程，而且子进程会根据其返回值继续执行，就是说，在子进程中， fork() ||fork()这条语句左边表达式的返回值是0, 所以||右边的表达式要执行，这时在子进程中又创建了一个进程， 即main进程-&gt;子进程-&gt;子进程，一共创建了3个进程。 （二）并发1.进程、线程、管程、协程？（1）线程和进程(Thread &amp; Process)线程是程序执行的一条路径，在多线程的OS中，线程是调度和分配的基本单位，而进程是拥有资源的基本单位。结合Java的内存区域（线程共享和线程私有）。 123456789101112131415161718192021222324252627282930package JavaDemo.MultiThreadTest;import java.lang.management.ManagementFactory;import java.lang.management.ThreadInfo;import java.lang.management.ThreadMXBean;/** * @Author MaoTian * @Classname MultiThread * @Description 查看有哪些线程 * * [5] Monitor Ctrl-Break * [4] Signal Dispatcher * [3] Finalizer * [2] Reference Handler * [1] main * * @Date 下午9:23 2019/8/13 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class MultiThread &#123; public static void main(String[] args) &#123; ThreadMXBean threadMXBean= ManagementFactory.getThreadMXBean(); ThreadInfo[] threadInfos=threadMXBean.dumpAllThreads(false,false); for (ThreadInfo threadInfo:threadInfos)&#123; System.out.println("[" + threadInfo.getThreadId() + "] " + threadInfo.getThreadName()); &#125; &#125;&#125; （2）线程的属性 轻型实体：线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。TCB包括以下信息： 线程状态。 当线程不运行时，被保存的现场资源。 一组执行堆栈。 存放每个线程的局部变量主存区。 访问同一个进程中的主存和其它资源。 用于指示被执行指令序列的程序计数器、保留局部变量、少数状态参数和返回地址等的一组寄存器和堆栈。 独立调度和分派的基本单位：在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 可并发执行：在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 共享进程资源：在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的地址空间（进程的地址空间），这意味着，线程可以访问该地址空间的每一个虚地址；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 （3）管程（4）协程协程：协程，英文Coroutines，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。最重要的是，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。（线程内核管理，协程自己控制切换，用户态和内核态切换，比线程切换代价更小） 2.进程之间的通信？（套共消，管信信） （1）套接字 （2）共享内存 （3）消息队列 （4）管程 （5）信号 （6）信号量 信号量 是一个确定的二元组（s，q），其中s是一个具有非负初值的整形变量，q是一个初始状态为空的队列，整形变量s表示系统中某类资源的数目： 当其值 &gt;= 0 时，表示系统中当前可用资源的数目 当其值 &lt; 0 时，其绝对值表示系统中因请求该类资源而被阻塞的进程数目 除信号量的初值外，信号量的值仅能由P操作和V操作更改，操作系统利用它的状态对进程和资源进行管理 P操作P操作记为P(s)，其中s为一信号量，它执行时主要完成以下动作： 12&gt;s.value = s.value - 1； /*可理解为占用1个资源，若原来就没有则记帐“欠”1个*/&gt; 若s.value ≥ 0，则进程继续执行，否则（即s.value &lt; 0），则进程被阻塞，并将该进程插入到信号量s的等待队列s.queue中 实际上，P操作可以理解为分配资源的计数器，或是使进程处于等待状态的控制指令 V操作V操作记为V(s)，其中s为一信号量，它执行时，主要完成以下动作： 12&gt;s.value = s.value + 1；/*可理解为归还1个资源，若原来就没有则意义是用此资源还1个欠帐*/&gt; 若s.value &gt; 0，则进程继续执行，否则（即s.value ≤ 0），则从信号量s的等待队s.queue中移出第一个进程，使其变为就绪状态，然后返回原进程继续执行 实际上，V操作可以理解为归还资源的计数器，或是唤醒进程使其处于就绪状态的控制指令 3.信号和信号量之间的区别？ 信号量：（Semaphore）进程间通信处理同步互斥的机制。是在多线程环境下使用的一种设施, 它负责协调各个线程, 以保证它们能够正确、合理的使用公共资源。（特点，pv操作，用于同步进程） 若信号S的初值为2，当前值为－1，则表示有（B ）个等待进程。 A.0 B.1 C.2 D.3 2代表有两个资源空闲,负数的绝对值表示在等待的进程数量 信号：（signal）是一种处理异步事件的方式。信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程外，还可以发送信号给进程本身。（特点，通知） 4.线程之间的通信？ （1）锁机制 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 （2）信号量机制 （3）信号机制 5.死锁产生的条件？ （1）互斥 （2）请求与保持:指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 （3）循环等待（解决：按照顺序来申请和释放资源） （4）不可剥夺（解决：主动释放） 6.死锁的解除和预防的方法？ 死锁避免:银行家算法 我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 7.调度 （1）调度策略 响应时间：从用户输入到产生反应的时间 周转时间：从任务开始到任务结束的时间 平均周转时间：周转总时间除以作业个数 （2）调度算法(8种) 1 FCFS：调度的顺序就是任务到达就绪队列的顺序。对短作业不公平。 公平、简单(FIFO队列)、非抢占、不适合交互式。未考虑任务特性，平均等待时间可以缩短 2 SJF：最短的作业(CPU区间长度最小)最先调度。 可以证明，SJF可以保证最小的平均等待时间。 [3] SRJF：SJF的可抢占版本，比SJF更有优势。 SJF(SRJF): 如何知道下一CPU区间大小？根据历史进行预测: 指数平均法。 [4] HRN：最高响应比优先法，是FCFS和SJF的综合平衡，响应比R定义如下： R =(W+T)/T 。 [5] 优先权调度：每个任务关联一个优先权，调度优先权最高的任务。 注意：优先权太低的任务一直就绪，得不到运行，出现“饥饿”现象。 FCFS是RR的特例，SJF是优先权调度的特例。这些调度算法都不适合于交互式系统。 [6] Round-Robin(RR)：设置一个时间片，按时间片来轮转调度 优点: 定时有响应，等待时间较短；缺点: 上下文切换次数较多； 时间片太大，响应时间太长；吞吐量变小，周转时间变长；当时间片过长时，退化为FCFS。 [7] 多级队列调度 按照一定的规则建立多个进程队列 不同的队列有固定的优先级（高优先级有抢占权） 不同的队列可以给不同的时间片和采用不同的调度方法 存在问题1：没法区分I/O bound和CPU bound； 存在问题2：也存在一定程度的“饥饿”现象； [8] 多级反馈队列：在多级队列的基础上，任务可以在队列之间移动，更细致的区分任务。可以根据“享用”CPU时间多少来移动队列，阻止“饥饿”。 最通用的调度算法，多数OS都使用该方法或其变形，如UNIX、Windows等。 #### 8.锁，死锁怎么检查？ 锁的类型 （1）互斥锁 同一时间只能有一个线程访问加锁的数据。 （2）自旋锁 互斥锁的一种实现，如果自旋锁已经被别的执行单元保持，调用者就一直 循环等待 是否该自旋锁的保持者已经释放了锁。 （3）读写锁 一种特殊的自旋锁，它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作。写者是排他性的，一个读写锁同时只能有一个写者或多个读者（与CPU数相关），但不能同时既有读者又有写者。 （4）阻塞锁 与自旋锁不同，改变了线程的运行状态。让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。 在Java中synchronized,ReentrantLock,Object.wait() / notify()都属于阻塞锁。 （5）可重入锁 也叫做递归锁，指的是同一线程上该锁是可重入的，对于不同线程则相当于普通的互斥锁。 （6）公平锁 加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得。 （7）非公平锁 加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待。ReentrantLock中的lock()默认就是非公平锁。 （8）悲观锁 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。加锁的时间可能会很长，也就是说悲观锁的并发访问性不好。 （9）乐观锁 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。乐观锁不能解决脏读的问题，可以通过添加时间戳和版本来来解决。 死锁的检查 https://blog.csdn.net/weixin_28760063/article/details/81266578 Jconsole Jstack 9.CAS比较并交换(compare and swap, CAS)，是原子操作的一种，可用于在多线程编程中实现不被打断的数据交换操作。该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。 在使用上，通常会记录下某块内存中的旧值，通过对旧值进行一系列的操作后得到新值，然后通过CAS操作将新值与旧值进行交换。如果这块内存的值在这期间内没被修改过，则旧值会与内存中的数据相同，这时CAS操作将会成功执行使内存中的数据变为新值。如果内存中的值在这期间内被修改过，则一般来说旧值会与内存中的数据不同，这时CAS操作将会失败，新值将不会被写入内存。 在Java中，锁在并发中占据了一席之地，但是使用锁的一个问题是：当一个线程没有获取到锁的时候就会被挂起，这将导致线程的上下文切换和重新调度的开销。 10.临界区进程中访问临界资源的那段程序称为临界区（临界资源是一次仅允许一个进程使用的共享资源）。每次只准许一个进程进入临界区，进入后不允许其他进程进入。 11.yield方法，join方法 yield方法 而当一个线程调用了 Thread 类的静态方法 yield 时，是在告诉线程调度器自己占有的时间片中还没有使用完的部分自己不想使用了，这暗示线程调度器现在就可以进行下一轮的线程调度 。 join方法 t.join()方法阻塞调用此方法的线程(calling thread)进入 TIMED_WAITING 状态，直到线程t完成，此线程再继续； 通常用于在main()主线程内，等待其它线程完成再结束main()主线程。 1234567891011121314151617181920212223242526272829303132333435&gt;public class JoinTester01 implements Runnable &#123;&gt; &gt; private String name;&gt; &gt;public JoinTester01(String name) &#123;&gt; this.name = name;&gt;&#125;&gt; &gt;public void run() &#123;&gt; System.out.printf("%s begins: %s\n", name, new Date());&gt;try &#123;&gt; TimeUnit.SECONDS.sleep(4);&gt;&#125; catch (InterruptedException e) &#123;&gt; e.printStackTrace();&gt;&#125;&gt; System.out.printf("%s has finished: %s\n", name, new Date());&gt; &#125;&gt; &gt; public static void main(String[] args) &#123;&gt; Thread thread1 = new Thread(new JoinTester01("One"));&gt; Thread thread2 = new Thread(new JoinTester01("Two"));&gt; thread1.start();&gt; thread2.start();&gt; &gt; try &#123;&gt; thread1.join();&gt; thread2.join();&gt; &#125; catch (InterruptedException e) &#123;&gt; // TODO Auto-generated catch block&gt; e.printStackTrace();&gt; &#125;&gt; System.out.println("Main thread is finished");&gt; &#125;&gt; &#125;&gt; 12.一般在什么时候使用volatile？写入的变量不依赖当前的值的时候。因为依赖当前值的话，将会是获取—-计算—-写入三步操作，这三步操作不是原子性的，volatile不保证原子性。 13.Linux内核select poll epoll？14.周转时间=作业完成时间减去作业到达时间 15.响应比=（作业等待时间+作业执行时间）/作业执行时间 关于平均周转时间的一些题目 （１）设一个系统中有5个进程，它们的到达时间和服务时间如下，A的到达时间为0，服务时间为3；B的到达时间为2，服务时间为6；C的到达时间为4，服务时间为4；D的到达时间为6，服务时间为5；E的 到达时间为8，服务时间为2，忽略1/0以及其他开销时间，若分别按先来先服务（fFCFS）进行CPU调度，其平均周转时间为？ 答：周转时间=作业完成时间减去作业到达时间 所以 A 完成时间 ０＋３＝３ 周转时间Ａ＝３－０； B 完成时间 ３＋６＝９ 周转时间Ｂ＝９－２＝７； C 完成时间 ９＋４＝１３ 周转时间Ｃ＝１３－４＝９； D 完成时间 １３＋５＝１８ 周转时间Ｄ＝１８－６＝１２； E 完成时间 １８＋２＝２０ 周转时间 Ｅ＝２０－８＝１２； 所以平均周转时间是 （３＋７＋９＋１２＋１２）／５＝８.６ （２）单道批处理系统有４个作业，J1 的提交时间为8 运行时间2， J2的提交时间8.6 运行时间0.6 ，J3的提交时间8.8 运行时间0.2 J4的提交时间9.0 运行时间0.5 在采用响应比优先调度算法时，其平均周转时间是？ 响应比=（作业等待时间+作业执行时间）/ 作业执行时间 J1 周转时间(8+2) -8 =2； 此时 J2等待时间为(8+2-8.6)=1.4 响应比为（1.4+0.6）/0.6=10/3 J3 等待时机是(8+2-8.8)=1.2 响应比（1.2+0.2）/0.2=7 J4 等待时间是(8+2-9.0)=1.0 响应比（1.0+0.5）/0.5=3 因为J3的响应比最高，所以J3开始运行。J3 的完成时间是10+0.2=10.2周转时间是10.2-8.8=1.4 此时 J2的等待时间是10.2-8.6=1.6 响应比( 1.6+0.6)/0.6=11/3=3.6667 J4的等待时间是10.2-9.0=1.2 响应比（1.2+0.5）/0.5=3.4 因为J2的响应比高，所以J2 开始运行，J2的完成时间是10.2+0.6=10.8；周转时间10.8-8.6=2.2； 这时候运行J4,J4 的完成时间是10.8+0.5=11.3 周转时间是11.3-9.0=2.3； 因此平均周转时间是（2+1.4+2.2+2.3 )/4=1.975 16. fork函数在fork函数执行完毕后，如果创建新进程成功，则出现两个进程，一个是子进程，一个是父进程。在子进程中，fork函数返回0，在父进程中，fork返回新创建子进程的进程ID。我们可以通过fork返回的值来判断当前进程是子进程还是父进程。fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值： 1）在父进程中，fork返回新创建子进程的进程ID； 2）在子进程中，fork返回0； 3）如果出现错误，fork返回一个负值； 12345678910111213141516171819202122#include &lt;unistd.h&gt; #include &lt;stdio.h&gt; int main () &#123; pid_t fpid; //fpid表示fork函数返回的值 int count=0; fpid=fork(); if (fpid &lt; 0) printf("error in fork!"); else if (fpid == 0) &#123; printf("i am the child process, my process id is %d/n",getpid()); printf("我是爹的儿子/n");//对某些人来说中文看着更直白。 count++; &#125; else &#123; printf("i am the parent process, my process id is %d/n",getpid()); printf("我是孩子他爹/n"); count++; &#125; printf("统计结果是: %d/n",count); return 0; &#125; 运行结果是： i am the child process, my process id is 5574 我是爹的儿子 统计结果是: 1 i am the parent process, my process id is 5573 我是孩子他爹 统计结果是: 1 1234567891011121314151617#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt; int main(void)&#123; int i; for(i=0; i&lt;2; i++)&#123; fork(); printf("%d.-\n",i); &#125; wait(NULL); wait(NULL); return 0;&#125; 17. 上下文切换​ Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好 CPU 寄存器和程序计数器(Program Counter，PC)。CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。根据任务的不同，CPU的上下文切换可以分为不同的场景，也就是进程上下文切换、线程上下文切换、中断上下文切换。 ​ 上下文切换指的是内核（操作系统的核心）在CPU上对进程或者线程进行切换。上下文切换过程中的信息被保存在进程控制块（PCB-Process Control Block）中。PCB又被称作切换帧（SwitchFrame）。上下文切换的信息会一直被保存在CPU的内存中，直到被再次使用。 https://www.cnblogs.com/williamjie/p/9466941.html 18. 进程切换（三）内存管理1.页面置换算法 FIFO算法：先入先出，即淘汰最早调入的页面。 OPT(MIN)算法：选未来最远将使用的页淘汰，是一种最优的方案，可以证明缺页数最小。可惜，MIN需要知道将来发生的事，只能在理论中存在，实际不可应用。 LRU(Least-Recently-Used)算法：用过去的历史预测将来，选最近最长时间没有使用的页淘汰(也称最近最少使用)。性能最接近OPT。与页面使用时间有关。 LFU(Least Frequently Used)算法：即最不经常使用页置换算法，要求在页置换时置换引用计数最小的页，因为经常使用的页应该有一个较大的引用次数。与页面使用次数有关。 Clock：给每个页帧关联一个使用位，当该页第一次装入内存或者被重新访问到时，将使用位置为1。每次需要替换时，查找使用位被置为0的第一个帧进行替换。在扫描过程中，如果碰到使用位为1的帧，将使用位置为0，在继续扫描。如果所谓帧的使用位都为0，则替换第一个帧。 在一个请求页式存储管理中，一个程序的页面走向为 3、4、2、1、4、5、3、4、5、1、2，并采用 LRU算法。设分配给该程序的存储块数 S 分别为 3 和 4，在该访问中发生的缺页次数 F 是8,7 https://www.nowcoder.com/questionTerminal/780dce19969445c5a7814c0ff087c103 2. 内存分页现代计算机都使用分页（Paging）的方式对虚拟地址空间和物理地址空间进行分割和映射，以减小换入换出的粒度，提高程序运行效率。 分页（Paging）的思想是指把地址空间人为地分成大小相等（并且固定）的若干份，这样的一份称为一页，就像一本书由很多页面组成，每个页面的大小相等。如此，就能够以页为单位对内存进行换入换出： 当程序运行时，只需要将必要的数据从磁盘读取到内存，暂时用不到的数据先留在磁盘中，什么时候用到什么时候读取。当物理内存不足时，只需要将原来程序的部分数据写入磁盘，腾出足够的空间即可，不用把整个程序都写入磁盘。 （四）I/O1.I/O模式？参考 阻塞I/O（blocking IO） 非阻塞I/O（nonblocking IO） 调用blocking IO会一直block住对应的进程直到操作完成 non-blocking IO在kernel还准备数据的情况下会立刻返回。 I/O多路复用（ IO multiplexing） 异步I/O（asynchronous IO） 下图对IO模型进行了对比： （1）阻塞I/O（blocking IO）​ 当用户进程调用了 recvfrom 这个系统调用， kernel 就开始了 IO 的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的 UDP 包。这个时候 kernel 就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当 kernel 一直等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。 blocking IO的特点就是在IO执行的两个阶段都被block了 （2）非阻塞I/O（nonblocking IO）​ 当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error 。从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送 read 操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call ，那么它马上就将数据拷贝到了用户内存，然后返回。 nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有. （3）I/O多路复用（ IO multiplexing）​ （Java中的NIO使用channel来完成多路复用）,IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 （4）信号驱动I/O（ signal driven IO）（5）异步I/O（asynchronous IO）​ （并不会加快io的过程）用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block 。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal ，告诉它 read 操作完成了。 2.零拷贝zero-copy​ 零拷贝操作减少了在用户空间与内核空间之间切换模式的次数。磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。 2.1 用户空间和内核空间​ 说到零拷贝，首先“用户空间”和“内核空间”说起，用户空间和内核空间的空间、权限以及作用是不一样的，用户空间是提供给各个用户使用的主要空间，内核空间是操作系统自身使用的空间，主要提供给程序调度、内存分配、连接硬件资源等。 用户空间不具备访问内核空间资源的权限，因此如果应用程序需要使用到内核资源的话，必须切换到内核空间：首先从用户空间切换到内核空间，然后在完成相关的操作后再从内核空间切换到用户空间。 2.2 DMA技术在没有 DMA 技术前，I/O 的过程是这样的： CPU 发出对应的指令给磁盘控制器，然后返回； 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断； CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。 流程如下： ​ 可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是直接内存访问（Direct Memory Access）技术。 什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。流程如下： 具体过程： 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态； 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务； DMA 进一步将 I/O 请求发送给磁盘； 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满； DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务； 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU； CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回； ​ 可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。 2.3 传统文件传输有多糟糕？代码通常如下，一般会需要两个系统调用： 12read(file, tmp_buf, len);write(socket, tmp_buf, len); 首先，期间共发生了 4 次用户态与内核态的上下文切换，因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。 上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程： 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。 我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。 所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。 2.4 零拷贝—-mmap+write在前面我们知道，read() 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 mmap() 替换 read() 系统调用函数。 具体过程如下： 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 ​ 我们可以得知，通过使用 mmap() 来代替 read()， 可以减少一次数据拷贝的过程。但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。 2.5 零拷贝—-sendfile在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下： 12#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 以上，只有一条系统命令，只会引起以此上下文切换。 它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。 首先，它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。 其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图： 2.6 零拷贝—-SG-DMA如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。 你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性： 12$ ethtool -k eth0 | grep scatter-gatherscatter-gather: on 于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， sendfile() 系统调用的过程发生了点变化，具体过程如下： 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 所以，这个过程之中，只进行了 2 次数据拷贝，如下图： 这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。（零拷贝：全程没有使用CPU搬运数据） 零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。 2.7 Netty的零拷贝实现？​ 在 OS 层面上的 Zero-copy 通常指避免在 用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据。而在 Netty 层面 ，零拷贝主要体现在对于数据操作的优化。 Netty 中的零拷贝体现在以下几个方面：使用 Netty 提供的 CompositeByteBuf 类, 可以将多个ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝。 ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝。 通过 FileRegion 包装的FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel, 避免了传统通过循环 write 方式导致的内存拷贝问题. 3.同步异步，阻塞非阻塞的区别。（1）阻塞，非阻塞指的有无返回值（2）同步异步指的是能否继续执行无论阻塞式IO还是非阻塞式IO，都是同步IO模型，区别就在与第一步是否完成后才返回，但第二步都需要当前进程去完成，异步IO呢，就是从第一步开始就返回，直到第二步完成后才会返回一个消息，也就是说，非阻塞能够让你在第一步时去做其它的事情，而真正的异步IO能让你第二步的过程也能去做其它事情. （五）linux使用1.CPU占用过高排查（1）top （2）ps -ef | grep java或者jps定位 （3）定位到具体的线程：ps -mp 进程 -o THREAD,tid,time （4）将线程ID转换为16进制的格式：print “%x\n” 数字 （5）jstack 进程id | grep tid（16进制线程id的小写）-A60 2.补充 计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成。 操作系统的五大功能，分别为：作业管理、文件管理、存储管理、输入输出设备管理、进程及处理机管理 中断：所谓的中断就是在计算机执行程序的过程中，由于出现了某些特殊事情，使得CPU暂停对程序的执行，转而去执行处理这一事件的程序。等这些特殊事情处理完之后再回去执行之前的程序。中断一般分为三类： 内部异常中断：由计算机硬件异常或故障引起的中断； 软中断：由程序中执行了引起中断的指令而造成的中断（这也是和我们将要说明的系统调用相关的中断）； 外部中断：由外部设备请求引起的中断，比如I/O请求。 系统调用：进程的执行在系统上的两个级别：用户级和核心级，也称为用户态和系统态(user mode and kernel mode)。程序的执行一般是在用户态下执行的，但当程序需要使用操作系统提供的服务时，比如说打开某一设备、创建文件、读写文件等，就需要向操作系统发出调用服务的请求，这就是系统调用。 3.命令? lsof -i:80 查看端口 awk 123456789&gt;#/usr/bin env&gt; # 通过find递归，得到所有的文件的完整路径,在当前路径下，递归遍历所有的文件，每个文件使用逗号分割，找出每一行第一列值为10的所有文件的记录的行号和文件名。&gt; files=$(find ./ -type f)&gt; for i in $files&gt;do&gt; # awk的-F选项指定分割符号，-v是指定的变量，可以在print中打印，'$1=="10"是指第一列中等于10的，print NR表示的是指示的行号，uniq指的是过滤掉重复的，&gt;&gt;out指的是追加到out文件&gt;awk -F "," -v mao=$PWD '$1=="10"&#123;print NR,FILENAME&#125;' $i | uniq &gt;&gt;out&gt; done&gt; sed 选项 -n -e（多条命令顺序执行，命令使用分号切割） -f -r -i（写入文件） 命令 a（append新增） c（行替换） d（delete删除） i（insert前面插入） p（print打印） s（字符串的替换 三、数据结构1.B-树，B+树？画出一颗简单的B+树，给定一个ID（主键），简述B+树的查找过程 https://www.cs.usfca.edu/~galles/visualization/Algorithms.html B树 https://blog.csdn.net/li_canhui/article/details/85305147 编号 特点 1 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 2 根结点的儿子数为[2, M]；根节点的数目是个例外 3 除根结点以外的非叶子结点的儿子数为[M/2, M]；,除了根节点,最少有M/2向上取整个子节点 4 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字） 5 非叶子结点的关键字个数=指向儿子的指针个数-1；（有减1,一个绳子砍三刀分为四截） 6 非叶子结点的关键字：K1, K2, …, K[M-1]；且K[i] &lt; K[i+1]； 7 非叶子结点的指针：P1, P2, …, P[M]；其中P1指向关键字小于K1的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树；,当成数轴来看, K[M-1]——————K[M],因为中间节点包含了数值,所以都是开区间 8 所有叶子结点位于同一层； B+树 （3,5,8,9,10,12,13,15,17,26,28,29,30,35,36,60,65,75,79,87,90,99） B+树是B-树的变体，也是一种多路搜索树，其定义基本与B-树同，除了： （1）非叶子结点的子树指针与关键字个数相同；(没有减1) （2）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])(前闭后开)的子树（B-树是开区间）； （3）为所有叶子结点增加一个链指针； （4）所有关键字都在叶子结点出现； 关键字和指针数量的关系:B树减1,B+树相等 指向区间:B树开区间,B+树前闭后开 叶节点是否增加指针?B树没有,B+树有 2.二叉树的遍历，非递归？ 前序 12345678910111213141516171819202122232425public void preorder1(BinaryTreeNode root)&#123; if (root==null) return; System.out.print(root.getData()+"\t"); preorder1(root.getLeft()); preorder1(root.getRight());&#125;public void preorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if (root==null) return; BinaryTreeNode cur; cur=root; while(cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; System.out.print(cur.getData()+"\t");//根 stack.push(cur); cur=cur.getLeft();//左 &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getRight();//右 &#125; &#125;&#125; 中序 123456789101112131415161718192021222324public void inorder1(BinaryTreeNode root)&#123; if (root==null) return; inorder1(root.getLeft()); System.out.print(root.getData()+"\t"); inorder1(root.getRight());&#125;public void inorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if(root==null) return; BinaryTreeNode cur=root; while(cur!=null||!stack.isEmpty())&#123; if(cur!=null)&#123; stack.push(cur); cur=cur.getLeft(); &#125;else&#123; cur=stack.peek(); stack.pop(); System.out.print(cur.getData()+"\t"); cur=cur.getRight(); &#125; &#125;&#125; 后序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public void postorder1(BinaryTreeNode root)&#123; if (root==null) return; postorder1(root.getLeft()); postorder1(root.getRight()); System.out.print(root.getData()+"\t");&#125;public void postorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); while (true)&#123; if(root!=null)&#123; stack.push(root); root=root.getLeft(); &#125; else &#123; if(stack.isEmpty()) return; if(stack.lastElement().getRight()==null)&#123; root=stack.pop(); System.out.print(root.getData()+"\t"); while (stack.lastElement().getRight()==root)&#123; System.out.print(stack.lastElement().getData()+"\t"); root=stack.pop(); if (stack.isEmpty())&#123; break; &#125; &#125; &#125; if(!stack.isEmpty()) root=stack.lastElement().getRight(); else root=null; &#125; &#125;&#125;public void postorder3(BinaryTreeNode root)&#123;//修改前序遍历的方式为：根右左 if(root==null) return; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); BinaryTreeNode cur; cur=root; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); while (cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; res.add(cur.getData()); stack.push(cur); cur=cur.getRight(); &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getLeft(); &#125; &#125; Collections.reverse(res); for (Integer i:res)&#123; System.out.print(i+"\t"); &#125;&#125; 层次遍历 123456789101112131415public void levelorder(BinaryTreeNode root)&#123; BinaryTreeNode temp; Queue&lt;BinaryTreeNode&gt; queue=new LinkedList&lt;BinaryTreeNode&gt;(); queue.offer(root); while (!queue.isEmpty())&#123; temp=queue.poll(); System.out.print(temp.getData()+"\t"); if(temp.getLeft()!=null)&#123;//左 queue.offer(temp.getLeft()); &#125; if(temp.getRight()!=null)&#123;//右 queue.offer(temp.getRight()); &#125; &#125; &#125; 3.循环队列？1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package CommonProblems.ArrayProblems;/** * @Author MaoTian * @Classname CycQueue * @Description 循环队列 * @Date 上午9:46 2019/9/17 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class CycQueue&lt;T&gt; &#123; private int maxsize; private Object[] arr; private int front; private int tail; public CycQueue(int maxsize) &#123; this.maxsize = maxsize; this.arr =new Object[maxsize]; this.front = 0; this.tail = 0; &#125; //判断是否为空 public boolean isEmpty()&#123; if (front==tail)&#123; return true; &#125;else &#123; return false; &#125; &#125; //销毁 public CycQueue destroy()&#123; arr=null; front=tail=0; return this; &#125; //清空 public CycQueue clear()&#123; front=tail=0; for (int i = 0; i &lt;maxsize ; i++) &#123; arr[i]=null; &#125; return this; &#125; //求元素的个数 public int size()&#123; return (tail-front+maxsize)%maxsize; &#125; // public Object head()&#123; return arr[front]; &#125; //入队 public boolean add(Object e)&#123; if((tail+1)%maxsize==front)&#123; return false; &#125; tail=(tail+1)%maxsize; arr[tail]=e; return true; &#125; //出队 public Object pop()&#123; if(front==tail)&#123; return null; &#125; T e=(T)arr[front]; front=(front+1)%maxsize; return e; &#125; public static void main(String[] args) &#123; CycQueue c=new CycQueue(6); c.add(1); c.add(2); c.add(3); c.add(4); c.add(5); c.add(6); System.out.println(c.add(7)); System.out.println(c.pop()); System.out.println(c.pop()); System.out.println(c.add(7)); System.out.println(c.add(8)); for (int i = 0; i &lt;6 ; i++) &#123; System.out.print(c.arr[i]+" "); &#125; &#125;&#125; 4.Trie Tree（208. Implement Trie (Prefix Tree)） Trie，又经常叫前缀树，字典树等等。它有很多变种，如后缀树，Radix Tree/Trie，PATRICIA tree，以及bitwise版本的crit-bit tree。当然很多名字的意义其实有交叉。 应用 字符串检索 文本预测、拼写检查 词频统计 排序 字符串最长公共前缀 字符串搜索的前缀匹配 作为其他数据结构和算法的辅助结构 5.树，图节点以及度数相关？ 二叉树中n个节点，0度、 1度、 2度的关系？ 度为2节点数 = 叶子节点数 - 1 证明： 树支路总数 = 树节点总数 - 1 树支路总数=0x0 + 1x1 + 2*x2 树节点总数=x0 + x1 + x2 - 1 得到：度为0与度为2的节点数的关系x2 = x0 - 1 无向图度数和边数的关系：度数等于二倍的边数。 四、算法1.算法的分类？2.弗洛伊德算法？（三层for循环，通过第三个点不断更新两个点之间的距离）https://www.cnblogs.com/lc-java/p/7840464.html 3.迪杰斯特拉算法？https://www.cnblogs.com/he-px/p/6677063.html https://www.cnblogs.com/zengzhihua/p/6755439.html 4.二分法？（边界问题）1234567891011121314151617public int binarySearch(int[] nums,int target)&#123; int start=0; //减1 int end=nums.length-1; //等号 while (start&lt;=end)&#123; int mid=(start+end)/2; if (nums[mid]==target)&#123; return mid; &#125;else if(nums[mid]&lt;target)&#123; start=mid+1; &#125;else&#123; end=mid-1; &#125; &#125; return -1;&#125; 5.排序算法（1）堆排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package ALiBaBa;/** * @Author MaoTian * @Classname HeapSort * @Description * 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] * 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] * @Date 下午11:35 2019/8/17 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */import java.util.Arrays;public class HeapSort &#123; public static void main(String []args)&#123; int []arr = &#123;3,1,4,2,8,5,9,7,6&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static void sort(int []arr)&#123; //1.构建大顶堆（头部就是0） for(int i=arr.length/2-1;i&gt;=0;i--)&#123; //从第一个非叶子结点从下至上，从右至左调整结构（不包含length） adjustHeap(arr,i,arr.length); &#125; //2.调整堆结构+交换堆顶元素与末尾元素（取值length-1次） for(int j=arr.length-1;j&gt;0;j--)&#123; swap(arr,0,j);//将堆顶元素与末尾元素进行交换 adjustHeap(arr,0,j);//重新对堆进行调整（不包含j） &#125; &#125; /** * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上） */ public static void adjustHeap(int []arr,int i,int length)&#123; int temp = arr[i];//先取出当前元素i for(int k=i*2+1;k&lt;length;k=k*2+1)&#123;//从i结点的左子结点开始，也就是2i+1处开始 if(k+1&lt;length &amp;&amp; arr[k]&lt;arr[k+1])&#123;//如果左子结点小于右子结点，k指向右子结点 k++; &#125; if(arr[k] &gt;temp)&#123;//如果子节点大于父节点，将子节点值赋给父节点（不用进行交换） arr[i] = arr[k]; i = k; &#125;else&#123; break; &#125; &#125; arr[i] = temp;//将temp值放到最终的位置 &#125; public static void swap(int []arr,int a ,int b)&#123; int temp=arr[a]; arr[a] = arr[b]; arr[b] = temp; &#125;&#125; （2）归并排序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package ALiBaBa;import org.junit.Test;/** * @Author MaoTian * @Classname MergeSort * @Description TODO * @Date 上午11:40 2019/8/17 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class MergeSort &#123; public void mergeSort(int[] nums,int left,int right)&#123; if(left&lt;right)&#123; int mid=(left+right)&gt;&gt;1; mergeSort(nums,left,mid); mergeSort(nums,mid+1,right); merge(nums,left,right,mid); &#125; &#125; public void merge(int[] nums,int left,int right,int mid)&#123; int leftPos=left; int pos=left; int rightPos=mid+1; int len=right-left+1; int[] tmp=new int[nums.length]; while(leftPos&lt;=mid&amp;&amp;rightPos&lt;=right)&#123; if(nums[leftPos]&gt;nums[rightPos])&#123; tmp[pos++]=nums[rightPos++]; &#125;else&#123; tmp[pos++]=nums[leftPos++]; &#125; &#125; while(leftPos&lt;=mid)&#123; tmp[pos++]=nums[leftPos++]; &#125; while(rightPos&lt;=right)&#123; tmp[pos++]=nums[rightPos++]; &#125; int m=left; int n=left; for(int i=0;i&lt;len;i++)&#123; nums[m++]=tmp[n++]; &#125; &#125; @Test public void check()&#123; int[] nums=&#123;2,4,2,5,73,2&#125;; mergeSort(nums,0,nums.length-1); for(int i=0;i&lt;nums.length;i++)&#123; System.out.println(nums[i]); &#125; &#125;&#125; （3）快速排序12345678910111213141516171819202122232425262728293031323334353637383940414243444546package ALiBaBa;import org.junit.Test;/** * @Author MaoTian * @Classname QuickSort * @Description TODO * @Date 上午11:39 2019/8/17 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class QuickSort &#123; public void quickSort(int[] nums,int left,int right)&#123; if(left&lt;right)&#123; int mid=partition(nums,left,right); quickSort(nums,left,mid-1); quickSort(nums,mid+1,right); &#125; &#125; public int partition(int[] nums,int left,int right)&#123; int pos=left; int value=nums[pos]; for (int i=left;i&lt;=right;i++)&#123; if(nums[i]&lt;value)&#123; pos++; if(pos!=i)&#123; int tmp=nums[pos]; nums[pos]=nums[i]; nums[i]=tmp; &#125; &#125; &#125; nums[left]=nums[pos]; nums[pos]=value; return pos; &#125; @Test public void check()&#123; int[] nums=&#123;2,4,2,6,7,3,2,6,8&#125;; quickSort(nums,0,nums.length-1); for (int i=0;i&lt;nums.length;i++)&#123; System.out.println(nums[i]); &#125; &#125;&#125; 6.LRU算法？ 双向链表加HashMap的实现，重要需要完成的功能点为： 将当前访问的节点放到队列的头部 当容量不够的时候，将队列尾部的元素移走，并且在HashMap中消除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package JavaDemo.AlgorithmDemo;import java.util.HashMap;/** * @Author MaoTian * @Classname LRUCache * @Description 使用链表保证相对顺序，使用栓链表，两个指针可以使得两种删除操作更加容易，使用HashMap是为了快速查找！ * @Date 上午9:16 2019/8/23 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class LRUNode &#123; String key; Object value; LRUNode prev; LRUNode next; public LRUNode(String key, Object value) &#123; this.key = key; this.value = value; &#125;&#125;public class LRUCache &#123; private HashMap&lt;String, LRUNode&gt; map; private int capacity; private LRUNode head;//记录尾部 private LRUNode tail;//记录尾部 public void set(String key, Object value) &#123; LRUNode node = map.get(key); if (node != null) &#123; //原来的节点 node = map.get(key); node.value = value; remove(node, false); &#125; else &#123; //新节点 node = new LRUNode(key, value); if (map.size() &gt;= capacity) &#123; // 每次容量不足时先删除最久未使用的元素 remove(tail, true); &#125; map.put(key, node); &#125; // 将刚添加的元素设置为head setHead(node); &#125; public Object get(String key) &#123; LRUNode node = map.get(key); if (node != null) &#123; // 将刚操作的元素放到head，将node从原来的位置删除（这里的删除其实就是将这个node移动到队列的头部） remove(node, false); setHead(node); return node.value; &#125; return null; &#125; private void setHead(LRUNode node) &#123; // 建立双向指针 if (head != null) &#123; node.next = head; head.prev = node; &#125; head = node; //第一个加入的node设置为尾节点 if (tail == null) &#123; tail = node; &#125; &#125; // 从链表中删除此Node，此时要注意该Node是head或者是tail的情形 private void remove(LRUNode node, boolean flag) &#123; //双向指针 if (node.prev != null) &#123; node.prev.next = node.next; &#125; else &#123; //删除的节点是head head = node.next; &#125; if (node.next != null) &#123; node.next.prev = node.prev; &#125; else &#123; //删除的节点是tail，需要重新设置tail为当前节点的前一个节点 tail = node.prev; &#125; //删除 node.next = null; node.prev = null; //容量不够的时候从map中删除 if (flag) &#123; map.remove(node.key); &#125; &#125; //构造函数 public LRUCache(int capacity) &#123; this.capacity = capacity; this.map = new HashMap&lt;String, LRUNode&gt;(); &#125;&#125; LRU的缓存，需要完成超时淘汰和LRU淘汰？,对上面的代码进行修改，每一次get或者set操作将不是当前的LRUNode的age加上一，当前的置为0，紧接着对于HashMap中的所有LRUNode的年龄进行判断，当年龄不够的时候从链表和HashMap中移走。 基于LinkedHashMap的实现 12345678910111213141516171819202122232425262728293031323334import java.util.HashMap;import java.util.LinkedHashMap;import java.util.Map;public class Main &#123; static class LRULinkedHashMap&lt;K,V&gt; extends LinkedHashMap&lt;K,V&gt; &#123; //定义缓存的容量 private int capacity; //带参数的构造器 LRULinkedHashMap(int capacity)&#123; //如果accessOrder为true的话，则会把访问过的元素放在链表后面，放置顺序是访问的顺序(最近访问的将会被放到队列尾部，即后删除) //如果accessOrder为flase的话，则按插入顺序来遍历 super(16,0.75f,true);//父类的构造器 //传入指定的缓存最大容量 this.capacity=capacity; &#125; //实现LRU的关键方法，如果map里面的元素个数大于了缓存最大容量，则删除链表的顶端元素 @Override public boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest)&#123; return size()&gt;capacity; &#125; &#125; //test public static void main(String[] args) &#123; LRULinkedHashMap&lt;String, Integer&gt; testCache = new LRULinkedHashMap&lt;&gt;(3); testCache.put("A", 1); testCache.put("B", 2); testCache.put("C", 3); System.out.println(testCache.get("B")); System.out.println(testCache.get("A")); testCache.put("D", 4); System.out.println(testCache.get("D")); System.out.println(testCache.get("C")); &#125;&#125; 7.洗牌算法？8.朋友圈LeetCode547?9.卡特兰数 字节跳动客户端2019笔试题目，圆圈，点，道路 阿里巴巴括号匹配的种类 10.排序算法https://www.runoob.com/w3cnote/sort-algorithm-summary.html 八大排序算法真的是面试宠儿 最常考 快速排序 和归并排序 不稳定(快些选堆) 时间复杂度（快些归堆O(nlogn)） 堆排 也应该掌握 11. 树 根据遍历结果恢复树，递归 二叉搜索树第k大 树的和为k的路径 树的最大路径和 层次遍历 根据层次遍历和后序遍历恢复树 镜像树 树的深度 是不是平衡二叉树 红黑树中序遍历是有序的。 红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求： 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。） 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。(黑色节点完美平衡) 下面是一个具体的红黑树的图例： 操作 着色 左旋转：你当我儿子，我的左二子当你的右儿子； 1234567891011121314151617181920212223242526272829303132333435/*** p p* | |* x y* / \ ------&gt; / \* lx y x ry* / \ / \* ly ry lx ly*/private void leftRotate(RBNode x) &#123; RBNode y = x.right; // 我的左儿子变为你的右儿子 x.right = y.left; // 注意parent不能够丢下！！！ if(y.left != null) &#123; y.left.parent = x; &#125; // 我变成爸爸啦 if(x.parent != null) &#123; y.parent = x.parent; if(x == x.panrent.left) &#123; x.parent.left = y; &#125; else &#123; x.parent.right = y; &#125; &#125; else &#123; this.root = y; &#125; // 你变成我儿子 x.parent = y; y.left = x;&#125; 右旋转：你当我儿子，我的右儿子当你的左儿子。 hashmap 1.说说你对hash算法的理解 追问：hash算法任意长度的输入 转化为了 固定长度的输出，会不会有问题呢？ 追问：hash冲突能避免么？2.你认为好的hash算法，应该考虑点有哪些呢？3.HashMap中存储数据的结构是什么样的呢？4.创建HashMap时，不指定散列表数组长度，初始长度是多少呢？ 追问：散列表是new HashMap() 时创建的么？5.默认负载因子是多少呢，并且这个负载因子有什么作用？6.链表转化为红黑树，需要达到什么条件呢？7.Node对象内部的hash字段，这个hash值是key对象的hashcode()返回值么？ 追问：这个hash值是怎么得到呢？ 追问：hash字段为什么采用高低位异或？8.HashMap put 写数据的具体流程，尽可能的详细点！9.红黑树的写入操作，是怎么找到父节点的，找父节点流程？10.TreeNode数据结构，简单说下。11.红黑树的原则有哪些呢？12.JDK8 hashmap为什么引入红黑树？解决什么问题？ 追问：为什么hash冲突后性能变低了？【送分题】13.hashmap 什么情况下会触发扩容呢？ 追问：触发扩容后，会扩容多大呢？算法是什么？ 追问：为什么采用位移运算，不是直接*2？14.hashmap扩容后，老表的数据怎么迁移到扩容后的表的呢？15.hashmap扩容后，迁移数据发现该slot是颗红黑树，怎么处理呢？ 12.链表 反转链表 链表环的入口 交叉链表的交点 复杂链表的复制 二叉搜索树变成双向链表 13.回溯算法 走迷宫 游戏通关 14.递推算法 走台阶 断钢筋 15. 背包问题 装最多的东西 16.贪心算法 覆盖问题 时间问题 17.编辑距离 对一个八位数有三种操作： 加一、减一、反转 。 至少多少次操作可以把一个八位数A变成八位数B。？ leetcode 72 123456789101112131415161718192021222324252627282930313233343536373839package CommonProblems.Dynamic;/** * @Author MaoTian * @Classname EditDistance * @Description TODO * @Date 下午3:40 2019/9/17 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class EditDistance &#123; public int helper(String s1,String s2)&#123; int m=s1.length(); int n=s2.length(); //dp[i][j]表示长度为i的s1的子串转换为长度为j的s2的子串的操作次数 int[][] dp=new int[m+1][n+1]; for (int i = 0; i &lt;=m ; i++) &#123; dp[i][0]=i;//删除 &#125; for (int i = 0; i &lt;=n ; i++) &#123; dp[0][i]=i;//插入 &#125; for(int i=1;i&lt;m+1;i++)&#123; for(int j=1;j&lt;n+1;j++)&#123; int replaces=0; if(s1.charAt(i-1)==s2.charAt(j-1))&#123; replaces=dp[i-1][j-1]; &#125;else &#123; replaces=dp[i-1][j-1]+1; &#125; dp[i][j]=Math.min(replaces,//替换 Math.min(dp[i-1][j],//插入 dp[i][j-1]+1)//删除 ); &#125; &#125; return dp[m][n]; &#125;&#125; 18.dfs 一个有向图用邻接矩阵表示，并且是有权图，现在问怎么判断图中有没有环。使用一个全局的一维数组保存访问的状态，对于每一个节点i，递归遍历当前节点指向的节点，当重新访问到当前的节点的时候，就是存在环路，直接返回true。 19.两个str 最大公共子序列和子串？12345678910111213141516171819202122232425262728 public int getMaxCommon(String s1,String s2)&#123; char[] arr1=s1.toCharArray(); int m=arr1.length+1; char[] arr2=s2.toCharArray(); int n=arr2.length+1; int[][] dp=new int[m][n]; for(int i=1;i&lt;m;i++)&#123; for(int j=1;j&lt;n;j++)&#123; if(arr1[i-1]==arr2[j-1])&#123; dp[i][j]=dp[i-1][j-1]+1; &#125;else&#123; dp[i][j]=Math.max(dp[i-1][j],dp[i][j-1]);//最长公共子序列// dp[i][j]=0;//最长公共子串 &#125; &#125; &#125; int res=0; for(int i=0;i&lt;m;i++)&#123; for (int j=0;j&lt;n;j++)&#123; if(res&lt;dp[i][j])&#123; res=dp[i][j]; &#125; &#125; &#125; return res; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java总结-4]]></title>
    <url>%2F2019%2F11%2F20%2FJava%E6%80%BB%E7%BB%93-4%2F</url>
    <content type="text"><![CDATA[Java总结-4 八、Redis高并发0.Redis线程模型，为什么快？Redis是基于Reactor模式开发了网络事件处理器，这个事件处理器叫作file event handler，这个事件是单线程的，因此说Redis是单线程模型。这个事件处理器使用IO多路复用机制监听多个socket，根据每一个socket上的事件类型选择相应的事件处理器处理这个事件。这样可以实现高性能的网络通信模型，又可以跟内部其他的单线程模块进行对接，保证了Redis内部线程模型的简单性。 graph TB st(( ))-.->a(文件事件处理器) a--监听-->b(多个socket) b-.有.->c(事件) c--根据事件类型-->d(选择相应的事件处理器) d--处理-->c d-.->e(命令处理器) d-.->f(命令回复处理器) d-.->g(连接应答处理器) 文件事件处理器结构由四个部分组成： 多个socket IO多路复用程序 文件事件分派器 事件处理器（命令处理器、命令回复处理器、连接应答处理器） 为什么快？ 完全基于内存操作 核心是基于非阻塞的IO多路复用 单线程避免了上下文切换带来的性能损耗 Ps: netty也是基于Reactor模式实现的！ 1.Redis的高并发和快速原因？（1）Redis是基于内存的，内存的读写速度非常快； （2）Redis是单线程的，省去了很多上下文切换线程的时间； （3）Redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。 2.为什么Redis是单线程的？（1）官方答案 因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 （2）性能指标 关于redis的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。 （3）详细原因 不需要各种锁的性能消耗 Redis的数据结构并不全是简单的Key-Value，还有list，hash等复杂的结构，这些结构有可能会进行很细粒度的操作，比如在很长的列表后面添加一个元素，在hash当中添加或者删除一个对象。这些操作可能就需要加非常多的锁，导致的结果是同步开销大大增加。总之，在单线程的情况下，就不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。 单线程多进程集群方案 单线程的威力实际上非常强大，每核心效率也非常高，多线程自然是可以比单线程有更高的性能上限，但是在今天的计算环境中，即使是单机多线程的上限也往往不能满足需要了，需要进一步摸索的是多服务器集群化的方案，这些方案中多线程的技术照样是用不上的。所以单线程、多进程的集群不失为一个时髦的解决方案。 CPU消耗 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU。 3.Redis支持的数据结构？(1)String：String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 (2)List：list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。 (3)hash：hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 (4)Set：set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 (5)ZSet：和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 4.redis 提供 6种数据淘汰策略？ 编号 名称 解释 1 volatile-lru 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 2 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 3 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 4 allkeys-lru 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） 5 allkeys-random 从数据集（server.db[i].dict）中任意选择数据淘汰 6 no-eviction 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 7 volatile-lfu 从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 8 allkeys-lfu 当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key Redis的过期键的删除策略？Redis使用了这两种策略。 惰性过期 定期过期：每个一段时间，会扫描一定数量的数据库的expires字典中的key，删除其中已经过期的key。 5.持久化机制？ Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 6.Redis事务？（REmote DIctionary Server(Redis)）Redis 通过 MULTI、EXEC、WATCH 、UNWATCH等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity）、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 7.epoll相关的原理？8.redis 和 memcached 的区别？ 编号 Redis memcached 1（数据类型） Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 memcache支持简单的数据类型，String。 2（持久化） Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用 Memecache把数据全部存在内存之中。 3（多线程） 是多线程，非阻塞IO复用的网络模型 Redis使用单线程的多路 IO 复用模型。 4（集群） redis 目前是原生支持 cluster 模式的. memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据； 9.Redis的rehash？ Redis的rehash采用的是渐进式rehash。 rehash要和数据分片区分开！！！ 10.redis高可用？ https://www.cnblogs.com/twinhead/p/9900659.html 11.redis相关的命令？ http://redisdoc.com/ 12.redis集群怎么进行数据分配，hash槽？13.aof和rdb的优缺点，你在项目中使用的哪一个？（默认的配置是什么？） RDB：Redis Database：fork一个子进程，将数据集写入到临时文件，写入成功后，替换原来的文件，并且使用了压缩存储。 AOF：Append only File 优点 缺点 RDB （1）整个Redis只包含一个dump.rdb文件，方便持久化。（2）容灾性好，方便备份。（3）性能最大化，fork进程完成写操作，主进程批量处理命令，所以是IO最大化。使用单独的子进程进行持久化，主进程不会有任何IO操作，保证了Redis的性能。（4）相对于数据集大的时候，比AOF效率更高。 （1）数据安全性低。RDB是间隔一段时间进行持久化的，如果在持久化之前发生故障，会导致数据丢失。（2）由于RDB是通过fork子进程来协助完成数据持久化工作的，如果数据集大，可能导致整个服务暂停服务几百毫秒，甚至更长。 AOF （1）数据安全，存在三种同步模式，每秒同步、每次修改同步、不同步。（2）通过append模式写文件，即使中途服务器宕机也不会破坏已经存在的文件内容，可以通过redis-check-aof工具解决一致性问题。（3）AOF的rewrite模式，定期对AOF文件进行重写，达到压缩的目的。 （1）AOF文件大。（2）数据集大的时候，比rdb慢。 14. Redis资料 https://redislabs.com/ebook/part-2-core-concepts/chapter-3-commands-in-redis/3-5-sorted-sets/ 15.Rehash16.Jedis 使用zset实现积分榜 跳表实现 17.缓存雪崩、缓存击穿、缓存穿透 缓存雪崩 缓存击穿 缓存穿透（一般来自攻击） 现象 同一个事件大量的key失效，请求落在数据库上，导致数据库短时间承担了大量的请求而崩溃。（查询多条数据） 缓存中没有的数据数据库中有的数据，由于并发用户特别多，同时没有读到数据，有同时去数据库读取数据，和缓存雪崩不同，缓存击穿指的是查询同一条数据。 缓存和数据库中都没有数据，导致请求落在了数据库上。 原因 - 同一时间key大面积过期- 系统刚启动的时候缓存没有数据 解决方案 - 将过期键设置为随机- 缓存预热- 给缓存添加标记位，记录缓存是否失效- 互斥锁 - 设置热键永远不过期- 互斥加锁 - 接口层校验，比如用户鉴权，基础校验拦截- 从缓存中取不到、数据库中也取不到，使用key-null，并且设置过期时间。- 使用布隆过滤器，将所有可能出现的数据hash到同一个bitmap中，一定不存在的数据会被这个bitmap拦截。 18. 集群机制18.1 哨兵模式18.2 cluster模式18.3 Redis sharding（客户端分片）redis集群可以被分为16384个槽，只有这些槽全被指派了处理的节点的情况下，集群的状态才能是上线状态(ok) 操作redis集群的时候，将key作为参数，就可以计算出对应的处理槽上，所以存储等操作都应该在该槽对应的节点上。 重要的数据结构包括： ClusterNode：slots，numslots ClusterState：slots 18.4 集群故障转移（1）发现故障节点 集群内的节点会向其他节点发送PING命令，检查是否在线 如果未能在规定时间内做出PONG响应，则会把对应的节点标记为疑似下线 集群中一半以上负责处理槽的主节点都将主节点X标记为疑似下线的话，那么这个主节点X就会被认为是已下线 向集群广播主节点X已下线,大家收到消息后都会把自己维护的结构体里的主节点X标记为已下线 （2）从节点选举 当从节点发现自己复制的主节点已下线了，会向集群里面广播一条消息，要求所有有投票权的节点给自己投票(所有负责处理槽的主节点都有投票权) 主节点会向第一个给他发选举消息的从节点回复支持 当支持数量超过N/2+1的情况下，该从节点当选新的主节点 （3）故障的转移 新当选的从节点执行 SLAVEOF no one,修改成主节点 新的主节点会撤销所有已下线的老的主节点的槽指派，指派给自己 新的主节点向集群发送命令，通知其他节点自己已经变成主节点了，负责哪些槽指派 新的主节点开始处理自己负责的槽的命令 18.5 哨兵模式故障转移（1）主观下线 哨兵(Sentinel)节点会每秒一次的频率向建立了命令连接的实例发送PING命令，如果在down-after-milliseconds毫秒内没有做出有效响应包括(PONG/LOADING/MASTERDOWN)以外的响应，哨兵就会将该实例在本结构体中的状态标记为SRI_S_DOWN主观下线。 （2）客观下线 当一个哨兵节点发现主节点处于主观下线状态是，会向其他的哨兵节点发出询问，该节点是不是已经主观下线了。如果超过配置参数quorum个节点认为是主观下线时，该哨兵节点就会将自己维护的结构体中该主节点标记为SRI_O_DOWN客观下线 询问命令SENTINEL is-master-down-by-addr （3）leader选举 新当选的从节点执行 SLAVEOF no one,修改成主节点 （4）故障迁移 在从节点中挑选出新的主节点（通讯正常、优先级排序、优先级相同是选择offset最大的） 将该节点设置成新的主节点 SLAVEOF no one,并确保在后续的INFO命令时，该节点返回状态为master 将其他的从节点设置成从新的主节点复制, SLAVEOF命令 将旧的主节点变成新的主节点的从节点 九、Kafka1.kafkahttps://www.cnblogs.com/qingyunzong/p/9004509.html （0）概念kafka中有不同的broker，保存数据的是topic，一个topic被分为多个partition。一个partition又被分为多个segment，一个segment又被分为index和log两部分构成。 partion：负载均衡使用 topic中有partition，partition有副本，客服端读写都是找leader，不会找follower 消费者组：同组的不能够消费同一个分区，可以消费不同的分区，不同的组的可以消费同一个分区。 zookeeper：consumer和broker和zookeeper打交道，producer不会。 （1）broker.id每一个broker，唯一的int类型数据 （2）delete.topic.enable （3）logs.dirs存储数据的位置 （4）zookeeper的集群 （4）配置zookeeper （5）时间 7天，168小时 （6）大小 1G kafka-server-start 生产数据的流程 kafka-topics —create —zookeeper hadoop：2181 —partition 2 —replication-factor —topic first 副本数目不能够超过broker的数目。 kafka-console-consumer kafka-console-producer 启动的进程的时候要记录进程名和id，jps -l 新版本：offset维护在本地，需要和leader通信，这样就会提高效率，—bootsrap-server。 面试的时候介绍： （1）数据的流程， （2）生产过程分析： 每一个消息都会append到分区（partition），属于顺序写磁盘（保证吞吐率） 写：分区内有序，每一个消息都赋予了一个offset 分区原则（三种分区规则）：指定partition，直接使用；指定了key，通过key的value进行hash；都没指定就采用轮询。 写入流程：1）producer首先从broker-list中获取partition的leader；2）producer将消息发送给leader；3）leader将消息写入到本地的log。4）follower从leader pull消息；5）写入本地的log后向leader发送ack。 ACK应答（0（不管leader）,1（不管follower）,2（多完成））follower（如何producer不丢失数据—-ACK设置为2） （3）存储 1）broker 2）zookeeper broker————&gt;[ids,topics,seqid], topics————-&gt;partitions——-&gt;state consumer———-&gt;offset （1）request.required.acks来设置数据的可靠性： 值 含义 可靠性 0 发送即可，不管是否成功 会丢失数据 1 Leader写成功即可 主备切换的时候可能丢失数据 -1 ISR中所有的机器写成功才算成功 不会丢失数据 （2）Kafka的用途有哪些？使用场景如何？（3）Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。 ISR：排好序，最接近的顺序排序，用于leader挂了选举用的。 分区中的所有副本统称为AR（Assigned Repllicas）。所有与leader副本保持一定程度同步的副本（包括Leader）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。前面所说的“一定程度”是指可以忍受的滞后范围，这个范围可以通过参数进行配置。与leader副本同步滞后过多的副本（不包括leader）副本，组成OSR(Out-Sync Relipcas),由此可见：AR=ISR+OSR。在正常情况下，所有的follower副本都应该与leader副本保持一定程度的同步，即AR=ISR,OSR集合为空。 （4）Kafka中的HW、LEO、LSO、LW等分别代表什么？（5）Kafka中是怎么体现消息顺序性的？ kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。整个topic不保证有序 （6）Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？（7）Kafka生产者客户端的整体结构是什么样子的？（8）Kafka生产者客户端中使用了几个线程来处理？分别是什么？（9）Kafka的旧版Scala的消费者客户端的设计有什么缺陷？（10）“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？（11）消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?offset+1 （12）有哪些情形会造成重复消费？ 消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible) （13）那些情景下会造成消息漏消费？ 消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束) （14）KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？（15）简述消费者与消费组之间的关系（16）当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？ 创建topic后的逻辑： 删除topic后的逻辑： （17）topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？分区可以增加 （18）topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 分区不可以减少，会丢失数据.ps：topic是可以删除的。 （19）创建topic时如何选择合适的分区数？（20）Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？ __consumer_offsets 以双下划线开头，保存消费组的偏移 （21）优先副本是什么？它有什么特殊的作用？（22）Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理（23）简述Kafka的日志目录结构partition相当于一个大文件，平均分成了多个segment数据文件，每一个segment由两个文件构成×××.index （索引）和×××.log（数据）两部分组成。 （24）Kafka中有那些索引文件？（25）如果我指定了一个offset，Kafka怎么查找到对应的消息？（26）如果我指定了一个timestamp，Kafka怎么查找到对应的消息？（27）聊一聊你对Kafka的Log Retention的理解（28）聊一聊你对Kafka的Log Compaction的理解（29）聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）（30）聊一聊Kafka的延时操作的原理（31）聊一聊Kafka控制器的作用（32）消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）（33）Kafka中的幂等是怎么实现的（34）Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）（35）Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？（36）失效副本是指什么？有那些应对措施？（37）多副本下，各个副本中的HW和LEO的演变过程（38）为什么Kafka不支持读写分离？（39）Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）（40）Kafka中怎么实现死信队列和重试队列？（41）Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）（42）Kafka中怎么做消息审计？（43）Kafka中怎么做消息轨迹？（44）Kafka中有那些配置参数比较有意思？聊一聊你的看法（45）Kafka中有那些命名比较有意思？聊一聊你的看法（46）Kafka有哪些指标需要着重关注？（47）怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)（48）Kafka的那些设计让它有如此高的性能？（49）Kafka有什么优缺点？（50）还用过什么同质类的其它产品，与Kafka相比有什么优缺点？为什么选择Kafka?（51）在使用Kafka的过程中遇到过什么困难？怎么解决的？（52）怎么样才能确保Kafka极大程度上的可靠性？十、MySQL0. MySQL结构graph TB A1(客户端)-.->B1(连接器) B1-.->C1(优化器) C1-.->D1(执行器) D1-.->E1(存储引擎) E1-.->F1(InnoDB) 1.数据库引擎？（InnoDB和MyISAM区别） 编号 区别 InnoDB MyISAM 1 锁 InnoDB 支持行级锁(row-level locking)和表级锁 MyISAM 只有表级锁(table-level locking) 2 索引 （B+树，存具体数据，聚簇索引）其数据文件本身就是索引文件。相比MyISAM，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。 （B+树，存地址，非聚簇索引）B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 3 事务 InnoDB 提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 4 外键 InnoDB支持 MyISAM不支持 5 崩溃恢复 InnoDB支持 MyISAM不支持 6 MVCC 应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现 MyISAM不支持 特变注意，两种数据库引擎对应的文件是不同的 InnoDB：由数据库的结构文件和（数据+索引）两部分构成：test_innodb_lock.frm,test_innodb_lock.idb，叶子节点存储看数据，索引文件和数据文件合并了。聚集索引：数据和索引聚集在一起了，主键索引和其他索引之间的区别：其他索引存储的是主键，主键存的是data。为什么推荐整型数据自增？主要是在分裂的时候减少平衡操作。 MyISAM：由数据库的结构文件，数据，索引三部分构成：test_myisam.frm,test_myisam.MYD,test_myisam.MYI，非聚集索引数据文件和索引文件是分开的。 2.sql语句（1）建立一张表格？1234567891011121314CREATE TABLE `user` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '用户Id', `email` varchar(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '邮箱', `username` varchar(32) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '用户名', `password` varchar(1024) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '密码', `avatar` varchar(1024) DEFAULT 'https://www.tupianku.com/view/large/13862/640.jpeg' COMMENT '头像', `resume` varchar(512) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT '简介', `register_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '注册时间', `login_time` datetime DEFAULT NULL COMMENT '上一次登录时间', PRIMARY KEY (`id`), UNIQUE KEY `ix_user_username` (`username`), UNIQUE KEY `ix_user_email` (`email`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COMMENT='用户表';/*!40101 SET character_set_client = @saved_cs_client */; （2）分页limit？1SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id （3）成绩排名第三？12select * from employeeswhere hire_date=(select distinct hire_date from employees order by hire_date Desc limit 2,1); DML———数据库操作语言 DDL———数据库定义语言 3.什么是事务?事务是逻辑上的一组操作，要么都执行，要么都不执行。 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 4.并发事务带来哪些问题? 编号 名称 解释 1 脏读(读了还没有提交的数据) 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。（写读） 2 数据丢失 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。（写写） 3 不可重复读（已提交事务更改数据） 在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。（同一个事务中多次读）(read_view在read committed隔离级别下，read_view会在别的事务commit后更新，因此在这个事务中可能会不一样) 4 幻读（针对其他事务已提交新增加的数据） 它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录（同一个事务中多次读）(read_view在read repeatable 隔离级别下，read_view是在事务开始的时候建立的，当前的事务没有结束不会更新。) 5.事务隔离级别？ 编号 名称 解释 1 read uncommitted（读取未提交） 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 2 read committed（读取已提交） 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 3 repeatable read（可重复读） 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。（默认的级别） 4 serializable（可串行化） 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行 6.MySQL InnoDB的锁？ 编号 名称 解释 1 Record lock（行锁） 单个行记录上的锁 2 Gap lock（间隙锁） 间隙锁，锁定一个范围，不包括记录本身 3 Next-key lock record+gap 锁定一个范围，包含记录本身 7.大表优化？（1）限定数据的范围：务必禁止不带任何限制数据范围条件的查询语句。 （2）读写分离：经典的数据库拆分方案，主库负责写，从库负责读； （3）垂直分表 （4）水平分表 8.一条sql执行的过程？(基本结构+执行+日志) （1）MySQL的基本架构 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器: 执行语句，然后从存储引擎返回数据。 简单来说 MySQL 主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块binglog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。 两个日志模块 redo log和bin log 更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下： 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。 9.一条sql很慢的原因？ 参考资料 一个 SQL 执行的很慢，我们要分两种情况讨论： （1）大多数情况下很正常，偶尔很慢，则有如下原因 数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。 执行的时候，遇到锁，如表锁、行锁。 （2）这条 SQL 语句一直执行的很慢，则有如下原因。 没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。 数据库选错了索引。 10.MySQL join操作？https://www.cnblogs.com/reaptomorrow-flydream/p/8145610.html 最常见的 JOIN 类型：SQL INNER JOIN（简单的 JOIN）、SQL LEFT JOIN、SQL RIGHT JOIN、SQL FULL JOIN 表A id name 1 Google 2 淘宝 3 微博 4 Facebook 表B id address 1 美国 5 中国 3 中国 6 美国 （1）Inner join12select * from Table A inner join Table Bon Table A.id=Table B.id 执行以上SQL输出结果如下： id name address 1 Google 美国 3 微博 中国 （2）left join1234select column_name(s)from table 1LEFT JOIN table 2ON table 1.column_name=table 2.column_name id name address 1 Google 美国 2 淘宝 null 3 微博 中国 4 Facebook null （3）right join1234select column_name(s)from table 1RIGHT JOIN table 2ON table 1.column_name=table 2.column_name id name address 1 Google 美国 5 null 中国 3 微博 中国 6 null 美国 （4）outer join1234select column_name(s)from table 1FULL OUTER JOIN table 2ON table 1.column_name=table 2.column_name id name address 1 Google 美国 2 淘宝 null 3 微博 中国 4 Facebook null 5 null 中国 6 null 美国 11.MySQL索引？（0）底层的数据结构B+树，主要是考虑二叉树的深度，主要考虑的是IO特别耗时间。一个节点会存储多个索引。 主键索引名为pk_字段名； 唯一索引名为 uk字段名； 普通索引名则为 idx_字段名。 （1）索引的类别？ 编号 名称 解释 1 普通索引 是最基本的索引，它没有任何限制. 2 唯一索引 与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一. 3 主键索引 是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。 4 组合索引 指多个字段上创建的索引，只有在查询条件中使用了创建一个字段，索引才会被使用。使用组合索引时遵循最左前缀。 5 全文索引 主要用来查找文本中的关键字，而不是直接与索引中的值相比较.fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的，其中语句的参数匹配.fulltext索引配合匹配操作使用，而不是一般的where语句加像。 （2）Hash索引和B+树所有有什么区别或者说优劣呢?（3）InnoDB为什么需要主键？（4）B树，B-树，B+树？ B树：B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树（查找路径不只两个），数据库索引技术里大量使用者B树和B+树的数据结构。 B-树：就是B树 B+树：在B树基础上，最大的改动是B+树叶子节点存放了关键字，添加了索引。 12.阿里巴巴技术手册SQL？ 【强制】表必备三字段：id, gmt_create, gmt_modified。 说明： 其中 id 必为主键，类型为 bigint unsigned、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为datetime 类型，前者现在时表示主动创建，后者过去分词表示被动更新。 【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 13.sql的explain？14.超大分页处理？（1）分页limit 【推荐】利用延迟关联或者子查询优化超多分页场景。(阿里巴巴技术手册) 说明： MySQL 并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行 SQL 改写。 正例： 先快速定位需要获取的 id 段，然后再关联： SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据 （2）在Spring boot中的实践？15.触发器，视图？16.什么是聚集索引？17.数据库如何建立索引？18. MVCC 版本链：trx_id read_view undo日志 记录版本链 record header里的deleted flag设置为true begin/start transaction 不同的session查询到的数据可能不一致。read_view的时机生成的时机是session在执行第一条select语句， 19. Q&amp;A？19.1 简述数据库三大范式第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 数据库第二范式：关系模式必须满足第一范式，并且所有非主属性都完全依赖于主码。注意，符合第二范式的关系模型可能还存在数据冗余、更新异常等问题。关系模型（学号，姓名，专业编号，专业名称）中，学号-&gt;姓名，而专业编号-&gt;专业名称，不满足数据库第二范式 数据库第三范式：关系模型满足第二范式，所有非主属性对任何候选关键字都不存在传递依赖。即每个属性都跟主键有直接关系而不是间接关系。接着以学生表举例，对于关系模型（学号，姓名，年龄，性别，所在院校，院校地址，院校电话）院校地址，院校电话和学号不存在直接关系，因此不满足第三范式。 19.2 简述MySQL的架构MySQL可以分为应用层,逻辑层,数据库引擎层,物理层。 应用层：负责和客户端，响应客户端请求，建立连接，返回数据。 逻辑层：包括SQK接口，解析器，优化器，Cache与buffer。 数据库引擎层：有常见的MyISAM,InnoDB等等。 物理层：负责文件存储，日志等等。 19.3 简述执行SQL语言的过程 客户端首先通过连接器进行身份认证和权限相关 如果是执行查询语句的时候，会先查询缓存，但MySQL 8.0 版本后该步骤移除。 没有命中缓存的话，SQL 语句就会经过解析器，分析语句，包括语法检查等等。 通过优化器，将用户的SQL语句按照 MySQL 认为最优的方案去执行。 执行语句，并从存储引擎返回数据。 19.4 简述MySQL的共享锁排它锁共享锁也称为读锁，相互不阻塞，多个客户在同一时刻可以同时读取同一个资源而不相互干扰。排他锁也称为写锁，会阻塞其他的写锁和读锁，确保在给定时间内只有一个用户能执行写入并防止其他用户读取正在写入的同一资源。 19.5 简述MySQL中的按粒度的锁分类表级锁: 对当前操作的整张表加锁,实现简单，加锁快，但并发能力低。 行锁: 锁住某一行，如果表存在索引，那么记录锁是锁在索引上的，如果表没有索引，那么 InnoDB 会创建一个隐藏的聚簇索引加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 Gap 锁：也称为间隙锁: 锁定一个范围但不包括记录本身。其目的是为了防止同一事物的两次当前读出现幻读的情况。 Next-key Lock： 行锁+gap锁。 19.6 如何解决数据库死锁 预先检测到死锁的循环依赖，并立即返回一个错误。 当查询的时间达到锁等待超时的设定后放弃锁请求。 19.7 简述乐观锁和悲观锁乐观锁：对于数据冲突保持一种乐观态度，操作数据时不会对操作的数据进行加锁，只有到数据提交的时候才通过一种机制来验证数据是否存在冲突。 悲观锁：对于数据冲突保持一种悲观态度，在修改数据之前把数据锁住，然后再对数据进行读写，在它释放锁之前任何人都不能对其数据进行操作，直到前面一个人把锁释放后下一个人数据加锁才可对数据进行加锁，然后才可以对数据进行操作，一般数据库本身锁的机制都是基于悲观锁的机制实现的。 19.8 简述InnoDB存储引擎InnoDB 是 MySQL 的默认事务型引擎，支持事务，表是基于聚簇索引建立的。支持表级锁和行级锁，支持外键，适合数据增删改查都频繁的情况。 InnoDB 采用 MVCC 来支持高并发，并且实现了四个标准的隔离级别。其默认级别是 REPEATABLE READ，并通过间隙锁策略防止幻读，间隙锁使 InnoDB 不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定防止幻行的插入。 19.9 简述MyISAM存储引擎MySQL5.1及之前，MyISAM 是默认存储引擎。MyISAM不支持事务，Myisam支持表级锁，不支持行级锁，表不支持外键，该存储引擎存有表的行数，count运算会更快。适合查询频繁，不适合对于增删改要求高的情况 19.10 简述Memory存储引擎Memory存储引擎将所有数据都保存在内存，不需要磁盘 IO。支持哈希索引，因此查找速度极快。Memory 表使用表级锁，因此并发写入的性能较低。 19.11索引是什么？索引是存储引擎中用于快速找到记录的一种数据结构。在关系型数据库中，索引具体是一种对数据库中一列或多列的值进行排序的存储结构。 19.12 为什么引入索引？为了提高数据查询的效率。索引对数据库查询良好的性能非常关键，当表中数据量越来越大，索引对性能的影响越重要。 但是，（1）索引会降低插入、删除、更新表的速度。（2）索引需要占据物理空间。聚集索引一旦改变，还需要改变非聚集索引。 19.13 Mysql有哪些常见索引类型？ 数据结构角度B-Tree索引 、哈希索引 、R-Tree索引 、全文索引（使用ES更好） 物理存储角度主键索引（聚簇索引）：叶子节点存的是整行的数据 非主键索引（二级索引）：叶子节点存的主键的值 19.14 简述B-Tree与B+树B-Tree 是一种自平衡的多叉树。每个节点都存储关键字值。其左子节点的关键字值小于该节点关键字值，且右子节点的关键字值大于或等于该节点关键字值。 B+树也是是一种自平衡的多叉树。其基本定义与B树相同，不同点在于数据只出现在叶子节点，所有叶子节点增加了一个链指针，方便进行范围查询。 B+树中间节点不存放数据，所以同样大小的磁盘页上可以容纳更多节点元素，访问叶子节点上关联的数据也具有更好的缓存命中率。并且数据顺序排列并且相连，所以便于区间查找和搜索。 B树每一个节点都包含key和value，查询效率比B+树高。 19.15 简述Hash索引哈希索引对于每一行数据计算一个哈希码，并将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。只有 Memory 引擎显式支持哈希索引。 Hash索引不支持范围查询，无法用于排序，也不支持部分索引列匹配查找。 19.16 简述自适应Hash索引InnoDB对于频繁使用的某些索引值，会在内存中基于 B-Tree 索引之上再创键一个哈希索引，这也被称为自适应Hash索引。 19.17 简述聚集索引和稀疏索引聚集索引按每张表的主键构建一棵B+树，数据库中的每个搜索键值都有一个索引记录，每个数据页通过双向链表连接。表数据访问更快，但表更新代价高。 稀疏索引不会为每个搜索关键字创建索引记录。搜索过程需要，我们首先按索引记录进行操作，并按顺序搜索，直到找到所需的数据为止。 19.18 简述辅助索引与回表查询辅助索引是非聚集索引，叶子节点不包含记录的全部数据，包含了一个书签用来告诉InnoDB哪里可以找到与索引相对应的行数据。 通过辅助索引查询，先通过书签查到聚集索引，再根据聚集索引查对应的值，需要两次，也称为回表查询。 19.19 简述联合索引和最左匹配原则联合索引是指对表上的多个列的关键词进行索引。 对于联合索引的查询，如果精确匹配联合索引的左边连续一列或者多列，则mysql会一直向右匹配直到遇到范围查询（&gt;,&lt;,between,like）就停止匹配。Mysql会对第一个索引字段数据进行排序，在第一个字段基础上，再对第二个字段排序。 联合索引怎么实现？采用B+树实现，每个节点含有多个关键字，排序时按照多个关键字来排序。最左前缀原则：顾名思义是最左优先，以最左边的为起点任何连续的索引都能匹配上， 注：在创建联合索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。这样的话扩展性较好。 19.20 简述覆盖索引覆盖索引指一个索引包含或覆盖了所有需要查询的字段的值，不需要回表查询，即索引本身存了对应的值。 19.21 为什么数据库不用红黑树用B+树红黑树的出度为 2，而 B Tree 的出度一般都非常大。红黑树的树高 h 很明显比 B Tree 大非常多，IO次数很多，导致会比较慢，因此检索的次数也就更多。 B+Tree 相比于 B-Tree 更适合外存索引，拥有更大的出度，IO次数较少，检索效率会更高。 19.22 基于主键索引的查询和非主键索引的查询有什么区别？对于select from 主键=XX，基于主键的普通查询仅查找主键这棵树，对于select from 非主键=XX，基于非主键的查询有可能存在回表过程（回到主键索引树搜索的过程称为回表），因为非主键索引叶子节点仅存主键值，无整行全部信息。 19.23 非主键索引的查询一定会回表吗？不一定，当查询语句的要求字段全部命中索引，不用回表查询。如select 主键 from 非主键=XX，此时非主键索引叶子节点即可拿到主键信息，不用回表。 19.24 简述MySQL使用EXPLAIN 的关键字段explain关键字用于分析sql语句的执行情况，可以通过他进行sql语句的性能分析。 字段 应用 type：表示连接类型， 从好到差的类型排序为：system：系统表，数据已经加载到内存里。const：常量连接，通过索引一次就找到。 eq_ref：唯一性索引扫描，返回所有匹配某个单独值的行。ref：非主键非唯一索引等值扫描，const或eq_ref改为普通非唯一索引。range：范围扫描，在索引上扫码特定范围内的值。 index：索引树扫描，扫描索引上的全部数据。 all：全表扫描。 key：显示MySQL实际决定使用的键。 key_len 显示MySQL决定使用的键长度，长度越短越好 Extra：额外信息 Using filesort：MySQL使用外部的索引排序，很慢需要优化。Using temporary：使用了临时表保存中间结果，很慢需要优化。 Using index：使用了覆盖索引。 Using where：使用了where。 19.25 简述MySQL优化流程 通过慢日志定位执行较慢的SQL语句 利用explain对这些关键字段进行分析 根据分析结果进行优化 19.26 简述MySQL中的日志logredo log: 存储引擎级别的log（InnoDB有，MyISAM没有），该log关注于事务的恢复.在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。 undo log：是存储引擎级别的log（InnoDB有，MyISAM没有）保证数据的原子性，该log保存了事务发生之前的数据的一个版本，可以用于回滚，是MVCC的重要实现方法之一。 bin log：数据库级别的log，关注恢复数据库的数据。 19.27 简述事务事务内的语句要么全部执行成功，要么全部执行失败。 事务满足如下几个特性： 原子性（Atomicity）: 一个事务中的所有操作要么全部完成，要么全部不完成。 一致性（Consistency）: 事务执行前后数据库的状态保存一致。 隔离性（Isolation） 多个并发事务对数据库进行操作，事务间互不干扰。 持久性（Durability） 事务执行完毕，对数据的修改是永久的，即使系统故障也不会丢失 19.28 数据库中多个事务同时进行可能会出现什么问题？ 丢失修改 脏读：当前事务可以查看到别的事务未提交的数据。 不可重读：在同一事务中，使用相同的查询语句，同一数据资源莫名改变了。 幻读：在同一事务中，使用相同的查询语句，莫名多出了一些之前不存在的数据，或莫名少了一些原先存在的数据。 19.29 SQL的事务隔离级别有哪些？ 读未提交： 一个事务还没提交，它做的变更就能被别的事务看到。 读提交： 一个事务提交后，它做的变更才能被别的事务看到。 可重复读： 一个事务执行过程中看到的数据总是和事务启动时看到的数据是一致的。在这个级别下事务未提交，做出的变更其它事务也看不到。 串行化： 对于同一行记录进行读写会分别加读写锁，当发生读写锁冲突，后面执行的事务需等前面执行的事务完成才能继续执行。 19.30 什么是MVCC？MVCC为多版本并发控制，即同一条记录在系统中存在多个版本。其存在目的是在保证数据一致性的前提下提供一种高并发的访问性能。对数据读写在不加读写锁的情况下实现互不干扰,从而实现数据库的隔离性,在事务隔离级别为读提交和可重复读中使用到。 在InnoDB中，事务在开始前会向事务系统申请一个事务ID，该ID是按申请顺序严格递增的。每行数据具有多个版本，每次事务更新数据都会生成新的数据版本，而不会直接覆盖旧的数据版本。数据的行结构中包含多个信息字段。其中实现MVCC的主要涉及最近更改该行数据的事务ID（DB_TRX_ID）和可以找到历史数据版本的指针（DB_ROLL_PTR）。InnoDB在每个事务开启瞬间会为其构造一个记录当前已经开启但未提交的事务ID的视图数组。通过比较链表中的事务ID与该行数据的值与对应的DB_TRX_ID，并通过DB_ROLL_PTR找到历史数据的值以及对应的DB_TRX_ID来决定当前版本的数据是否应该被当前事务所见。最终实现在不加锁的情况下保证数据的一致性。 19.31 读提交和可重复读都基于MVCC实现，有什么区别？在可重复读级别下，只会在事务开始前创建视图，事务中后续的查询共用一个视图。而读提交级别下每个语句执行前都会创建新的视图。因此对于可重复读，查询只能看到事务创建前就已经提交的数据。而对于读提交，查询能看到每个语句启动前已经提交的数据。 19.32 InnoDB如何保证事务的原子性、持久性和一致性？ 利用undo log保障原子性。该log保存了事务发生之前的数据的一个版本，可以用于回滚，从而保证事务原子性。 利用redo log保证事务的持久性，该log关注于事务的恢复.在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。 利用undo log+redo log保障一致性。事务中的执行需要redo log，如果执行失败，需要undo log 回滚。 19.33 MySQL是如何保证主备一致的？MySQL通过binlog（二进制日志）实现主备一致。binlog记录了所有修改了数据库或可能修改数据库的语句，而不会记录select、show这种不会修改数据库的语句。在备份的过程中，主库A会有一个专门的线程将主库A的binlog发送给备库B进行备份。其中binlog有三种记录格式： statement:记录对数据库进行修改的语句本身，有可能会记录一些额外的相关信息。优点是binlog日志量少，IO压力小，性能较高。缺点是由于记录的信息相对较少，在不同库执行时由于上下文的环境不同可能导致主备不一致。 row:记录对数据库做出修改的语句所影响到的数据行以及对这些行的修改。比如当修改涉及多行数据，会把涉及的每行数据都记录到binlog。优点是能够完全的还原或者复制日志被记录时的操作。缺点是日志量占用空间较大，IO压力大，性能消耗较大。 mixed:混合使用上述两种模式，一般的语句使用statment方式进行保存，如果遇到一些特殊的函数，则使用row模式进行记录。MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式， 否则就用statement格式。但是在生产环境中，一般会使用row模式。 19.34 redo log与binlog的区别？ redo log是InnoDB引擎特有的，只记录该引擎中表的修改记录。binlog是MySQL的Server层实现的，会记录所有引擎对数据库的修改。 redo log是物理日志，记录的是在具体某个数据页上做了什么修改；binlog是逻辑日志，记录的是这个语句的原始逻辑。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 19.35 crash-safe能力是什么？InnoDB通过redo log保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 19.36 WAL技术是什么？WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。事务在提交写入磁盘前，会先写到redo log里面去。如果直接写入磁盘涉及磁盘的随机I/O访问，涉及磁盘随机I/O访问是非常消耗时间的一个过程，相比之下先写入redo log，后面再找合适的时机批量刷盘能提升性能。 19.37 两阶段提交是什么？为了保证binlog和redo log两份日志的逻辑一致，最终保证恢复到主备数据库的数据是一致的，采用两阶段提交的机制。 执行器调用存储引擎接口，存储引擎将修改更新到内存中后，将修改操作记录redo log中，此时redo log处于prepare状态。 存储引擎告知执行器执行完毕，执行器生成这个操作对应的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交commit状态，更新完成。 19.38 只靠binlog可以支持数据库崩溃恢复吗？不可以。 历史原因： InnoDB在作为MySQL的插件加入MySQL引擎家族之前，就已经是一个提供了崩溃恢复和事务支持的引擎了。InnoDB接入了MySQL后，发现既然binlog没有崩溃恢复的能力，那引入InnoDB原有的redo log来保证崩溃恢复能力。 实现原因： binlog没有记录数据页修改的详细信息，不具备恢复数据页的能力。binlog记录着数据行的增删改，但是不记录事务对数据页的改动，这样细致的改动只记录在redo log中。当一个事务做增删改时，其实涉及到的数据页改动非常细致和复杂，包括行的字段改动以及行头部以及数据页头部的改动，甚至b+tree会因为插入一行而发生若干次页面分裂，那么事务也会把所有这些改动记录下来到redo log中。因为数据库系统进程crash时刻，磁盘上面页面镜像可以非常混乱，其中有些页面含有一些正在运行着的事务的改动，而一些已提交的事务的改动并没有刷上磁盘。事务恢复过程可以理解为是要把没有提交的事务的页面改动都去掉，并把已经提交的事务的页面改动都加上去这样一个过程。这些信息，都是binlog中没有记录的，只记录在了存储引擎的redo log中。 操作写入binlog可细分为write和fsync两个过程，write指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘,fsync才是将数据持久化到磁盘的操作。通过参数设置sync_binlog为0的时候，表示每次提交事务都只write，不fsync。此时数据库崩溃可能导致部分提交的事务以及binlog日志由于没有持久化而丢失。 19.39 简述MySQL主从复制MySQL提供主从复制功能，可以方便的实现数据的多处自动备份，不仅能增加数据库的安全性，还能进行读写分离，提升数据库负载性能。 主从复制流程： 在事务完成之前，主库在binlog上记录这些改变，完成binlog写入过程后，主库通知存储引擎提交事物 从库将主库的binlog复制到对应的中继日志，即开辟一个I/O工作线程，I/O线程在主库上打开一个普通的连接，然后开始binlog dump process，&gt;将这些事件写入中继日志。从主库的binlog中读取事件，如果已经读到最新了，线程进入睡眠并等待ma主库产生新的事件。 读写分离：即只在MySQL主库上写，只在MySQL从库上读，以减少数据库压力，提高性能。 19.40 如何保证数据库和缓存数据一致binlog刷新缓存。 20. MySQL事务和Spring事务spring事务本质上使用数据库事务，而数据库事务本质上使用数据库锁，所以spring事务本质上使用数据库锁，开启spring事务意味着使用数据库锁。假如数据库不支持事务的话,spring的事务是没有作用的.数据库的事务说简单就只有开启,回滚和关闭,spring对数据库事务的包装,原理就是拿一个数据连接,根据spring的事务配置,操作这个数据连接对数据库进行事务开启,回滚或关闭操作.但是spring除了实现这些,还配合spring的传播行为对事务进行了更广泛的管理.其实这里还有个重要的点,那就是事务中涉及的隔离级别,以及spring如何对数据库的隔离级别进行封装 20.1 Spring 事务的传播属性所谓spring事务的传播属性，就是定义在存在多个事务同时存在的时候，spring应该如何处理这些事务的行为。这些属性在TransactionDefinition中定义，具体常量的解释见下表：默认的传播级别是什么？ 常量名称 常量解释 PROPAGATION_REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择，也是 Spring 默认的事务的传播。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 支持当前事务，如果当前没有事务，就抛出异常。(ps: mandatory强制的; 法定的; 义务的;) PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager事务管理器起效。 传播行为和数据库功能无关，只是事务管理器为了处理复杂业务而设计的一个机制。比如现在有这样一个调用场景，A Service -&gt; B Service -&gt; C Service，但是希望A/B在一个事务内，C是一个独立的事务，同时C如果出错，不影响AB所在的事务。此时，就可以通过传播行为来处理；将C Service的事务配置为@Transactional(propagation = Propagation.REQUIRES_NEW)即可 20.2 数据库隔离级别 隔离级别 隔离级别的值 导致的问题 Read-Uncommitted 0 导致脏读 Read-Committed 1 避免脏读，允许不可重复读和幻读 Repeatable-Read 2 避免脏读，不可重复读，允许幻读 Serializable 3 串行化读，事务只能一个一个执行，避免了脏读、不可重复读、幻读。执行效率慢，使用时慎重 20.3 Spring中的隔离级别 常量 解释 ISOLATION_DEFAULT 这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与 JDBC 的隔离级别相对应。 ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它充许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。 ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。 20.4 Spring 事务同步管理器的原理20.4.1 事务管理的关键抽象graph TB B1(PlatformTransactionManager)A1(TransactionDefinition) B1-.->C1(TransactionStatus) TransactionDefinition：描述事务的隔离级别、超时时间、是否为只读事务、事务的传播规则等控制事务具体行为的事务属性。 PlatformTransactionManager：根据TransactionDefinition提供的事务属性配置创建事务。（采用事务管理器提交/回滚事务操作） TransactionStatus：描述事务的状态。 20.4.2 ThreadLocal的使用Spring将JDBC的Connection，Hibernate的Session等访问数据库的连接或者会话对象统称为资源，这些资源在同一时刻是不能够被多线程共享的，为了让DAO，Service类可以做到singleton，Spring的事务同步管理器Transaction SynchronizationManager使用ThreadLocal为不同的事务提供独立的资源。 20.4.3 编程式事务、申明式事务 try catch住事务提交还是回滚？答案：提交,异常没有提交给AOP拦截！！！使用@Transactional注解的时候不要使用try catch，如果try catch后需要手动使用TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();回滚。 十一、tomcat等web服务十二、RPChttps://xiaomi-info.github.io/2020/03/02/rpc-achieve/ Proxy、InvocationHandler 网络通信 序列化、反序列化，编解码 管理服务地址 响应 本文主要论述的是“RPC 实现原理”，那么首先明确一个问题什么是 RPC 呢？RPC 是 Remote Procedure Call 的缩写，即，远程过程调用。RPC 是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而开发人员无需额外地为这个交互编程。值得注意是，两个或多个应用程序都分布在不同的服务器上，它们之间的调用都像是本地方法调用一样。接下来我们便来分析一下一次 RPC 调用发生了些什么？ 现在业界内比较流行的一些 RPC 框架，例如 Dubbo 提供的是基于接口的远程方法调用，即客户端只需要知道接口的定义即可调用远程服务。在 Java 中接口并不能直接调用实例方法，必须通过其实现类对象来完成此操作，这意味着客户端必须为这些接口生成代理对象，对此 Java 提供了 Proxy、InvocationHandler 生成动态代理的支持；生成了代理对象，那么每个具体的发方法是怎么调用的呢？jdk 动态代理生成的代理对象调用指定方法时实际会执行 InvocationHandler 中定义的 #invoke 方法，在该方法中完成远程方法调用并获取结果。 抛开客户端，回过头来看 RPC 是两台计算机间的调用，实质上是两台主机间的网络通信，涉及到网络通信又必然会有序列化、反序列化，编解码等一些必须要考虑的问题；同时实际上现在大多系统都是集群部署的，多台主机/容器对外提供相同的服务，如果集群的节点数量很大的话，那么管理服务地址也将是一件十分繁琐的事情，常见的做法是各个服务节点将自己的地址和提供的服务列表注册到一个 注册中心，由 注册中心 来统一管理服务列表；这样的做法解决了一些问题同时为客户端增加了一项新的工作——那就是服务发现，通俗来说就是从注册中心中找到远程方法对应的服务列表并通过某种策略从中选取一个服务地址来完成网络通信。 聊了客户端和 注册中心，另外一个重要的角色自然是服务端，服务端最重要的任务便是提供服务接口的真正实现并在某个端口上监听网络请求，监听到请求后从网络请求中获取到对应的参数（比如服务接口、方法、请求参数等），再根据这些参数通过反射的方式调用接口的真正实现获取结果并将其写入对应的响应流中。 综上所述，一次基本的 RPC 调用流程大致如下： 生产者端流程： 加载服务接口，并缓存 服务注册，将服务接口以及服务主机信息写入注册中心（本例使用的是 zookeeper) 启动网络服务器并监听 反射，本地调用 消费者端流程： 代理服务接口生成代理对象 服务发现（连接 zookeeper，拿到服务地址列表，通过客户端负载策略获取合适的服务地址） 远程方法调用（本例通过 Netty，发送消息，并获取响应结果） 十三、服务发现设计1. 为什么需要服务发现？​ 为什么一定需要一个服务发现系统呢？服务启动的时候直接读取一个本地配置，然后通过远程配置系统，动态推送下来不行吗？实际上，当服务节点规模较小时，该方案也行得通，但如果遇到以下的场景呢？ 场景 1. 在微服务的世界中，服务节点的扩缩容、服务版本的迭代是常态，服务消费端需要能够快速及时的感知到节点信息的变更（网络地址、节点数量）。 2. 当服务节点规模巨大时，节点的不可用也会变成常态，服务提供者要能够及时上报自己的健康状态，从而做到及时剔除不健康节点（或降低权重）。 3. 当服务部署在多个可用区时，需要将多个可用区的服务节点信息互相同步，当某个可用区的服务不可用时，服务消费者能够及时切换到其他可用区（通过负载均衡算法自动切换或手动紧急切换），从而做到多活和高可用。 4. 服务发现背后的存储应该是分布式的，这样当部分服务发现节点不可用的时候，也能提供基本的服务发现功能 5. 除了ip、port我们需要更多的信息，比如节点权重、路由标签信息等等。 2. 服务发现设计？2.1 客户端发现模式2.2 服务端发现模式3. 服务发现中的数据一致性问题4. 健康检查5. 服务优雅上下线6. 容灾和高可用十四、设计模式1. 设计模式大纲？ 编号 设计模式 解释 举例子 1 观察者模式 表示对象与对象之间具有依赖关系,当一个对象发生改变的时候,这个对象依赖的对象也会作出响应.(Observable,Obser) Spring 事件驱动模型就是观察者模式很经典的⼀个应⽤。 2 装饰者模式 不修改底层代码的前提下，给对象赋予新的职责 3 工厂模式 BeanFactory（AppliclicationContext） 4 单例模式 创建独一无二的，只能有一个的实例对象。(懒汉，恶汉) Spring Bean默认是单例模式 5 命令模式 6 适配器模式和外观模式 适配器模式将一个接口转换成客户希望的另外一个接口,适配器模式使接口不兼容的那些类可以一起工作 Spring AOP 的增强或通知(Advice)使⽤到了适配器模式、Spring MVC 中也是⽤到了适配器模式适配 Controller 。 7 模板方法模式 Mysql，Redis，Kafaka，MongoDB（Spring中jdbcTemplate, hibernateTemplate） 8 迭代和组合模式 9 状态模式 10 代理模式 Spring AOP的实现 11 复合模式]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java总结-2]]></title>
    <url>%2F2019%2F11%2F20%2FJava%E6%80%BB%E7%BB%93-2%2F</url>
    <content type="text"><![CDATA[Java总结-2 五、Java（一）基础0. Java对象普通对象 markword：（1）锁信息，（2）GC信息，（3）hashcode（Identity HashCode）,怎么标记这个锁是自己持有的？ 类型指针class pointer 实例数据instance data 对齐padding 数组 markword class pointer 数组长度length（4个字节） 实例数据instance data 对齐 padding 1.泛型？ 泛型是如何实现的? 泛型是通过类型擦除实现的。 什么是泛型中的限定通配符和非限定通配符 ? 这是另一个非常流行的Java泛型面试题。限定通配符对类型进行了限制。有两种限定通配符，一种是&lt;? extends T&gt;它通过确保类型必须是T的子类来设定类型的上界，另一种是&lt;? super T&gt;它通过确保类型必须是T的父类来设定类型的下界。泛型类型必须用限定内的类型来进行初始化，否则会导致编译错误。另一方面&lt;?&gt;表示了非限定通配符，因为&lt;?&gt;可以用任意类型来替代。 List&lt;? extends T&gt;和List &lt;? super T&gt;之间有什么区别 ? List&lt;? extends T&gt;可以接受任何继承自T的类型的List，而List&lt;? super T&gt;可以接受任何T的父类构成的List。例如List&lt;? extends Number&gt;可以接受List或List 可以把List传递给一个接受List参数的方法吗？ 编译错误。 补充（List并不是ArrayList的父类） 2.内部类？（成局静匿）（1）成员内部类 一个类定义在一个类的内部，看起来像类的成员。 问题： 有隐藏的问题 访问：成员内部类可以无条件地访问外部类的成员，而外部类想访问成员内部类的成员却不是这么随心所欲了。在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问 （2）局部内部类 定义在方法或者是一个作用域内，和成员内部类的区别在于访问权限上。 问题： 局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。 是不能有 public、protected、private 以及 static 修饰符的。 （3）静态内部类 静态内部类也是定义在另一个类里面的类，只不过在类的前面多了一个关键字static。静态内部类是不需要依赖于外部类的，这点和类的静态成员属性有点类似，并且它不能使用外部类的非static成员变量或者方法 静态内部类是不依赖于外部类的，也就说可以在不创建外部类对象的情况下创建内部类的对象。 （4）匿名内部类 一般使用匿名内部类的方法来编写事件监听代码。同样的，匿名内部类也是不能有访问修饰符和 static 修饰符的。 3.构造器能不能override？不可以，可以重载，不可以重写，理由：父类的私有属性以及构造器不能够被重载。 4.String，StringBulider，StringBuffer？对方法或者被调用的方法加了同步锁。 String的基本特征，不可变类，声明为final类，当然使用类反射是可以更改的，一般用在常量申明 StringBuffer与StringBuilder 内存不够，将原来的复制过来 5.static相关？(类，方法，属性，代码块) （目的：方便在没有new的情况下使用方法和属性） （1）修饰？ 类 方法：是没有this的，因为它不依附于任何对象，在静态方法中不能访问类的非静态成员变量和非静态成员方法，因为非静态成员方法/变量都是必须依赖具体的对象才能够被调用。非静态成员方法中是可以访问静态成员方法/变量的。 属性：静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。而非静态变量是对象所拥有的，在创建对象的时候被初始化，存在多个副本，各个对象拥有的副本互不影响。 代码块：用来形成静态代码块以优化程序性能。static块可以置于类中的任何地方，类中可以有多个static块。在类初次被加载的时候，会按照static块的顺序来执行每个static块，并且只会执行一次。 （2）特点 在static方法内部不能调用非静态方法。 例题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @Author MaoTian * @Classname StaticTest * @Description static使用 * * test static * myclass static * person static * person Test * test constructor * person MyClass * myclass constructor * * @Date 下午8:23 2019/8/12 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class Person&#123; static&#123; System.out.println("person static"); &#125; public Person(String str) &#123; System.out.println("person "+str); &#125;&#125;//StaticTest已经加载过了//而在生成对象的时候，必须先初始化父类的成员变量，因此会执行Test中的Person person = new Person()class MyClass extends StaticTest &#123; Person person = new Person("MyClass"); //（3）被加载 static&#123; System.out.println("myclass static"); &#125; public MyClass() &#123; System.out.println("myclass constructor"); &#125;&#125;//public class StaticTest &#123; Person person = new Person("Test"); //（1）首先被加载 static&#123; System.out.println("test static"); &#125; public StaticTest() &#123; System.out.println("test constructor"); &#125; public static void main(String[] args) &#123; //（2）被执行 new MyClass(); &#125; 6.接口和抽象类的区别？ （1）接口？ 一个类可以实现多个接口。 接口中除了static、final变量，不能有其他变量， 不能够有方法的实现，java 8开始支持default。 接口默认是：public的 从设计上讲：抽象是对类的抽象 （2）抽象类？ 一个类只能继承一个抽象类。 抽象类中可以有属性以及方法的实现。 从设计上讲：是对行为的抽象。 抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！） 7.equals和hashcode？ 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖（默认使用的地址来计算，不重写hashcode很可能不一样） hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 8.Throwable类？（1）结构？ java.lang Class Throwable java.lang.Object java.lang.Throwable All Implemented Interfaces: Serializable Direct Known Subclasses: Error, Exception 常用的方法：getMessage，printStackTrace public string getMessage():返回异常发生时的详细信息 public string toString():返回异常发生时的简要描述 public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以声称本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 （2）常见的异常和错误？ 异常（可检查的，不可检查的（运行时运行）） ArithmeticException IndexOutOfBoundsException RuntimeException（在运行的时候抛出） NullPointerException 原则： 尽量捕获特定的异常 不要生吞异常 只捕获必要的代码段 不要使用异常处理块控制代码的流程 throw抛出异常 throws try with语法糖，自动关闭资源，（类比Python中的 | 编号 | 类型 | 代表 || :—- | :—————————————————————————————- | :——————————————————- || 1 | 检查型（在函数的声明上必须添加throws）（不是RuntimeException的派生类） | IOException，SQLException || 2 | 非检查型（是RuntimeException的派生类） | NullPointException，ClassCastException | 错误（Linkage Error，virtual machine Error） OutOfMemoryError StackOverflowError 编号 异常Exception 错误Error 1 ArithmeticException AbstractMethodError 2 ArrayIndexOutOfBoundsException AssertionError 3 ArrayStoreException BootstrapMethodError 4 ClassCastException ClassCircularityError 5 ClassNotFoundException ClassFormatError 6 CloneNotSupportedException Error 7 EnumConstantNotPresentException ExceptionInInitializerError 8 Exception IllegalAccessError 9 IllegalAccessException IncompatibleClassChangeError 10 IllegalArgumentException InstantiationError 11 IllegalMonitorStateException InternalError 12 IllegalStateException LinkageError 13 IllegalThreadStateException NoClassDefFoundError 14 IndexOutOfBoundsException NoSuchFieldError 15 InstantiationException NoSuchMethodError 16 InterruptedException OutOfMemoryError 17 NegativeArraySizeException StackOverflowError 18 NoSuchFieldException ThreadDeath 19 NoSuchMethodException UnknownError 20 NullPointerException UnsatisfiedLinkError 21 NumberFormatException UnsupportedClassVersionError 22 ReflectiveOperationException VerifyError 23 RuntimeException VirtualMachineError 24 SecurityException 25 StringIndexOutOfBoundsException 26 TypeNotPresentException 27 UnsupportedOperationException （3）Throw和Throws的区别？ Throw用于方法内部，Throws用于方法声明上 Throw后跟异常对象，Throws后跟异常类型 Throw后只能跟一个异常对象，Throws后可以一次声明多种异常类型 （4）异常是怎么实现的？ 在编译器生成的字节码中，每一个方法都附有一个异常表（Exception Table），由四部分构成： from指针：指示了该异常处理器所监控的范围 to指针：指示了该异常处理器所监控的范围 target指针：指向异常处理器的起始位置 捕获的异常类型 123456789public static void test()&#123; try &#123; int a=1/0; &#125;catch (Exception e)&#123; &#125;finally &#123; &#125;&#125; 9. OutOfMemory详解 名字 图片 解释 Exception in thread “main” java.lang.StackOverflowError 递归调用，深度太深 Exception in thread “main” java.lang.OutOfMemoryError: Java heap space 堆空间不够（） java.lang.OutOfMemoryError: GC overhead limit exceeded 垃圾回收事倍功半，GC直接罢工 Exception in thread “main” java.lang.OutOfMemoryError: Direct buffer memory nio分配直接内存，不够导致 Exception in thread “main” java.lang.OutOfMemoryError: unable to create new native thread 同一个进程创建的线程太多了，Linux默认限制为1024个。 方法区溢出（方法区又被称作永久代）： 12345678910111213public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; // 使用List保持着常量池引用，避免Full GC回收常量池行为 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); // 10MB的PermSize在integer范围内足够产生OOM了 int i = 0; while (true) &#123; //intern作用是把首次遇到的字符串实例复制到永久代去，返回的也是永久代中这个字符串实例的引用。 list.add(String.valueOf(i++).intern()); &#125; &#125;&#125; 10.Collections和Arrays常见的方法？https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/Basis/Arrays,CollectionsCommonMethods.md 11.反射 JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。 类名 用途 Class类 代表类的实体，在运行的Java应用程序中表示类和接口 //第1种方式获取Class对象 ：通过实例对象getClass()Person p1=new Person(); Class clazz1=p1.getClass(); //第2种方式获取Class对象:类名.class Class clazz2=Person.class; //第3种方式获取Class对象 Class :全类名clazz3=Class.forName(“JavaDemo.VMTest.ReflectDemo.Person”); Field类 代表类的成员变量（成员变量也称为类的属性） Field field2=clazz.getDeclaredField(“idcard”); field2.setAccessible(true); field2.set(obj, “123456”); Method类 代表类的方法 Method m2=clazz.getDeclaredMethod(“show”,String.class); m2.setAccessible(true);m2.invoke(obj,”smt”); Constructor类 代表类的构造方法 //获取私有构造方法 Constructor cc3=clazz.getDeclaredConstructor(int.class); //暴力访问 cc3.setAccessible(true); Object oo3=cc3.newInstance(1); 如何使用反射(1)使用Class类,获取出被解剖的这个类的class文件对象 (2) 使用Class类方法,获取出类中的所有成员 (3) 将成员获取出来后,交给对应类,对应类中的方法,运行成员 如何获取,class文件对象 使用类的对象获取 每个类都使用Object作为父类,Object类方法 getClass()返回这个类的class文件对象,方法返回值Class类型对象 使用类的静态属性获取:类名.class 返回这个类的class文件对象.属性运行结果也是Class类型对象(并不是使用的是编译后的字节码class文件！) 使用Class类的静态方法获取:Class类静态方法 forName(String 类名) 传递字符串类名获取到这个类的class文件对象,方法返回值也是Class类型对象 不管用哪种方式获取的Class对象，他们都是相等的。 如何创建对象 （1）newInstance方法 （2）获取构造器 123&gt;Constructor c=clazz.getDeclaredConstructor(String.class,String.class,int.class); &gt; Person p1=(Person) c.newInstance("李四","男",20);&gt; 12.Object方法 编号 名字 解释 １ getClass native方法，用于返回当前对象的Class对象 ２ hashCOde native方法，用于返回对象的哈希码（将对象的内存地址转换为整数返回）哈希碰撞，hashcode降低了搜索的成本。 ３ equals 用于比较两个对象的地址是否一样 ４ clone 用于返回当前对象的一份拷贝 ５ toString 返回类的名字@实例的哈希编码的十六进制字符串 ６ notify ７ notifyAll 唤醒监视线程 ８ wait native方法，不可以重写，暂停线程的执行 9 finalize 实例被垃圾回收的时候触发的操作 13.POJO POJO是Plain Ordinary Java Object的缩写，但是它通指没有使用Entity Beans的普通java对象，可以把POJO作为支持业务逻辑的协助类。 14.hashmap为什么是数组+链表，不是数组+数组?15.数组为什么要连续存放？16.包装类和基础类的区别？拆箱装箱 17.Java中 try..catch关闭流的语法糖?18.final，finally，finalize？ final修饰类，方法，变量，为什么要final？不想被修改。 finally：出现异常的时候，重要的代码会被执行（比如关闭数据库连接池） finalize：定义在Object类中，垃圾回收之前调用，在对象回收以前释放资源，没一个对象的finalize方法只会被执行一次，在Java9中已经抛弃了，缺陷：不能保证GC马上执行。finalize执行的流程：在GC的时候判断有没有覆盖finalize，实现了的话就会加入F-QUEUE队列，再次判断reachable是否复活还是回收。进入队列等待低优先级的线程来处理，所以并不一定会被回收。 19.面向对象的原则 单一职责原则（Single Responsibility Principle）每一个类应该专注于做一件事情。 里氏替换原则（Liskov Substitution Principle）超类存在的地方，子类是可以替换的。 依赖倒置原则（Dependence Inversion Principle）实现尽量依赖抽象，不依赖具体实现。 接口隔离原则（Interface Segregation Principle）应当为客户端提供尽可能小的单独的接口，而不是提供大的总的接口。 迪米特法则（Law Of Demeter）又叫最少知识原则，一个软件实体应当尽可能少的与其他实体发生相互作用。 开闭原则（Open Close Principle）面向扩展开放，面向修改关闭。 组合/聚合复用原则（Composite/Aggregate Reuse Principle CARP）尽量使用合成/聚合达到复用，尽量少用继承。原则： 一个类中有另一个类的对象。 20.多态相关123456789101112131415161718下列代码的输出结果是：public class A &#123; String name="a"; String go()&#123; return "- function in A"; &#125; &#125; public class B extends A &#123; String name="b"; String go()&#123; return "- function in B"; &#125; public static void main(String[] args) &#123; A a= new B(); System.out.println(a.name+a.go()); &#125; &#125; a - function in B 1、属性不存在重写，只有方法(非私有方法、非静态方法、非final方法) 才存在重写，才能发生多态； 2、向上转型 21.字符串编码的区别？22.Math.floor, Math.ceil,Math.round? 编号 名称 含义 例子 1 Math.floor 小于等于自身(向下取整) Math.floor(-11.5)=-12.0Math.floor(11.5)=11.0 2 Math.ceil 大于等于自身(向上取整) Math.ceil(-11.5)=-11.0Math.ceil(11.5)=12.0 3 Math.round 加0.5向下取整 Math.round(-11.4)=-11Math.round(-11.6)=-12 ceil 英语含义是天花板 floor英语含义是地板 12345---------ceil------------ 数字---------floor----------- 23.修饰范围 编号 关键字 修饰位置 1 static（不想新建对象使用） 类，方法，属性，代码块（执行一次） 2 final（不想被修改） 类，方法，变量 3 synchronized 方法，代码块(锁定一个对象) 24. equals和hashcode 如果两个对象相等，那么它们的hashCode()值一定相同。这里的相等是指，通过equals()比较两个对象时返回true。 如果两个对象hashCode()相等，它们并不一定相等 （二）容器0.整体架构图中的绿色的虚线代表实现，绿色实线代表接口之间的继承，蓝色实线代表类之间的继承。 1.HashMap？ 并发场景下如果要保证一种可行的方式是使用 Collections.synchronizedMap()方法来包装我们的 HashMap。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。 遍历（四种方式） 123456789101112131415161718192021222324252627282930313233343536373839&gt; &gt; public class HashMapTraversal &#123;&gt; &gt; public static void main(String[] args) &#123;&gt; &gt; Map&lt;String,Integer&gt; map=new HashMap&lt;&gt;();&gt; &gt; for(int i=0;i&lt;5;i++)&#123;&gt; &gt; map.put("key_"+String.valueOf(i),i);&gt; &gt; &#125;&gt; &gt; //(1)使用keySet&gt; &gt; for (String key:map.keySet())&#123;&gt; &gt; System.out.println(key+":"+map.get(key));&gt; &gt; &#125;&gt; &gt; &gt; &gt; System.out.println("-----------------------------------");&gt; &gt; &gt; &gt; //(2)entrySet&gt; &gt; for (Map.Entry&lt;String,Integer&gt; entry:map.entrySet())&#123;&gt; &gt; System.out.println(entry.getKey()+":"+entry.getValue());&gt; &gt; &#125;&gt; &gt; &gt; &gt; System.out.println("-----------------------------------");&gt; &gt; &gt; &gt; //(3)和（2）是一致的&gt; &gt; Set&lt;Map.Entry&lt;String,Integer&gt;&gt; entrySet = map.entrySet();&gt; &gt; for(Map.Entry&lt;String,Integer&gt; entry:entrySet)&#123;&gt; &gt; System.out.println(entry.getKey()+":"+entry.getValue());&gt; &gt; &#125;&gt; &gt; &gt; &gt; System.out.println("-----------------------------------");&gt; &gt; &gt; &gt; //(4)使用Iterator&gt; &gt; Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; it = map.entrySet().iterator();//上下相同, 这个清晰一些&gt; &gt; while (it.hasNext()) &#123;&gt; &gt; Map.Entry&lt;String, Integer&gt; tmp=it.next();&gt; &gt; System.out.println(tmp.getKey()+":"+tmp.getValue());&gt; &gt; &#125;&gt; &gt; &gt; &gt; &#125;&gt; &gt; &#125;&gt; &gt; &gt; &gt; 2.ArrayList的扩容？（扩容为原来的1.5倍）(1)先从 ArrayList 的构造函数 编号 构造函数 意义 1 指定了初始化的容量大小 2 默认为空，在add的时候会生成一个大小为10的ArrayList 3 使用集合来构造，会拷贝集合中的值，使用集合大小进行初始化 （2）扩容（通常大小会变为原来大小的1.5倍，注意和HashMap的扩容机制进行比较） 编号 方法 解释 1 add 确保判断当前要加入的有空间 2 ensureCapacityInternal 和默认的大小10进行比较 3 ensureExplicitCapacity 判读是不是需要扩容，需要扩容就进行扩容 4 grow 将原来的大小变为1.5倍，看看符合要求不，不符合，就是用需求的大小，如果计算出的新的大小必最大值还大，就是用hugeCapacity 5 hugeCapacity 就是使用最大的整数进行初始化 6 ensureCapacity 这个函数是提供给用户使用的，可以节约时间 （3）补充 System.arraycopy()和Arrays.copyOf()方法 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); 使用 Arrays.copyOf()方法主要是为了给原有数组扩容 3.HashMap的扩容？（扩容为原来的两倍） HashMap的扩容要和ArrayList的扩容进行区分，HashMap的扩容的真正原因是为了回避Hash冲突，当table中已经倍占用了75%（默认负载因子）就需要进行扩容，扩容的大小为原来的两倍。 时机：阈值=负载因子×table大小 扩容table rehash 4.HashMap和HashTable的区别？ 区别 HashMap HashTable 底层实现 1.8：Node数组+红黑树，1.8之前（链表散列） 数组+链表（链表散列） 扩容机制 默认是16，扩容翻倍；HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证） 默认是11，扩容2n+1；指定的话，将会按照指定大小。 效率 相对高 相对低一些 对Null key 和Null value的支持 null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null 但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException 线程是否安全 HashMap 是非线程安全的 HashTable 内部的方法基本都经过synchronized 修饰 5.HashMap中的Node，及其类型？（1）Node（实现接口Map.Entry） （2）TreeNode(继承自LinkedHashMap.Entry) （3）ConcurrentHash的节点？ 编号 名字 解释 1 Node结点 默认链接到table[i]——桶上的结点就是Node结点。 2 TreeNode结点 TreeNode就是红黑树的结点，TreeNode不会直接链接到table[i]——桶上面，而是由TreeBin链接，TreeBin会指向红黑树的根结点。 3 TreeBin节点 TreeBin相当于TreeNode的代理结点。TreeBin会直接链接到table[i]——桶上面，该结点提供了一系列红黑树相关的操作，以及加锁、解锁操作。 4 ForwardingNode节点 ForwardingNode结点仅仅在扩容时才会使用。 5 ReservationNode节点 保留结点，ConcurrentHashMap中的一些特殊方法会专门用到该类结点。 6.ArrayLis实现 实现接口 List RandomAccess Cloneable java.io.Serializable 继承的类 AbstractList 内部类 private class Itr implements Iterator private class ListItr extends Itr implements ListIterator private class SubList extends AbstractList implements RandomAccess static final class ArrayListSpliterator implements Spliterator ArrayList有四个内部类，其中的Itr是实现了Iterator接口，同时重写了里面的hasNext()，next()，remove()等方法；其中的ListItr继承Itr，实现了ListIterator接口，同时重写了hasPrevious()，nextIndex()，previousIndex()，previous()，set(E e)，add(E e)等方法，所以这也可以看出了 Iterator和ListIterator的区别:ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 7.LinkedList实现 LinkedList是通过双向链表实现的！ 8.TreeSet，TreeMap TreeMap底层是一棵红黑树，TreeMap实现的是NavigableMap 个人认为TreeSet之于TreeMap和HashSet之于HashMap是一样的。 （三）并发1.Java内存模型？《深入理解Java虚拟机》 深入理解Java虚拟机中说：Java虚拟机视图定义一种Java内存模型（Java memory model）来屏蔽各种硬件和操作系统之间的内存访问差异，实现Java程序在各种平台下都能够达到一致性访问的效果。定义了程序中各个变量的访问规则，特别注意的是，这些变量不包括局部变量和方法参数！，因为这些是方法私有的！ 2.单例模式的实现？ 双重检查加锁（恶汉模式） 1234567891011121314151617181920public class Singleton &#123; //uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); //这段代码其实是分为三步执行： //1. 为 uniqueInstance 分配内存空间 //2. 初始化 uniqueInstance //3. 将 uniqueInstance 指向分配的内存地址 private volatile static Singleton uniqueInstance; private Singleton()&#123;&#125; public static Singleton getUniqueInstance()&#123; if(uniqueInstance==null)&#123; synchronized (Singleton.class)&#123; if(uniqueInstance==null)&#123; uniqueInstance=new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 静态内部类模式（恶汉模式） 123456789public class Singleton2 &#123; private static final class SingleHandler&#123; private static final Singleton2 INSTANCE=new Singleton2(); &#125; private Singleton2()&#123;&#125; public static Singleton2 getInstance()&#123; return SingleHandler.INSTANCE; &#125;&#125; 3.线程的实现方式？怎么使用lambda的形式？ ps:Python通过两个标准库thread和threading提供对线程的支持。thread提供了低级别的、原始的线程以及一个简单的锁。 （1）Thread——————run()方法 （2）Runnable——————run()方法 （3）Callable—————call()—————FutureTask 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package JavaBasic;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;/** * @Classname ThreadTest * @Description 多线程的三种实现方式 * @Date 19-7-22 上午9:30 * @Created by mao&lt;tianmao818@qq.com&gt; */class MyThread1 extends Thread&#123; @Override public void run()&#123; System.out.println("thread by extends Thread"); &#125;&#125;class MyThread2 implements Runnable&#123; @Override public void run()&#123; System.out.println("thread by implements Runnable"); &#125;&#125;class MyThread3 implements Callable&#123; @Override public String call() throws Exception&#123; return "thread by implements Callable"; &#125;&#125;public class ThreadTest &#123; public static void main(String[] args)&#123; MyThread1 myThread1=new MyThread1(); myThread1.start(); MyThread2 myThread2=new MyThread2(); new Thread(myThread2).start(); //FutureTask的使用 //Future类的使用 FutureTask&lt;String&gt; futureTask=new FutureTask&lt;&gt;(new MyThread3()); new Thread(futureTask).start(); try &#123; String res=futureTask.get(); System.out.println(res); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; (4)使用匿名内部类 12345678910111213141516new Thread(()-&gt;&#123; System.out.println(Thread.currentThread()+"thread by lambda");&#125;).start();new Thread() &#123; public void run() &#123; System.out.println(Thread.currentThread()+"thread by 1"); &#125;&#125;.start();Runnable r = new Runnable() &#123;//创建方式2 public void run() &#123; System.out.println(Thread.currentThread()+"thread by 2"); &#125;&#125;;new Thread(r).start(); 补充： 对于同一个Thread使用两次start将会有什么结果？ 4.线程池(1) 为什么要使用线程池？ 在执行大量异步任务的时候能够提供良好的性能。（线程的创建和销毁是有时间开销的） 提供了一种良好的资源限制和管理的方式。 (2) 线程池的架构图 编号 名称 说明 1 Executor 只有execute方法 2 ExecutorService（继承） 3 AbstractExecutorService（实现） 4 ThreadPoolExecutor（继承） 构造器有七个参数 5 Executors（只是一个工具类） 注意：接口和接口之间不可以使用implements，但是是可以使用继承的！，其实很好理解：implements是需要将所有的方法都实现的，但是interface是不能够包含具体的实现的！切记！ ThreadPoolExecutor继承了AbstractExecutorService，AbstractExecutorService实现了ExecutorService接口，ExecutorService继承了Executor接口。 Executors是一个工具类，主要的方法有： Executors.newFixedThreadPool 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; Executors.newCachedThreadPool 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; Executors.newSingleThreadExecutor（一个看来称不上是pool） 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; ThreadPoolExecutor参数(7个参数) @param corePoolSize 核心线程池中的最大线程数 @param maximumPoolSize 总线程池中的最大线程数 @param keepAliveTime 空闲线程的存活时间 @param unit keepAliveTime的单位 @param workQueue 任务队列, 保存已经提交但尚未被执行的线程 @param threadFactory 线程工厂(用于指定如果创建一个线程) @param handler 拒绝策略 (当任务太多导致工作队列满时的处理策略) 线程池的状态（高三位表示状态，第29位表示线程的数量）,ThreadPoolExecutor的ctl属性，三位，取值范围共计5个，特别注意，这一个变量是原子类型的，在谈论原子类型的使用的时候，可以拿来举例！，还要注意的是，只有在线程池的状态等于-1的时候不会产生中断，当状态大于等于0的时候将会产生中断。 RUNNING -1 接受新任务, 且处理已经进入阻塞队列的任务 SHUTDOWN 0 不接受新任务, 但处理已经进入阻塞队列的任务 STOP 1 接受新任务, 且不处理已经进入阻塞队列的任务, 同时中断正在运行的任务 TIDYING 2 所有任务都已终止, 工作线程数为0, 线程转化为TIDYING状态并准备调用terminated方法 TERMINATED 3 terminated方法已经执行完成 execute执行流程图 场景：今天是周末，一银行网点只开放了几个窗口，当前值班窗口的数量就是核心线程池的上限。一开始，顾客陆续进来，值班的窗口还有空闲的，则进来一个人就可以直接去柜台办理。随着人数的增加，当天值班窗口全部有人在办理业务，这个时候有人进来，就要在大厅里找个座位坐下来等待，大厅的座位就是阻塞队列。但是，当天人数越来越多，连大厅的座位都坐满了人，这个时候这个网点领导将会通知将当天休息的窗口也打开（所有的窗口数目就是总的线程池上限），当人数还继续增加的话，处于安全考虑，银行就会拒绝继续进入，这就是执行了拒绝策略。 ThreadPoolExecutor： 12&gt; &gt;private static ExecutorService executor = new ThreadPoolExecutor(13, 13,60L, TimeUnit.SECONDS,new ArrayBlockingQueue(13)); &gt; &gt; 使⽤ Executors 创建： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列⻓度为 Integer.MAX_VALUE,可能堆积⼤量的请求，从⽽导致OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ==，可能会创建⼤量线程，从⽽导致OOM。 guava提供的ThreadFactoryBuilder来创建线程池: 通过上述⽅式创建线程时，不仅可以避免OOM的问题，还可以⾃定义线程名称，更加⽅便的出错的时候溯源。 拒绝策略 | 编号 | 拒绝策略 | 解释 || :—- | :————————— | :—————————————————- || 1 | CallerRunsPolicy | 使用调用者所在的线程来执行任务 || 2 | AbortPolicy | 抛出异常 || 3 | DiscardPolicy | 默认丢弃，不抛出异常 || 4 | DiscardOldestPolicy | 调用poll抛弃一个任务，执行当前的任务 | 5.Java 并发包提供了哪些并发工具类？ juc-locks 锁框架 juc-locks锁框架中一共就三个接口：Lock、Condition、ReadWriteLock ReadWriteLock: 一个单独的接口（未继承Lock接口），该接口提供了获取读锁和写锁的方法。 ReentrantLock：ReentrantLock内部通过内部类实现了AQS框架(AbstractQueuedSynchronizer)的API来实现独占锁的功能。 ReentrantReadWriteLock：ReentrantReadWriteLock使得多个读线程同时持有读锁（只要写锁未被占用），而写锁是独占的。写锁可以降级成读锁，读锁不能升级成写锁。 juc-atomic 原子类框架（ J.U.C之atomic框架：Unsafe类） 其实底层就是通过Unsafe类实现的一种比较并交换的算法，大致的结构如下（具体入参，根据上下文有所不同）： boolean compareAndSet(expectedValue, updateValue); 当希望修改的值与expectedValue相同时，则尝试将值更新为updateValue，更新成功返回true，否则返回false。 Unsafe类，来源于sun.misc包。该类封装了许多类似指针操作，可以直接进行内存管理、操纵对象、阻塞/唤醒线程等操作。Java本身不直接支持指针的操作，所以这也是该类命名为Unsafe的原因之一。 juc-sync 同步器框架 juc-collections 集合框架 juc-executors 执行器框架 线程池 Future模式,Future接口仅仅定义了5个方法。 Fork/Join框架 (1)提供了比 synchronized 更加高级的各种同步结构 包括 CountDownLatch、CyclicBarrier、Sempahore 等，可以实现更加丰富的多线程操作，比如利用 Semaphore 作为资源控制器，限制同时进行工作的线程数量。 CountDownLatch，允许一个或多个线程等待某些操作完成 CyclicBarrier，一种辅助性的同步结构，允许多个线程等待到达某个屏障 CountDownLatch 是不可以重置的，所以无法重用；而 CyclicBarrier 则没有这种限制，可以重用。 CountDownLatch 的基本操作组合是 countDown/await。调用 await 的线程阻塞等待countDown 足够的次数，不管你是在一个线程还是多个线程里 countDown，只要次数足够即可。所以就像 Brain Goetz 说过的，CountDownLatch 操作的是事件。 CyclicBarrier 的基本操作组合，则就是 await，当所有的伙伴（parties）都调用了 await，才会继续进行任务，并自动进行重置。 Semaphore，Java 版本的信号量实现,总的来说，Semaphore 就是个计数器，其基本逻辑基于 acquire/release. (2)各种线程安全的容器 比如最常见的 ConcurrentHashMap、有序的ConcunrrentSkipListMap，或者通过类似快照机制，实现线程安全的动态数组CopyOnWriteArrayList 等。 Concurrent CopyOnWrite Blocking (3)并发队列实现 如各种 BlockedQueue 实现，比较典型的 ArrayBlockingQueue、SynchorousQueue 或针对特定场景的 PriorityBlockingQueue 等。 (4)强大的 Executor 框架 可以创建各种不同类型的线程池，调度任务运行等，绝大部分情况下，不再需要自己从头实现线程池和任务调度器 (5)AQS框架(AbstractQueuedSynchronizer抽象类) AQS利用了模板方法模式，其中大多数方法都是final或是private的，我们把这类方法称为Skeleton Method，也就是说这些方法是AQS框架自身定义好的骨架，子类是不能覆写的。 支持中断、超时 支持独占模式和共享模式 支持Condition条件等待 AQS方法说明: CAS操作 等待队列的核心操作 资源的获取操作 资源的释放操作 三个基本问题： 同步状态（synchronization state）的管理 阻塞/唤醒线程的操作 线程等待队列的管理 CLH队列 CLH队列中的结点是对线程的包装，结点一共有两种类型：独占（EXCLUSIVE）和共享（SHARED）。 每种类型的结点都有一些状态，其中独占结点使用其中的CANCELLED(1)、SIGNAL(-1)、CONDITION(-2)，共享结点使用其中的CANCELLED(1)、SIGNAL(-1)、PROPAGATE(-3)。 Node节点 6.哪些队列是有界的，哪些是无界的？从源码的角度，常见的线程安全队列是如何实现的，并进行了哪些改进以提高性能表现？ (1)有界or无界 ArrayBlockingQueue 是最典型的的有界队列，其内部以 final 的数组保存数据，数组的大小就决定了队列的边界，所以我们在创建 ArrayBlockingQueue 时，都要指定容量 LinkedBlockingQueue，容易被误解为无边界，但其实其行为和内部代码都是基于有界的逻辑实现的，只不过如果我们没有在创建队列时就指定容量，那么其容量限制就自动被设置为 Integer.MAX_VALUE，成为了无界队列。 SynchronousQueue，每个删除操作都要等待插入操作，反之每个插入操作也都要等待删除动作。那么这个队列的容量是多少呢？是 1 吗？其实不是的，其内部容量是 0。 PriorityBlockingQueue 是无边界的优先队列，虽然严格意义上来讲，其大小总归是要受系统资源影响 DelayedQueue 和 LinkedTransferQueue 同样是无边界的队列。 (2)安全? BlockingQueue 基本都是基于锁实现 类似 ConcurrentLinkedQueue 等，则是基于 CAS 的无锁技术，不需要在每个操作时使用锁，所以扩展性表现要更加优异 7.生产者 - 消费者? （1）传统版synchronized: sync———————-&gt;wait——————-&gt;notify （2）传统版lock： lock—————&gt;await—————&gt;Signal （3）阻塞队列+原子类 1234&gt; * 1 线程 操作 资源类&gt; * 2 判断 干活 通知&gt;* 3 虚假唤醒&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package JavaDemo.MultiThreadTest;/** * @Author MaoTian * @Classname ProducerConsumerSync * @Description TODO * @Date 上午8:48 2019/8/9 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class ShareSource&#123; private int number=0; public synchronized void increment()throws InterruptedException&#123; while (number!=0)&#123; this.wait(); &#125; ++number; System.out.println(Thread.currentThread().getName()+"\t"+number); this.notifyAll(); &#125; public synchronized void decrement()throws InterruptedException&#123; while (number==0)&#123; this.wait(); &#125; --number; System.out.println(Thread.currentThread().getName()+"\t"+number); this.notifyAll(); &#125;&#125;public class ProducerConsumerSync &#123; public static void main(String[] args) &#123; ShareSource shareSource=new ShareSource(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; shareSource.increment(); &#125;catch (Exception e)&#123; &#125; &#125; &#125;,"producer-1").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; shareSource.decrement(); &#125;catch (Exception e)&#123; &#125; &#125; &#125;,"consumer-1").start(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class ShareData&#123; private int number=0; private Lock lock=new ReentrantLock(); private Condition condition=lock.newCondition(); public void increment()throws Exception&#123; lock.lock(); try&#123; //判断,不能够使用if判断，必须使用while判断 while (number!=0)&#123; //等待 condition.await(); &#125; //干活 number++; System.out.println(Thread.currentThread().getName()+":"+number); //通知 condition.signalAll(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125; public void decrement()throws Exception&#123; lock.lock(); try&#123; //判断 while (number==0)&#123; //等待 condition.await(); &#125; //干活 number--; System.out.println(Thread.currentThread().getName()+":"+number); //通知 condition.signalAll(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125;&#125;public class ProducerConsumerTraditional &#123; public static void main(String[] args) &#123; ShareData shareData=new ShareData(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.increment(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"producer").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.decrement(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"consumer").start(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package JavaDemo.MultiThreadTest;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;/** * @Author MaoTian * @Classname ProducerConsumerBlockingQueue * @Description 使用阻塞队列，生产一个消费一个 * @Date 下午8:51 2019/8/8 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class Resource&#123; private volatile boolean FLAG=true; //可见性 private AtomicInteger atomicInteger=new AtomicInteger();//原子类 BlockingQueue&lt;String&gt; blockingQueue=null;//阻塞队列 public Resource(BlockingQueue&lt;String&gt; blockingQueue)&#123; this.blockingQueue=blockingQueue; System.out.println(blockingQueue.getClass().getName()); &#125; //生产者 public void myProd()throws Exception&#123; String data=null; boolean retvalue; while (FLAG)&#123; data=atomicInteger.incrementAndGet()+""; retvalue=blockingQueue.offer(data,2L, TimeUnit.SECONDS); if(retvalue)&#123; System.out.println(Thread.currentThread()+":insert ok "+data); &#125;else&#123; System.out.println(Thread.currentThread()+":insert fail"); &#125;// TimeUnit.SECONDS.sleep(1); &#125; System.out.println(Thread.currentThread()+":producer stop"); &#125; //消费者 public void myCons()throws Exception&#123; String result; while (FLAG)&#123; result=blockingQueue.poll(2L, TimeUnit.SECONDS); if(null==result||result.equalsIgnoreCase(""))&#123; FLAG=false; System.out.println(Thread.currentThread()+":consumer stop"); return; &#125; System.out.println(Thread.currentThread()+":consume ok "+result); &#125; &#125; public void stop()&#123; this.FLAG=false; &#125;&#125;public class ProducerConsumerBlockingQueue &#123; public static void main(String[] args) throws InterruptedException &#123; Resource resource=new Resource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+" producer start"); try &#123; resource.myProd(); &#125;catch (Exception e)&#123; &#125; &#125;,"producer").start(); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+" consumer start"); try &#123; resource.myCons(); &#125;catch (Exception e)&#123; &#125; &#125;,"consumer").start(); TimeUnit.SECONDS.sleep(5); resource.stop(); &#125;&#125; 8.synchronized和lock的区别？用lock的好处？ 区别 synchronized lock 原始构成 （关键字）jvm层面，底层通过monitor对象来完成，monitorenter和monitorexit（两个monitorexit） （具体类）Lock是具体的类（java.concurrent.locks.Lock）,是api层面的锁（使用java p） 使用方法 自动释放 需要使用try、finally释放 等待是否可中断 不可以被中断 可以被中断，lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 加锁是否公平 非公平锁 默认是非公平锁，构造函数传参，true公平，false非公平 锁绑定多个条件 没有 可以用来实现分组唤醒需要唤醒的线程，可以精确唤醒，synchronized随机唤醒一个或者多个 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @Author MaoTian * @Classname SyncAndLockCondition * @Description lock可以绑定多个condition，可以精确唤醒A-B-C-D-A * @Date 下午8:23 2019/8/8 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class ShareResource&#123; private int number=0; //A=1,B=2,C=3 // private Lock lock=new ReentrantLock(); private Condition c1=lock.newCondition(); private Condition c2=lock.newCondition(); private Condition c3=lock.newCondition(); // public void print_5()&#123; lock.lock(); try &#123; while (number!=0)&#123; c1.await(); &#125; for (int i = 0; i &lt;5 ; i++) &#123; System.out.println(Thread.currentThread()+":"+number); &#125; number=1; c2.signal(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125; // public void print_10()&#123; lock.lock(); try &#123; while (number!=1)&#123; c2.await(); &#125; for (int i = 0; i &lt;10 ; i++) &#123; System.out.println(Thread.currentThread()+":"+number); &#125; number=2; c3.signal(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125; public void print_15()&#123; lock.lock(); try &#123; while (number!=2)&#123; c3.await(); &#125; for (int i = 0; i &lt;15 ; i++) &#123; System.out.println(Thread.currentThread()+":"+number); &#125; number=0; c1.signal(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125;&#125;public class SyncAndLockCondition &#123; public static void main(String[] args) &#123; ShareResource shareResource=new ShareResource(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; shareResource.print_5(); &#125; &#125;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; shareResource.print_10(); &#125; &#125;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; shareResource.print_15(); &#125; &#125;).start(); &#125;&#125; 9.JUC主要包含的内容？透彻理解Java并发编程（1）概览 （2） 并发容器 CopyOnWrite*(List,Set) Concurrent*(SkipListSet,SkipListMap,Map,LinkedQueue) Blocking*(Queue, Deque)(Array,Linked,Priority) List，Set Map Queue CopyOnWriteArrayListCopyOnWriteArraySetConcurrentSkipListSet ConcurrentHashMapConcurrentSkipListMap ArrayBlockingQueueLinkedBlockingQueueConcurrentLinkedQueueConcurrentLinkedDeque 类 ji(Ctrl+H,Alt+7) 并发 CopyOnWriteArrayList Cloneable (java.lang)List (java.util) Collection (java.util) Iterable (java.lang)Object (java.lang)RandomAccess (java.util)Serializable (java.io) ReentrantLock,读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。CopyOnWriteArrayList 类的所有可变操作（add，set等等）都是通过创建底层数组的新副本来实现的。 CopyOnWriteArraySet AbstractSet (java.util) AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang) Set (java.util) Collection (java.util)Serializable (java.io) A {@link java.util.Set} that uses an internal {@link CopyOnWriteArrayList} for all of its operations. Thus, it shares the same basic properties* ReentrantLock ConcurrentSkipListSet AbstractSet (java.util) AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang) Set (java.util) Collection (java.util) Iterable (java.lang)Cloneable (java.lang)NavigableSet (java.util) SortedSet (java.util) Set (java.util) Collection (java.util) Iterable (java.lang)Serializable (java.io) ConcurrentHashMap AbstractMap (java.util) Map (java.util) Object (java.lang)ConcurrentMap (java.util.concurrent) Map (java.util)Serializable (java.io) JDK1.8:Node + CAS + Synchronized ConcurrentSkipListMap AbstractMap (java.util) Map (java.util) Object (java.lang)Cloneable (java.lang)ConcurrentNavigableMap (java.util.concurrent) ConcurrentMap (java.util.concurrent) Map (java.util) NavigableMap (java.util) SortedMap (java.util) Map (java.util)Serializable (java.io) 使用跳表实现Map 和使用哈希算法实现Map的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。JDK 中实现这一数据结构的类是ConcurrentSkipListMap。 ArrayBlockingQueue AbstractQueue (java.util) AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang) Queue (java.util) Collection (java.util) Iterable (java.lang)BlockingQueue (java.util.concurrent) Queue (java.util) Collection (java.util) Iterable (java.lang)Serializable (java.io) Lock+Condition LinkedBlockingQueue AbstractQueue (java.util) AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang) Queue (java.util) Collection (java.util) Iterable (java.lang)BlockingQueue (java.util.concurrent) Queue (java.util) Collection (java.util) Iterable (java.lang)Serializable (java.io) ArrayBlockingQueue与LinkedBlockingQueue的比较?相同点：ArrayBlockingQueue和LinkedBlockingQueue都是通过condition通知机制来实现可阻塞式插入和删除元素，并满足线程安全的特性；ArrayBlockingQueue底层是采用的数组进行实现,而LinkedBlockingQueue则是采用链表数据结构；不同点:ArrayBlockingQueue插入和删除数据，只采用了一个lock，而LinkedBlockingQueue则是在插入和删除分别采用了putLock和takeLock，这样可以降低线程由于线程无法获取到lock而进入WAITING状态的可能性，从而提高了线程并发执行的效率。 ConcurrentLinkedQueue AbstractQueue (java.util) AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang) Queue (java.util) Collection (java.util) Iterable (java.lang)Queue (java.util) Collection (java.util) Iterable (java.lang)Serializable (java.io) 阻塞队列的典型例子是 BlockingQueue,非阻塞队列的典型例子是ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。 无锁算法，底层基于自旋+CAS的方式实现。 ConcurrentLinkedDeque AbstractCollection (java.util) Collection (java.util) Iterable (java.lang) Object (java.lang)Deque (java.util) Queue (java.util) Collection (java.util) Iterable (java.lang)Serializable (java.io) CopyOnWrite*：只有两个 Concurrent* Blocking* （3）ConcurrentHashMap (注意，和HashMap相比较，实现上，ConcurrentHashMap节点种类是5种) 针对ConcurrentHashMap的讲解要分析以下的几个点（注意条理！） 构成上，分析五个Node Node TreeBin TreeNode ForwardingNode ReservationNode 操作上，分析： get方法采用了unsafe方法，来保证线程安全。 如果table[i]的key和待查找key相同，那直接返回； 如果table[i]对应的结点是特殊结点（hash值小于0），则通过find方法查找； 如果table[i]对应的结点是普通链表结点，则按链表方式查找。 put 首次初始化table —— 懒加载 table[i]对应的桶为空，最简单的情况，直接CAS操作占用桶table[i]即可。 发现ForwardingNode结点，说明此时table正在扩容，则尝试协助进行数据迁移 出现hash冲突,也就是table[i]桶中已经有了结点 table[i]的结点类型为Node——链表结点时，就会将新结点以“尾插法”的形式插入链表的尾部。 当table[i]的结点类型为TreeBin——红黑树代理结点时，就会将新结点通过红黑树的插入方式插入。 putVal方法的最后，涉及将链表转换为红黑树 —— treeifyBin ，但实际情况并非立即就会转换，当table的容量小于64时，static final int MIN_TREEIFY_CAPACITY = 64;出于性能考虑，只是对table数组扩容1倍——tryPresize 计数（分段计数） 扩容问题 table数组的扩容，一般就是新建一个2倍大小的槽数组，这个过程通过由一个单线程完成，且不允许出现并发。（时机：和负载因子有关系） 数据迁移 链表 红黑树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //不允许 key或value为null if (key == null || value == null) throw new NullPointerException(); //计算hash值 int hash = spread(key.hashCode()); int binCount = 0; //死循环 何时插入成功 何时跳出 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空的话，初始化table if (tab == null || (n = tab.length) == 0) tab = initTable(); //根据hash值计算出在table里面的位置 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有值 ，直接放进去，不需要加锁 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //当遇到表连接点时，需要进行整合表的操作 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; //结点上锁 这里的结点可以理解为hash值相同组成的链表的头结点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; //在这里遍历链表所有的结点 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果hash值和key值相同 则修改对应结点的value值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到了最后一个结点，那么就证明新的节点需要插入 就把它插入在链表尾部 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果这个节点是树节点，就按照树的方式插入值 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //如果链表长度已经达到临界值8 就需要把链表转换为树结构 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //将当前ConcurrentHashMap的元素数量+1 addCount(1L, binCount); return null;&#125; （4）原子类 AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 编号 名字 解释 1 基本类型 AtomicInteger：整型原子类 ，AtomicLong：长整型原子类 ，AtomicBoolean ：布尔型原子类 2 数组类型 AtomicIntegerArray：整型数组原子类， AtomicLongArray：长整型数组原子类， AtomicReferenceArray：引用类型数组原子类 3 引用类型 CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。AtomicReference：引用类型原子类,AtomicStampedReference：原子更新引用类型里的字段原子类AtomicMarkableReference：原子更新带有标记位的引用类型 4 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整型字段的更新器,AtomicLongFieldUpdater：原子更新长整型字段的更新器 引用类型 12345678910111213141516171819202122232425262728293031323334&gt;import java.util.concurrent.atomic.AtomicReference;&gt; public class AtomicReferenceTest &#123;&gt; public static void main(String[] args) &#123;&gt; AtomicReference&lt;Person&gt; ar = new AtomicReference&lt;Person&gt;();&gt; Person person = new Person("SnailClimb", 22);&gt; ar.set(person);&gt; Person updatePerson = new Person("Daisy", 20);&gt; ar.compareAndSet(person, updatePerson);&gt; System.out.println(ar.get().getName());&gt; System.out.println(ar.get().getAge());&gt; &#125;&gt; &#125;&gt;class Person &#123;&gt; private String name;&gt; private int age;&gt; public Person(String name, int age) &#123;&gt; super();&gt; this.name = name;&gt; this.age = age;&gt; &#125;&gt; public String getName() &#123;&gt; return name;&gt; &#125;&gt; public void setName(String name) &#123;&gt; this.name = name;&gt; &#125;&gt; public int getAge() &#123;&gt; return age;&gt; &#125;&gt; public void setAge(int age) &#123;&gt; this.age = age;&gt; &#125;&gt; &#125;&gt; （5）synchronized和ReentrantLock的比较？ReentrantLock可以在部分场合替代synchronized， （1）可以实现公平锁———进入队列就是公平，非公平就是抢锁是插队抢， （2）可以打断上锁的过程，可以使用trylock()或者lockInterruptibly （3）锁尚明的队列可以指定任意数量 （4）可以手动释放锁，lock.unlock. 10.为什么要使用多线程？ 先从总体上来说： 从计算机底层来说：线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 再深入到计算机底层来探讨： 单核时代： 在单核时代多线程主要是为了提高 CPU 和 IO 设备的综合利用率。举个例子：当只有一个线程的时候会导致 CPU 计算时，IO 设备空闲；进行 IO 操作时，CPU 空闲。我们可以简单地说这两者的利用率目前都是 50%左右。但是当有两个线程的时候就不一样了，当一个线程执行 CPU 计算时，另外一个线程可以进行 IO 操作，这样两个的利用率就可以在理想情况下达到 100%了。 多核时代: 多核时代多线程主要是为了提高 CPU 利用率。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，CPU 只会一个 CPU 核心被利用到，而创建多个线程就可以让多个 CPU 核心被利用到，这样就提高了 CPU 的利用率。 11.上下文切换？ 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换会这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 12.synchronized 关键字是什么？（一句话介绍）———&gt;用来做什么？（作用对象）———&gt;实现的原理是什么？（synchronized内存语义，锁状态及其转化） （1）说一说对synchronized关键字？synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。且为非公平锁。 另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 （1）synchronized 同步语句块的情况？ synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。** 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 （2）synchronized 修饰方法的的情况 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的是ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 （2）优化锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 编号 名字 解释 1 偏向锁 引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉 2 轻量级锁 轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 3 自旋锁和自适应自旋 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 4 锁消除 锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 5 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。大部分情况下，上面的原则都是没有问题的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，那么会带来很多不必要的性能消耗。 （3）sychronized C++实现源码Synchronized 本质是对一个对象监视器(monitor)进行获取：在Java虚拟机执行到monitorenter指令时，（1）首先它会尝试获取对象的锁，如果该对象没有锁，或者当前线程已经拥有了这个对象的锁时，它会把计数器+1；然后当执行到monitorexit 指令时就会将计数器-1；然后当计数器为0时，锁就释放了。（2）如果获取锁 失败，那么当前线程就要阻塞等待，直到对象锁被另一个线程释放为止。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128// monitorbool has_monitor() const &#123; return ((value() &amp; monitor_value) != 0); &#125; ObjectMonitor* monitor() const &#123; assert(has_monitor(), "check"); // Use xor instead of &amp;~ to provide one extra tag-bit check. return (ObjectMonitor*) (value() ^ monitor_value); &#125; ObjectMonitor() &#123; _header = NULL; //markOop对象头 _count = 0; _waiters = 0, //等待线程数 _recursions = 0; //线程重入次数 _object = NULL; //存储Monitor对象 _owner = NULL; //获得ObjectMonitor对象的线程 _WaitSet = NULL; //wait状态的线程列表 _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; // 单向列表 FreeNext = NULL ; _EntryList = NULL ; //处于等待锁BLOCKED状态的线程 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; _previous_owner_tid = 0; //监视器前一个拥有线程的ID &#125; // monitorenterIRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter(JavaThread* thread, BasicObjectLock* elem))#ifdef ASSERT thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);#endif if (PrintBiasedLockingStatistics) &#123; Atomic::inc(BiasedLocking::slow_path_entry_count_addr()); &#125; Handle h_obj(thread, elem-&gt;obj()); assert(Universe::heap()-&gt;is_in_reserved_or_null(h_obj()), "must be NULL or an object"); if (UseBiasedLocking) &#123; // Retry fast entry if bias is revoked to avoid unnecessary inflation ObjectSynchronizer::fast_enter(h_obj, elem-&gt;lock(), true, CHECK); &#125; else &#123;//绕过偏向锁，直接进入轻量级锁 ObjectSynchronizer::slow_enter(h_obj, elem-&gt;lock(), CHECK); &#125; assert(Universe::heap()-&gt;is_in_reserved_or_null(elem-&gt;obj()), "must be NULL or an object");#ifdef ASSERT thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);#endifIRT_END // monitorexitIRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorexit(JavaThread* thread, BasicObjectLock* elem))#ifdef ASSERT thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);#endif Handle h_obj(thread, elem-&gt;obj()); assert(Universe::heap()-&gt;is_in_reserved_or_null(h_obj()), "must be NULL or an object"); if (elem == NULL || h_obj()-&gt;is_unlocked()) &#123; THROW(vmSymbols::java_lang_IllegalMonitorStateException()); &#125; ObjectSynchronizer::slow_exit(h_obj(), elem-&gt;lock(), thread); // Free entry. This must be done here, since a pending exception might be installed on // exit. If it is not cleared, the exception handling code will try to unlock the monitor again. elem-&gt;set_obj(NULL);#ifdef ASSERT thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);#endifIRT_END // 偏向锁void ObjectSynchronizer::fast_enter(Handle obj, BasicLock* lock, bool attempt_rebias, TRAPS) &#123; if (UseBiasedLocking) &#123;//判断是否开启锁 if (!SafepointSynchronize::is_at_safepoint()) &#123;//如果不处于全局安全点 //通过`revoke_and_rebias`这个函数尝试获取偏向锁 BiasedLocking::Condition cond = BiasedLocking::revoke_and_rebias(obj, attempt_rebias, THREAD); if (cond == BiasedLocking::BIAS_REVOKED_AND_REBIASED) &#123;//如果是撤销与重偏向直接返回 return; &#125; &#125; else &#123;//如果在安全点，撤销偏向锁 assert(!attempt_rebias, "can not rebias toward VM thread"); BiasedLocking::revoke_at_safepoint(obj); &#125; assert(!obj-&gt;mark()-&gt;has_bias_pattern(), "biases should be revoked by now"); &#125; slow_enter (obj, lock, THREAD) ;&#125;// 轻量级🔒void ObjectSynchronizer::slow_enter(Handle obj, BasicLock* lock, TRAPS) &#123; markOop mark = obj-&gt;mark(); assert(!mark-&gt;has_bias_pattern(), "should not see bias pattern here"); if (mark-&gt;is_neutral()) &#123;//如果当前是无锁状态 //直接把mark保存到BasicLock对象的_displaced_header字段 lock-&gt;set_displaced_header(mark); //通过CAS将mark word更新为指向BasicLock对象的指针，更新成功表示获得了轻量级锁 if (mark == (markOop) Atomic::cmpxchg_ptr(lock, obj()-&gt;mark_addr(), mark)) &#123; TEVENT (slow_enter: release stacklock) ; return ; &#125; // Fall through to inflate() ... &#125; //如果markword处于加锁状态、且markword中的ptr指针指向当前线程的栈帧，表示为重入操作，不需要争抢锁 else if (mark-&gt;has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark-&gt;locker())) &#123; assert(lock != mark-&gt;locker(), "must not re-lock the same lock"); assert(lock != (BasicLock*)obj-&gt;mark(), "don't relock with same BasicLock"); lock-&gt;set_displaced_header(NULL); return; &#125; #if 0 // The following optimization isn't particularly useful. if (mark-&gt;has_monitor() &amp;&amp; mark-&gt;monitor()-&gt;is_entered(THREAD)) &#123; lock-&gt;set_displaced_header (NULL) ; return ; &#125;#endif //代码执行到这里，说明有多个线程竞争轻量级锁，轻量级锁通过`inflate`进行膨胀升级为重量级锁 lock-&gt;set_displaced_header(markOopDesc::unused_mark()); ObjectSynchronizer::inflate(THREAD, obj())-&gt;enter(THREAD);&#125; 当多个线程同时访问一段同步代码时，首先会进入 _EntryList 队列中，当某个线程获取到对象的 monitor 后进入 _Owner 区域并把 monitor 中的 _owner 变量设置为当前线程，同时 monitor 中的计数器 _count 加 1。即获得对象锁。 若持有 monitor 的线程调用 wait() 方法，将释放当前持有的 monitor，_owner 变量恢复为 null，_count 自减 1，同时该线程进入 _WaitSet 集合中等待被唤醒(等待 synchronized 锁的线程不可被中断，即使调用了该线程的 interrupt 方法。只有获取到锁之后才会中断。另外中断操作只是给线程一个标记，最终执行是看线程本身的状态)。 （4）openjdk官方文档jdk wiki https://wiki.openjdk.java.net/display/HotSpot/Synchronization The following figure shows the layout of the header word and the representation of different object states. 不同的锁状态 12. CAS（1）ABA问题 版本号解决，或者直接加一个Boolean类的标记 初看ABA问题像是没有什么问题？但是对于引用类型来说就有问题了，比如B线程修改了属性。 （2）比较和操作的原子性（3） CAS一直在消耗CPU一直在转圈，比如竞争就几个，循环OK。 （4）CAS和synchronized的效率 不同的场景效率不一样 synchronized有排序，但是涉及到cpu的一些列操作 12. UNSAFE Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类。 compareAndSwapInt AutomicInteger—-&gt;unsafe.getAndAddInt—-&gt;compareAndSwapInt—-&gt;(native)compareAndSwapInt—-&gt;unsafe.cpp—-&gt;Unsafe_CompareAndSwapInt——&gt;Atomic::cmpxchg—-&gt;asm的汇编代码lock comxchg,（comxchg汇编语句不保证原子性，lock指令支持锁总线） 13.AQS原理分析（AbstractQueuedSynchronizer） AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 ReentrangLock、Semaphore，它们的实现都用到了一个共同的基类—AbstractQueuedSynchronizer,简称AQS。 AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 AQS相当于CAS+volatile. 14.AQS组件 Semaphore(信号量)-允许多个线程同时访问：** synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 CountDownLatch （倒计时器）： CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 编号 CountDownLatch CyclicBarrier 1 减数方式 加数方式 2 countDown函数+await函数 await函数 3 计数为0以后，无法重置 计数达到指定的值后，从0重新开始 4 不可重复利用 可以重复利用 15.AQS对于资源的共享方式 Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 16.自定义同步器时需要重写下面几个AQS提供的模板方法12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 17.Future和FutureTask的区别？(Future等结果，FutureTask可以被提交) 线程的创建方式中有两种，一种是实现Runnable接口，另一种是继承Thread，但是这两种方式都有个缺点，那就是在任务执行完成之后无法获取返回结果，于是就有了Callable接口，Future接口与FutureTask类的配和取得返回的结果。FutureTask除了实现了Future接口外还实现了Runnable接口,Future接口是用来获取异步计算结果的.无论是Runnable接口的实现类还是Callable接口的实现类，都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行 Future模式是Java多线程设计模式中的一种常见模式，它的主要作用就是异步地执行任务，并在需要的时候获取结果。我们知道，一般调用一个函数，需要等待函数执行完成，调用线程才会继续往下执行，如果是一些计算密集型任务，需要等待的时间可能就会比较长。 123&gt;&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&gt; &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);&gt; 123456789&gt; //创建线程池 &gt; ExecutorService es = Executors.newSingleThreadExecutor(); &gt;//创建Callable对象任务 &gt; CallableDemo calTask=new CallableDemo(); &gt;//提交任务并获取执行结果 &gt; Future&lt;Integer&gt; future =es.submit(calTask); &gt;//关闭线程池 &gt; es.shutdown(); &gt; 1234567891011&gt; //创建线程池 &gt; ExecutorService es = Executors.newSingleThreadExecutor(); &gt; //创建Callable对象任务 &gt; CallableDemo calTask=new CallableDemo(); &gt; //创建FutureTask &gt; FutureTask&lt;Integer&gt; futureTask=new FutureTask&lt;&gt;(calTask); &gt; //执行任务 &gt; es.submit(futureTask); &gt; //关闭线程池 &gt; es.shutdown(); &gt; 18.FutureTask的7种状态 FutureTask的字段定义非常简单，State标识任务的当前状态，状态之间的转换通过Unsafe来操作，所有操作都基于自旋+CAS完成： 编号 状态 解释 1 new 表示任务的初始化状态； 2 canceled 表示任务还没开始执行就被取消（非中断方式），属于最终状态； 3 interrupting 表示任务还没开始执行就被取消（中断方式），正式被中断前的过渡状态，属于中间状态； 4 interrupted 表示任务还没开始执行就被取消（中断方式），且已被中断，属于最终状态。 5 completing 表示任务已执行完成（正常完成或异常完成），但任务结果或异常原因还未设置完成，属于中间状态； 6 exceptional 表示任务已经执行完成（异常完成），且任务异常已设置完成，属于最终状态； 7 normal 表示任务已经执行完成（正常完成），且任务结果已设置完成，属于最终状态； 19.Java中的锁（1）乐观锁，悲观锁 编号 类别 解释 实现机制 1 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁一般会使用版本号机制或CAS算法实现。（提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略） 2 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 CAS的问题？ ABA 问题 循环时间长开销大 只能保证一个共享变量的原子操作 （2）condition实现的原理？20.线程同步和互斥 互斥是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。 同步是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。 同步其实已经实现了互斥，所以同步是一种更为复杂的互斥。 互斥是一种特殊的同步。 21.Java如何实现原子操作？在Java中可以通过锁和循环CAS的方式来实现原子操作。 22.Java中AutomicInteger如何实现的？ 总结一下，AtomicInteger 中主要实现了整型的原子操作，防止并发情况下出现异常结果，其内部主要依靠JDK 中的unsafe 类操作内存中的数据来实现的。volatile 修饰符保证了value在内存中其他线程可以看到其值得改变。CAS操作保证了AtomicInteger 可以安全的修改value 的值。 23.HashMap为何可能出现环？ https://blog.csdn.net/hhx0626/article/details/54024222 24.为什么要使用线程池？ b降低资源消耗。 通过重复利⽤已创建的线程降低线程创建和销毁造成的消耗。 提⾼响应速度。 当任务到达时，任务可以不需要的等到线程创建就能⽴即执⾏。 提⾼线程的可管理性。 线程是稀缺资源，如果⽆限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使⽤线程池可以进⾏统⼀的分配，调优和监控. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * returns workersExecutorService. * * returns the service stored in the appContext or creates it if * necessary. * * @return ExecutorService for the &#123;@code SwingWorkers&#125; */private static synchronized ExecutorService getWorkersExecutorService() &#123; final AppContext appContext = AppContext.getAppContext(); ExecutorService executorService = (ExecutorService) appContext.get(SwingWorker.class); if (executorService == null) &#123; //this creates daemon threads. ThreadFactory threadFactory = new ThreadFactory() &#123; final ThreadFactory defaultFactory = Executors.defaultThreadFactory(); public Thread newThread(final Runnable r) &#123; Thread thread = defaultFactory.newThread(r); thread.setName("SwingWorker-" + thread.getName()); thread.setDaemon(true); return thread; &#125; &#125;; executorService = new ThreadPoolExecutor(MAX_WORKER_THREADS, MAX_WORKER_THREADS, 10L, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory); appContext.put(SwingWorker.class, executorService); // Don't use ShutdownHook here as it's not enough. We should track // AppContext disposal instead of JVM shutdown, see 6799345 for details final ExecutorService es = executorService; appContext.addPropertyChangeListener(AppContext.DISPOSED_PROPERTY_NAME, new PropertyChangeListener() &#123; @Override public void propertyChange(PropertyChangeEvent pce) &#123; boolean disposed = (Boolean)pce.getNewValue(); if (disposed) &#123; final WeakReference&lt;ExecutorService&gt; executorServiceRef = new WeakReference&lt;ExecutorService&gt;(es); final ExecutorService executorService = executorServiceRef.get(); if (executorService != null) &#123; AccessController.doPrivileged( new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; executorService.shutdown(); return null; &#125; &#125; ); &#125; &#125; &#125; &#125; ); &#125; return executorService;&#125; 25.如何线程安全的实现一个计数器？26.volatile关键字？参考：volatile: http://ifeve.com/volatile/ Java语言规范第三版中对volatile的定义如下： java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。Volatile变量修饰符如果使用恰当的话，它比synchronized的使用和执行成本会更低，因为它不会引起线程上下文的切换和调度。 Java内存模型 内存屏障 27. Thread 类中的start() 和 run() 方法有什么区别？start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。 当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。 创建线程的方式： 方式1：继承Java.lang.Thread类，并覆盖run() 方法。 优势：编写简单； 劣势：单继承的限制——无法继承其它父类，同时不能实现资源共享。 方式2：实现Java.lang.Runnable接口，并实现run()方法。 优势：可继承其它类，多线程可共享同一个Thread对象； 劣势：编程方式稍微复杂，如需访问当前线程，需调用Thread.currentThread()方法 28. 线程池大小 计算密集型：为什么是核心数+1？对于计算密集型的任务，在拥有N个处理器的系统上，当线程池的大小为N+1时，通常能实现最优的效率。(即使当计算密集型的线程偶尔由于缺失故障或者其他原因而暂停时，这个额外的线程也能确保CPU的时钟周期不会被浪费。) IO密集型：核心数*2 29. 什么是可重入锁？实现的原理是什么？是什么：当一个线程需要获取一个被其他线程锁持有的独占锁的时候，会被阻塞。当一个线程需要获取到自己已经持有的锁的时候是否也会被阻塞呢？如果不被阻塞，那么我们说这个锁是可以重入的，也就是说这个线程获取了这个锁就可以无限次（严格来说是有限次）进入被锁定的代码。 实现的原理：可重入锁实现的原理就是内部维护一个线程标识，使用这个标识来确定当前的锁被哪一个线程占用，然后还需要关联一个计数器。一开始，这个计数器为0，说明这个锁没有被任何线程占用。当一个线程获取到这个锁的时候，计数器的值将会加一，其他线程过来的时候发现持有锁的线程不是自己而被阻塞。如果发现是自己，计数器将会被加一，当释放计数器的时候减一，当计数器为零的时候，锁里面的线程标识重置为null。 30. 生产者和消费者问题？生产者生产满了就阻塞，消费者消费完了就阻塞，notify和notifyAll是如何实现的？ 使用Condition去唤醒不同的队列？ 31. ABCABC问题使用Condition 12345678910111213141516public class Tset&#123; public void main(String[] argc)&#123; // 思路：三个线程。三个不同的队列Condition，A唤醒B，B唤醒C，C唤醒A new Thread(()-&gt;&#123; &#125;, "Thread A").start(); new Thread(()-&gt;&#123; &#125;, "Thread B").start(); new Thread(()-&gt;&#123; &#125;, "Thread C").start(); &#125;&#125; ps: t.start()会导致run()方法被调用，run()方法中的内容称为线程体，它就是这个线程需要执行的工作。 用start()来启动线程，实现了真正意义上的启动线程，此时会出现异步执行的效果，即在线程的创建和启动中所述的随机性。而如果使用run()来启动线程，就不是异步执行了，而是同步执行，不会达到使用线程的意义. 32. 有多个任务，一个执行错误，其他都取消（分布式事务）33. ReentrantLock33.0 重点(1)类图，(2)state, (3)子类实现，(4)ConditionObject lock()——-&gt;compareAndSetState——&gt;setExclusiveOwnerThread ​ ———&gt;acquire———&gt;tryAcquire———&gt;acquireQueued（这个方法里有死循环） lock锁缓存行 33.1 如何实现互斥的？使用cas操作state. 33.2 如何和实现重入的？使用cas操作AQS的state，如果是当前锁的持有者，state会增加，表示重入的次数，如果哦不是当前线程的持有者，当前线程会被挂起放入到AQS队列中阻塞。 33.3阻塞队列是不是公平的？34. Disruptor 环形队列 35. Condition 使用condition的步骤：lock.newCondition创建condition对象，获取锁，然后调用condition的方法 一个ReentrantLock支持多个condition对象 void await() throws InterruptedException;方法会释放锁，让当前线程等待，支持唤醒，支持线程中断 void awaitUninterruptibly();方法会释放锁，让当前线程等待，支持唤醒，不支持线程中断 long awaitNanos(long nanosTimeout) throws InterruptedException;参数为纳秒，此方法会释放锁，让当前线程等待，支持唤醒，支持中断。超时之后返回的，结果为负数；超时之前返回的，结果为正数（表示返回时距离超时时间相差的纳秒数） boolean await(long time, TimeUnit unit) throws InterruptedException;方法会释放锁，让当前线程等待，支持唤醒，支持中断。超时之后返回的，结果为false；超时之前返回的，结果为true boolean awaitUntil(Date deadline) throws InterruptedException;参数表示超时的截止时间点，方法会释放锁，让当前线程等待，支持唤醒，支持中断。超时之后返回的，结果为false；超时之前返回的，结果为true void signal();会唤醒一个等待中的线程，然后被唤醒的线程会被加入同步队列，去尝试获取锁 void signalAll();会唤醒所有等待中的线程，将所有等待中的线程加入同步队列，然后去尝试获取锁 36. LockSupport类介绍36.1 原理LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程。主要是通过park()和unpark(thread)方法来实现阻塞和唤醒线程的操作的。 每个线程都有一个许可(permit)，permit只有两个值1和0，默认是0。 当调用unpark(thread)方法，就会将thread线程的许可permit设置成1(注意多次调用unpark方法，不会累加，permit值还是1**)。 当调用park()方法，如果当前线程的permit是1，那么将permit设置为0，并立即返回。如果当前线程的permit是0，那么当前线程就会阻塞，直到别的线程将当前线程的permit设置为1时，park方法会被唤醒，然后会将permit再次设置为0，并返回。 注意：因为permit默认是0，所以一开始调用park()方法，线程必定会被阻塞。调用unpark(thread)方法后，会自动唤醒thread线程，即park方法立即返回。 36.2 LockSupport中常用的方法 阻塞线程 • void park()：阻塞当前线程，如果调用unpark**方法或者当前线程被中断**，从能从park()方法中返回 • void park(Object blocker)：功能同方法1，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查 • void parkNanos(long nanos)：阻塞当前线程，最长不超过nanos纳秒，增加了超时返回的特性 • void parkNanos(Object blocker, long nanos)：功能同方法3，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查 • void parkUntil(long deadline)：阻塞当前线程，直到deadline，deadline是一个绝对时间，表示某个时间的毫秒格式 • void parkUntil(Object blocker, long deadline)：功能同方法5，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查； 唤醒线程 • void unpark(Thread thread):唤醒处于阻塞状态的指定线程 37.线程唤醒的方法 方式1：Object中的wait、notify、notifyAll方法 方式2：juc中Condition接口提供的await、signal、signalAll方法 方式3：juc中的LockSupport提供的park、unpark方法 3种方式对比： Object Condtion LockSupport 前置条件 需要在synchronized中运行 需要先获取Lock的锁 无 无限等待 支持 支持 支持 超时等待 支持 支持 支持 等待到将来某个时间返回 不支持 支持 支持 等待状态中释放锁 会释放 会释放 不会释放 唤醒方法先于等待方法执行，能否唤醒线程 否 否 可以 是否能响应线程中断 是 是 是 线程中断是否会清除中断标志 是 是 否 是否支持等待状态中不响应中断 不支持 支持 不支持 38. 并发队列 线程池阻塞队列的设置： 无界队列 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 有界队列 常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 同步移交队列 如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 Summary of BlockingQueue methods Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable （四）JVM1.虚拟机的参数 编号 参数 含义 1 -XX:PrintFlagsInitial 打印默认的配置信息 2 -Xms8m 堆空间8m 3 -Xmx8x 堆空间最大8m 4 5 -XX:MaxDirectMemorySize=5m 最大直接内存5m 6 -XX:MetaspaceSize=8m -XX:MaxMetaspaceSize=8m 指定元空间的大小8m 7 -XX:MaxTenuringThreshold 对象晋升到老年代的年龄阈值 8 -XX:PreBlockSpin 自旋次数的默认值是10次 2.JVM内存区域（运行时数据区） 编号 名字 功能 备注 1 堆 主要用于存放新创建的对象 (所有对象都在这里分配内存) jdk1.8之后永久代被替换成为了元空间（Metaspace） 2 方法区 被虚拟机加载的类信息(版本、字段、方法、接口)、常量、静态变量、即时编译器编译后的代码等数据（加常静即） 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）方法中的基本类型本地变量将直接存储在工作内存的栈帧结构中； 3 虚拟机栈（线程私有） 动态链接，方法出口，操作数栈，局部变量表（动方操局） 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 4 本地方法栈（线程私有） 区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 5 程序计数器（线程私有） 程序计数器主要有下面两个作用：（1）字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。在多线程的情况下，（2）程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 堆的细节信息（使用虚拟机参数-XX:+PrintGCDetails） Heap PSYoungGen total 74752K, used 3871K [0x000000076d180000, 0x0000000772480000, 0x00000007c0000000) eden space 64512K, 6% used [0x000000076d180000,0x000000076d547c70,0x0000000771080000) from space 10240K, 0% used [0x0000000771a80000,0x0000000771a80000,0x0000000772480000) to space 10240K, 0% used [0x0000000771080000,0x0000000771080000,0x0000000771a80000) ParOldGen total 171008K, used 0K [0x00000006c7400000, 0x00000006d1b00000, 0x000000076d180000) object space 171008K, 0% used [0x00000006c7400000,0x00000006c7400000,0x00000006d1b00000) Metaspace used 3009K, capacity 4496K, committed 4864K, reserved 1056768K class space used 330K, capacity 388K, committed 512K, reserved 1048576K 当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。 3.直接内存也会爆出OutOfMemoryError 直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。 JDK1.4中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用Native函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。（demo：修改虚拟机参数，使用allocateDirect方法直接分配直接内存的空间） 4.对象的创建过程？加分初设执, 双亲委派模型 （1）类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 类加载过程（加验准解初） 加载 protected Class&lt;?&gt; loadClass(String name, boolean resolve)是线程安全的！ 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的Class 对象,作为方法区这些数据的访问入口 验证（文元字符） 文件格式验证 元数据验证 字节码验证 符号引用验证 准备：准备阶段是正式为类变量分配内存并设置类变量初始值的阶段 解析：解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 初始化：初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 &lt;clinit&gt; ()方法的过程。 加载器 BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。 每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果不用没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 （2）分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表”两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 （3）初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 （4）设置对象头（元数据信息，对象的哈希码，对象的GC分代信息） 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 （5）执行init方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 5.JVM常见面试题（1）如何判断对象是否死亡（两种方法）? 引用计数法（和引用有关系） 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 可达性分析（和引用有关系） 这个算法的基本思想就是通过一系列的称为GC Roots的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 （2）简单的介绍一下强引用、软引用、弱引用、虚引用 强引用 我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空 间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 软引用 如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 弱引用 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 虚引用 与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 （3）如何判断一个常量是废弃常量?假如在常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 （4）如何判断一个类是无用的类?方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。(双亲委派模型) 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 （5）垃圾收集有哪些算法，各自的特点？ 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 标记清除 该算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 标记整理 根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 分代 （6）HotSpot 为什么要分为新生代和老年代？ 根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 分代收集算法（HotSpot 虚拟机 GC 采用分代收集算法） 根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，根据年代的特点来选择最佳的收集算法。 新生代：复制算法 老年代：标记-整理算法 堆大小=新生代+老年代（默认分别占堆空间为1/3、2/3），新生代又被分为Eden、from survivor、to survivor（默认8:1:1） 这样划分是为了更好的管理堆内存中的对象，方便 GC 算法来进行垃圾回收。 对象的分配通常在 Eden 中（大对象（需要大量连续内存空间的 Java 对象，如很长的字符串或数据）直接进入老年代，-XX:PretenureSizeThreshold）。 当 Eden 区满后，会触发 Minor GC，把 Eden 区和 from survivor 区中存活的对象进行转移，其中到达年龄（经过多次Minor GC）的会被放入老年代，未到达年龄的放入 to survivor 区。 然后清空 Eden 区和 from survivor 区，交换 from survivor 与 to survivor 的名字。 若存活对象大于 to survivor 区容量，则会被直接放入老年代。若打开了自适应（-XX:+AdaptiveSizePolicy），GC会自动重新调整新生代大小。 若老年代满了，则触发 Full GC。 Minor GC vs Major GC/Full GC： Minor GC：回收新生代（包括 Eden 和 Survivor 区域），因为 Java 对象大多都具备朝生夕灭的特性，所以 Minor GC 非常频繁，一般回收速度也比较快。 Major GC / Full GC: 回收老年代，出现了 Major GC，经常会伴随至少一次的 Minor GC，但这并非绝对。Major GC 的速度一般会比 Minor GC 慢 10 倍 以上。 在 JVM 规范中，Major GC 和 Full GC 都没有一个正式的定义，所以有人也简单地认为 Major GC 清理老年代，而 Full GC 清理整个内存堆。 （7）常见的垃圾回收器有那些？ 编号 回收器 算法 步骤 备注 1 Serial 收集器（串行） 单CPU的client模式 2 ParNew 收集器（并行） 它是许多运行在Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器配合工作。 3 Parallel Scavenge 收集器（并行） （吞吐量）Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU），后台运行不需要太多交互 4 Serial Old 收集器（串行） 单CPU的client模式，和CMS配合 5 Parallel Old 收集器（并行） （吞吐量）后台运行不需要太多交互 6 CMS 收集器（并发） 标记清除 （1）初始标记（2）并发标记（混合）（3）重新标记（4）并发清除（混合） CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验），集中使用在互联网或者B/S系统服务端 7 G1 收集器（并发） 面向服务端，将来替换CMS （8）Minor Gc 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 （9）什么时候触发FULL GC？除直接调用System.gc外，触发Full GC执行的情况有如下四种。 旧生代空间不足旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出如下错误： java.lang.OutOfMemoryError: Java heap space 为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。 Permanet Generation空间满 PermanetGeneration中存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Permanet Generation可能会被占满，在未配置为采用CMS GC的情况下会执行Full GC。如果经过Full GC仍然回收不了，那么JVM会抛出如下错误信息： java.lang.OutOfMemoryError: PermGen space 为避免Perm Gen占满造成Full GC现象，可采用的方法为增大Perm Gen空间或转为使用CMS GC。 CMS GC时出现promotion failed和concurrent mode failure 对于采用CMS进行旧生代GC的程序而言，尤其要注意GC日志中是否有promotion failed和concurrent mode failure两种状况，当这两种状况出现时可能会触发Full GC。 promotionfailed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。 应对措施为：增大survivorspace、旧生代空间或调低触发并发GC的比率，但在JDK 5.0+、6.0+的版本中有可能会由于JDK的bug29导致CMS在remark完毕后很久才触发sweeping动作。对于这种状况，可通过设置-XX:CMSMaxAbortablePrecleanTime=5（单位为ms）来避免。 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间 这是一个较为复杂的触发情况，Hotspot为了避免由于新生代对象晋升到旧生代导致旧生代空间不足的现象，在进行Minor GC时，做了一个判断，如果之前统计所得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间，那么就直接触发Full GC。 例如程序第一次触发MinorGC后，有6MB的对象晋升到旧生代，那么当下一次Minor GC发生时，首先检查旧生代的剩余空间是否大于6MB，如果小于6MB，则执行Full GC。 当新生代采用PSGC时，方式稍有不同，PS GC是在Minor GC后也会检查，例如上面的例子中第一次Minor GC后，PS GC会检查此时旧生代的剩余空间是否大于6MB，如小于，则触发对旧生代的回收。 除了以上4种状况外，对于使用RMI来进行RPC或管理的Sun JDK应用而言，默认情况下会一小时执行一次Full GC。可通过在启动时通过- java-Dsun.rmi.dgc.client.gcInterval=3600000来设置Full GC执行的间隔时间或通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。 （10）对象分配规则 对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 6.JDK 监控和故障处理工具总结故障排查：https://www.cnblogs.com/stateis0/p/9062196.html 线上 CPU 飚高问题大家应该都遇到过，那么如何定位问题呢？ 思路：首先找到 CPU 飚高的那个 Java 进程，因为你的服务器会有多个 JVM 进程。然后找到那个进程中的 “问题线程”，最后根据线程堆栈信息找到问题代码。最后对代码进行排查,如何操作呢？ 通过 top 命令找到 CPU 消耗最高的进程，并记住进程 ID。 再次通过 top -Hp [进程 ID] 找到 CPU 消耗最高的线程 ID，并记住线程 ID. 通过 JDK 提供的 jstack 工具 dump 线程堆栈信息到指定文件中。具体命令：jstack -l [进程 ID] &gt;jstack.log。 由于刚刚的线程 ID 是十进制的，而堆栈信息中的线程 ID 是16进制的，因此我们需要将10进制的转换成16进制的，并用这个线程 ID 在堆栈中查找。使用 printf “%x\n” [十进制数字] ，可以将10进制转换成16进制。 通过刚刚转换的16进制数字从堆栈信息里找到对应的线程堆栈。就可以从该堆栈中看出端倪。 编号 工具 解释 用法 1 jps (JVM Process Status）: 类似 UNIX 的 ps 命令。用户查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； 2 jinfo (Configuration Info for Java) : Configuration Info forJava,显示虚拟机配置信息; （1）jinfo -flag MaxHeapSize 3 jstat (JVM Statistics Monitoring Tool): 用于收集 HotSpot 虚拟机各方面的运行数据; 4 jmap (Memory Map for Java) :生成堆转储快照; （1）可以使用-XX:+HeapDumpOnOutOfMemoryError代替 5 jhat (JVM Heap Dump Browser ) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; 6 jstack (Stack Trace for Java):生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 7 jconsole 8 javap 查看字节码 7.类文件结构（魔文常访当字方属） 编号 数据 解释 1 魔数 每个 Class 文件的头四个字节称为魔数（Magic Number）,它的唯一作用是确定这个文件是否为一个能被虚拟机接收的 Class 文件。 2 Class文件版本 高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致。 3 常量池 常量池主要存放两大常量：字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。javap -v *.class查看 4 访问标志 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等。 5 当前类索引，父类索引和接口索引集合 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按implents(如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。 6 字段表集合 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。 7 方法表集合 Class 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。 8 属性表集合 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。 8.常量池存放（字面量，符号引用）（1）字面量 字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。 （2）符号引用(全限定名，描述符) 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 9.自定义加载器(不要轻易覆盖loadClass) 双亲委派模型是为了保证 如果加载的类是一个系统类，那么会优先由Bootstrap ClassLoader 、Extension ClassLoader先去加载，而不是使用我们自定义的ClassLoader去加载，保证系统的安全！ 123&gt; URLClassLoader v2 = new URLClassLoader(new URL[]&#123;new URL(v2dir)&#125;);&gt; Class&lt;?&gt; depv1Class = v1.loadClass("JavaDemo.VMTest.DiamondDependency.v1.Dep");&gt; 为什么要自定义ClassLoader？ 因为系统的ClassLoader只会加载指定目录下的class文件,如果你想加载自己的class文件,那么就可以自定义一个ClassLoader。而且我们可以根据自己的需求，对class文件进行加密和解密。有很多字节码加密技术就是依靠定制 ClassLoader 来实现的。先使用工具对字节码文件进行加密，运行时使用定制的 ClassLoader 先解密文件内容再加载这些解密后的字节码。 如何自定义ClassLoader？（findClass———&gt;defineClass———&gt;loadClass） 新建一个类继承自java.lang.ClassLoader,重写它的findClass方法。 将class字节码数组转换为Class类的实例（这点需要和判断一个类是不是没用建立联系） 使用：调用loadClass方法即可（这点要和判断一个类是不是没用建立联系） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&gt;package JavaDemo.VMTest.ClassLoaderDemo;&gt; &gt; import java.io.*;&gt; &gt;public class MyClassLoader extends ClassLoader &#123;&gt; //指定路径&gt; private String path ;&gt; &gt; public MyClassLoader(String classPath)&#123;&gt; path=classPath;&gt; &#125;&gt; /**&gt; * 重写findClass方法&gt; * @param name 是我们这个类的全路径&gt; * @return&gt; * @throws ClassNotFoundException&gt; */&gt; @Override&gt; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123;&gt; Class log = null;&gt; // 获取该class文件字节码数组&gt; byte[] classData = getData();&gt; &gt; if (classData != null) &#123;&gt; // 将class的字节码数组转换成Class类的实例&gt; log = defineClass(name, classData, 0, classData.length);&gt; &#125;&gt; return log;&gt; &#125;&gt; &gt; /**&gt; * 将class文件转化为字节码数组&gt; * @return&gt; */&gt; private byte[] getData() &#123;&gt; File file = new File(path);&gt; if (file.exists())&#123;&gt; FileInputStream in = null;&gt; ByteArrayOutputStream out = null;&gt; try &#123;&gt; in = new FileInputStream(file);&gt; out = new ByteArrayOutputStream();&gt; &gt; byte[] buffer = new byte[1024];&gt; int size = 0;&gt; while ((size = in.read(buffer)) != -1) &#123;&gt; out.write(buffer, 0, size);&gt; &#125;&gt; &gt; &#125; catch (IOException e) &#123;&gt; e.printStackTrace();&gt; &#125; finally &#123;&gt; try &#123;&gt; in.close();&gt; &#125; catch (IOException e) &#123;&gt; e.printStackTrace();&gt; &#125;&gt; &#125;&gt; return out.toByteArray();&gt; &#125;else&#123;&gt; return null;&gt; &#125;&gt; &#125;&gt; &#125;&gt; 12345678&gt; package JavaDemo.VMTest.ClassLoaderDemo;&gt; &gt; public class Log &#123;&gt; public static void main(String[] args) &#123;&gt; System.out.println("load Log class successfully");&gt; &#125;&gt; &#125;&gt; 12345678910111213141516171819202122232425262728&gt;package JavaDemo.VMTest.ClassLoaderDemo;&gt; &gt;import java.lang.reflect.InvocationTargetException;&gt; import java.lang.reflect.Method;&gt;&gt; public class ClassLoaderMain &#123;&gt; public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, SecurityException, IllegalArgumentException, InvocationTargetException, InvocationTargetException &#123;&gt; //这个类class的路径&gt; String classPath = "/home/mao/workspace/java/out/production/java/JavaDemo/VMTest/ClassLoaderDemo/Log.class";&gt; &gt; MyClassLoader myClassLoader = new MyClassLoader(classPath);&gt; &gt; //类的全称&gt; String packageNamePath = "JavaDemo.VMTest.ClassLoaderDemo.Log";&gt; //加载Log这个class文件&gt; Class&lt;?&gt; Log = myClassLoader.loadClass(packageNamePath);&gt;&gt; System.out.println("类加载器是:" + Log.getClassLoader());&gt;&gt; //利用反射获取main方法&gt; Method method = Log.getDeclaredMethod("main", String[].class);&gt; Object object = Log.newInstance();&gt; String[] arg = &#123;"ad"&#125;;&gt; method.invoke(object, (Object) arg);&gt; &#125;&gt; &#125;&gt; &gt; 10.钻石依赖问题 项目管理上有一个著名的概念叫着「钻石依赖」，是指软件依赖导致同一个软件包的两个版本需要共存而不能冲突。ClassLoader固然可以解决依赖冲突问题，不过它也限制了不同软件包的操作界面必须使用反射或接口的方式进行动态调用。 1234567package JavaDemo.VMTest.DiamondDependency.v1;public class Dep &#123; public void print() &#123; System.out.println("v1"); &#125;&#125; 1234567package JavaDemo.VMTest.DiamondDependency.v2;public class Dep &#123; public void print() &#123; System.out.println("v2"); &#125;&#125; 12345678910111213141516171819202122232425262728293031package JavaDemo.VMTest.DiamondDependency;import java.lang.reflect.InvocationTargetException;import java.net.MalformedURLException;import java.net.URL;import java.net.URLClassLoader;public class DiamondDependencyTest &#123; public static void main(String[] args) throws MalformedURLException, ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; String v1dir = "file:///home/mao/workspace/java/out/production/java/JavaDemo/VMTest/DiamondDependency/v1/"; String v2dir = "file:///home/mao/workspace/java/out/production/java/JavaDemo/VMTest/DiamondDependency/v2/"; URLClassLoader v1 = new URLClassLoader(new URL[]&#123;new URL(v1dir)&#125;); URLClassLoader v2 = new URLClassLoader(new URL[]&#123;new URL(v2dir)&#125;); Class&lt;?&gt; depv1Class = v1.loadClass("JavaDemo.VMTest.DiamondDependency.v1.Dep"); Object depv1 = depv1Class.getConstructor().newInstance(); depv1Class.getMethod("print").invoke(depv1); Class&lt;?&gt; depv2Class = v2.loadClass("JavaDemo.VMTest.DiamondDependency.v2.Dep"); Object depv2 = depv2Class.getConstructor().newInstance(); depv2Class.getMethod("print").invoke(depv2); System.out.println(depv1Class.equals(depv2Class)); &#125;&#125; 11.JMM Java内存模型简称JMM（Java Memory Model），是Java虚拟机所定义的一种抽象规范，用来屏蔽不同硬件和操作系统的内存访问差异，让java程序在各种平台下都能达到一致的内存访问效果。 具体说来，JVM中存在一个主存区（Main Memory或Java Heap Memory），对于所有线程进行共享，而每个线程又有自己的工作内存（Working Memory，实际上是一个虚拟的概念），工作内存中保存的是主存中某些变量的拷贝，线程对所有变量的操作并非发生在主存区，而是发生在工作内存中，而线程之间是不能直接相互访问的，变量在程序中的传递，是依赖主存来完成的。 JMM描述的是一组规则，围绕原子性、有序性和可见性展开； 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 操作： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占的状态。 unclock（解锁）：作用于主内存的变量，把一个处于锁定的状态释放出来。 read（读取）：作用于主内存的变量，把一个变量的值从主内存传输到线程的工作内存中 load（载入）：作用于工作内存的变量，把read操作从主内存 得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的 值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，把一个从执行引擎接收到的值 赋值给工作内存的变量，每当虚拟机遇到一个给变 量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传递到主内存，以便write操作使用。 write（写入）：作用于主内存的变量，把store操作从工作内存中得到的变量的值放入主内存的变量中。 12.内存模型三大特性(1)原子性Java 内存模型保证了 read、write、load、use、assign、store、lock 和 unlock 操作具有原子性 (2)可见性可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。JMM 内部的实现通常是依赖于所谓的内存屏障，通过禁止某些重排序的方式，提供内存可见性保证，也就是实现了各种 happen-before 规则。与此同时，更多复杂度在于，需要尽量确保各种编译器、各种体系结构的处理器，都能够提供一致的行为。 (3)有序性有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 13.先行发生原则(Happen-Before)（可见性保证）由于指令重排序的存在，两个操作之间有happen-before关系，并不意味着前一个操作必须要在后一个操作之前执行。仅仅要求前一个操作的执行结果对于后一个操作是可见的，并且前一个操作按顺序排在第二个操作之前. 内存模型使用先行发生原则在Java内存模型中保证多线程操作可见性的机制 编号 规则 解释 1 单一线程原则（程序员顺序规则） 在一个线程内，在程序前面的操作先行发生于后面的操作。 2 管程锁定规则（监视器锁规则） 一个 unlock（解锁） 操作先行发生于后面对同一个锁的 lock（加锁） 操作。 3 volatile 变量规则 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4 线程启动规则 Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5 线程加入规则 Thread 对象的结束先行发生于 join() 方法返回。 6 线程中断规则 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。 7 对象终结规则 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。 8 传递性 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 14. 一个Main方法执行的过程？15. JVM调优命令Sun JDK监控和故障处理命令有jps、jstat、jmap、jhat、jstack、jinfo. jps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。 jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jmap，JVM Memory Map命令用于生成heap dump文件 jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看 jstack，用于生成java虚拟机当前时刻的线程快照。 jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。 （五）I/O资料（http://wiki.jikexueyuan.com/project/java-nio-zh/java-nio-tutorial.html） 1.InputStream,OutputStream,Reader,Writer？ InputStream,OutputStream:面向字节流 Reader,Writer:面向字符流 操作对象 文件,数组,基本数据类型,缓冲,管道,打印,对象序列化,转化 (1)文件 FileInputStream FileOutputStream FileReader FileWriter (2)数组 ByteArrayInputStream ByteArrayOutputStream CharArrayReader CharArrayWriter (3)基本数据类型 DataInputStream DataOutputStream (4)缓冲 BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter (5)管道 PipedInputStream PipedOutputStream PipedReader PipedWriter (6)打印 printStream printWriter (7)对象序列化 ObjectInputStream ObjectOutputStream (8)转化 InputStreamReader OutputStreamWriter 2.I/O相关的概念（如同步/异步，阻塞/非阻塞）（1）I/O分类 java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io流共涉及40多个类， Java I0流的40多个类都是从如下4个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 （2）同步/异步，阻塞/非阻塞 同步/异步（被调用者不会/会直接返回结果） 阻塞/非阻塞（调用者不会/会去做其他的事情，需不需要一直等） 3.Java NIO和IO之间的区别IO是面向流的，NIO是面向缓冲区的. IO NIO IO流是阻塞的 NIO流是不阻塞的 IO 面向流(Stream oriented) NIO 面向缓冲区(Buffer oriented) NIO 通过Channel（通道） 进行读写。 NIO有选择器，选择器用于使用单个线程处理多个通道。 （1）面向流和面向缓冲区比较(Stream Oriented vs. Buffer Oriented) 第一个重大差异是IO是面向流的，而NIO是面向缓存区的。这句话是什么意思呢？ Java IO面向流意思是我们每次从流当中读取一个或多个字节。怎么处理读取到的字节是我们自己的事情。他们不会再任何地方缓存。再有就是我们不能在流数据中向前后移动。如果需要向前后移动读取位置，那么我们需要首先为它创建一个缓存区。socket.getOutputStream().write(), InputStream inputStream = socket.getInputStream(); inputStream.read(data)) 这里data可以被指定为一个数组 Java NIO是面向缓冲区的，这有些细微差异。数据是被读取到缓存当中以便后续加工。我们可以在缓存中向向后移动。这个特性给我们处理数据提供了更大的弹性空间。当然我们任然需要在使用数据前检查缓存中是否包含我们需要的所有数据。另外需要确保在往缓存中写入数据时避免覆盖了已经写入但是还未被处理的数据。 （2）阻塞和非阻塞IO比较（Blocking vs. No-blocking IO） ​ Java IO的各种流都是阻塞的。这意味着一个线程一旦调用了read(),write()方法，那么该线程就被阻塞住了，知道读取到数据或者数据完整写入了。在此期间线程不能做其他任何事情。 ​ Java NIO的非阻塞模式使得线程可以通过channel来读数据，并且是返回当前已有的数据，或者什么都不返回如果但钱没有数据可读的话。这样一来线程不会被阻塞住，它可以继续向下执行。 NIO提供了与传统BIO模型中的 Socket和 ServerSocket相对应的 SocketChannel和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。 ​ 通常线程在调用非阻塞操作后，会通知处理其他channel上的IO操作。因此一个线程可以管理多个channel的输入输出。 4.NIO和AIO的对比？ NIO是同步非阻塞的，AIO是异步非阻塞的 由于NIO的读写过程依然在应用线程里完成，所以对于那些读写过程时间长的，NIO就不太适合。而AIO的读写过程完成后才被通知，所以AIO能够胜任那些重量级，读写过程长的任务。 NIO:http://wiki.jikexueyuan.com/project/java-nio-zh/java-nio-non-blocking-server.html 5.Channel，Buffer,Seletor? Channel Buffer Selector FileChannel DatagramChannel SocketChannel ServerSocketChannel 7种基本类型+MappedBytesBuffer(1) ByteBuffer(2) CharBuffer(3) DoubleBuffer(4) FloatBuffer(5) IntBuffer(6) LongBuffer(7) ShortBuffer （1）Channel FileChanel 123456789101112131415161718192021RandomAccessFile aFile = new RandomAccessFile("data/nio-data.txt", "rw");FileChannel inChannel = aFile.getChannel();// 首先把数据读取到Buffer中ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) &#123; System.out.println("Read " + bytesRead); // 然后调用flip()方法 buf.flip(); while(buf.hasRemaining())&#123; // 接着再把数据读取出来。 System.out.print((char) buf.get()); &#125; buf.clear(); bytesRead = inChannel.read(buf);&#125;aFile.close(); ​ 在Java NIO中如果一个channel是FileChannel类型的，那么他可以直接把数据传输到另一个channel。逐个特性得益于FileChannel包含的transferTo和transferFrom两个方法。 12345678910RandomAccessFile fromFile = new RandomAccessFile("fromFile.txt", "rw");FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile("toFile.txt", "rw");FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();toChannel.transferFrom(fromChannel, position, count); 12345678910RandomAccessFile fromFile = new RandomAccessFile("fromFile.txt", "rw");FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile("toFile.txt", "rw");FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();fromChannel.transferTo(position, count, toChannel); SocketChannel ​ 在Java NIO体系中，SocketChannel是用于TCP网络连接的套接字接口，相当于Java网络编程中的Socket套接字接口。创建SocketChannel主要有两种方式，如下： 打开一个SocketChannel并连接网络上的一台服务器。 当ServerSocketChannel接收到一个连接请求时，会创建一个SocketChannel。 ​ 在Java NIO中，ServerSocketChannel是用于监听TCP链接请求的通道，正如Java网络编程中的ServerSocket一样。ServerSocketChannel实现类位于java.nio.channels包下面。 1234567// 打开ServerSocketChannelServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bin(new InetSocketAddress(9999));while(true) &#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; （2）Buffer​ Java NIO Buffers用于和NIO Channel交互。我们从channel中读取数据到buffers里，从buffer把数据写入到channels. buffer本质上就是一块内存区，可以用来写入数据，并在稍后读取出来。这块内存被NIO Buffer包裹起来，对外提供一系列的读写方便开发的接口。 利用Buffer读写数据，通常遵循四个步骤： 把数据写入buffer； 调用flip； 从Buffer中读取数据； 调用buffer.clear()或者buffer.compact() Java NIO发布时内置了对scatter / gather的支持。scatter / gather是通过通道读写数据的两个概念。 | | || :————————————-: | :————————————: || Scatter | Gather | （3）Selector​ Selector是Java NIO中的一个组件，用于检查一个或多个NIO Channel的状态是否处于可读、可写。如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 创建SelectorSelector selector = Selector.open();// 注册Channel到Selector上channel.configureBlocking(false);/** 四种就绪状态用SelectionKey中的常量表示如下：* SelectionKey.OP_CONNECT* SelectionKey.OP_ACCEPT* SelectionKey.OP_READ* SelectionKey.OP_WRITE*/SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; int readyChannels = selector.selectNow(); if(readyChannels == 0) continue; Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125;&#125; 6.Java NIO Channel通道和流的区别 通道可以读也可以写，流一般来说是单向的（只能读或者写）。 通道可以异步读写。 通道总是基于缓冲区Buffer来读写。 7.SocketChannel , ServerSocketChannel SocketChannel ServerSocketChannel 方法 open(); close(); write(); read(); connect(); open(); close(); accept(); 8.AsynchronousChannel(提前返回)(1)read 第一种方式（使用CompletionHandler） 第二种方式（使用Future） (2)write 第一种方式（使用CompletionHandler） 第二种方式（使用Future） 9.什么是Java序列化，如何实现Java序列化？ （1）含义：（扯一点Python相关） 序列化：把对象转换为字节序列的过程称为对象的序列化。 反序列化：把字节序列恢复为对象的过程称为对象的反序列化。 （2）实现：import java.io.Serializable;(注意：这个接口只用来标示，并没有实际的字段和方法) The serialization interface has no methods or fields(没有字段和方法) and serves only to identify the semantics of being serializable. 1234&gt; &gt;//序列化&gt; &gt; ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(new File("××××文件名")));&gt; &gt; oos.writeObject(flyPig);&gt; &gt; &gt; 1234&gt; &gt;//反序列化&gt; &gt; ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File("××××××文件名")));&gt; &gt;FlyPig person = (FlyPig) ois.readObject();&gt; &gt; &gt; transient 修饰的属性，是不会被序列化的。 （3） serialVersionUID 的作用和用法? 就是在实现这个Serializable 接口的时候，一定要给这个 serialVersionUID 赋值 10.Java有几种文件拷贝方式？哪一种最高效？(1)利用 java.io 类库，直接为源文件构建一个 FileInputStream 读取，然后再为目标文件构建一个FileOutputStream，完成写入工作. (2)利用 java.nio 类库提供的transferTo 或 transferFrom方法实现。 (3)Java 标准类库本身提供了几种 Files.copy 的实现。（java.nio.file.Files.copy） 对于 Copy 的效率，这个其实与操作系统和配置等情况相关，总体上来说，NIO transferTo/From 的方式可能更快，因为它更能利用现代操作系统底层机制，避免不必要拷贝 和上下文切换。 11.FileInputStream在使用完以后，不关闭流，想二次使用可以怎么操作？12.Reactor, Proactor?13.NettyNetty 是一个高性能、异步事件驱动的 NIO 框架，基于 JAVA NIO 提供的 API 实现。它提供了对TCP、 UDP 和文件传输的支持，作为一个异步 NIO 框架， Netty 的所有 IO 操作都是异步非阻塞的， 通过 Future-Listener 机制，用户可以方便的主动获取或者通过通知机制获得 IO 操作结果。 （六）Java 81.接口的默认方法(Default Methods for Interfaces)2.函数式接口(Functional Interfaces)@FunctionalInterface3.Lamda 表达式作用域(Lambda Scopes)4.java.util.StreamFilter(过滤), Sorted(排序), Map(映射) , Match(匹配), Count(计数), Reduce(规约) 5.时间相关（七）quartz和timer对比（八）RPCRPC（Remote Procedure Call）即远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？这个时候就需要 RPC 了！ RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单。 1. 设计一个RPC框架一个完整的 RPC 框架使用示意图如下： （1）注册中心服务端启动的时候将服务名称及其对应的地址(ip+port)注册到注册中心，服务消费端根据服务名称找到对应的服务地址。有了服务地址之后，服务消费端就可以通过网络请求服务端了。 （2）网络传输既然我们要调用远程的方法，就要发送网络请求来传递目标类和方法的信息以及方法的参数等数据到服务提供端。网络传输具体实现你可以使用 Socket （ Java 中最原始、最基础的网络通信方式。但是，Socket 是阻塞 IO、性能低并且功能单一）。你也可以使用同步非阻塞的 I/O 模型 NIO ，但是用它来进行网络编程真的太麻烦了。基于 NIO 的网络编程框架 Netty ，它将是你最好的选择！ （3）序列化要在网络传输数据就要涉及到序列化 。所以，我们还要考虑选择哪种序列化协议。序列化方式有很多种，比较常见的有 hessian、kyro、protostuff ……。我会在下一篇文章中简单对比一下这些序列化方式。 （4）动态代理RPC 的主要目的就是让我们调用远程方法像调用本地方法一样简单，使用动态代理屏蔽远程方法调用的细节比如网络传输。也就是说当你调用远程方法的时候，实际会通过代理对象来传输网络请求 （5）负载均衡（九）Guava1. 如何实现限流？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[诗词曲]]></title>
    <url>%2F2019%2F10%2F12%2F%E8%AF%97%E8%AF%8D%E6%9B%B2%2F</url>
    <content type="text"><![CDATA[收集学生时代接触到的古诗文,欣赏+配图 第一章第一节《山村咏怀》———邵雍（北宋） 一去二三里，烟村四五家。亭台六七座，八九十枝花。 《登鹳雀楼》———王之涣（唐） 白日依山尽，黄河入海流。欲穷千里目，更上一层楼。 《静夜思》———李白（唐） 床前明月光，疑是地上霜。举头望明月，低头思故乡 《咏鹅》———骆宾王（唐） 鹅鹅鹅，曲项向天歌。白毛浮绿水，红掌拨清波。 《画》———王维（唐） 远看山有色，近听水无声。春去花还在，人来鸟不惊 《夜宿山寺》———李白（唐） 危楼高百尺，手可摘星辰。不敢高声语，恐惊天上人。 《梅花》———王安石（宋） 墙角数枝梅，凌寒独自开。遥知不是雪，为有暗香来。 《春晓》———孟浩然（唐） 春眠不觉晓，处处闻啼鸟。夜来风雨声，花落知多少。 《咏柳》———贺知章（唐） 碧玉妆成一树高，万条垂下绿丝绦。不知细叶谁裁出，二月春风似剪刀 《悯农》———李绅（唐） 春种一粒粟，秋收万颗子。四海无闲田，农夫犹饿死。 《绝句》———杜甫（唐） 两个黄鹂鸣翠柳，一行白鹭上青天。窗含西岭千秋雪，门泊东吴万里船。 《春游胡》———徐俯（宋） 双飞燕子几时回？夹岸桃花蘸水开。春雨断桥人不度，小舟撑出柳阴来。 《小池》———杨万里（宋） 泉眼无声惜细流，树阴照水爱晴柔。小荷才露尖尖角，早有蜻蜓立上头。 《早发白帝城》———李白（唐） 朝辞白帝彩云间，千里江陵一日还。两岸猿声啼不住，轻舟已过万重山。 《宿新市徐公店》———杨万里（宋） 篱落疏疏一径深，树头花落未成阴。儿童急走追黄蝶，飞入菜花无处寻。 第二节《游子吟》———孟郊（唐） 慈母手中线，游子身上衣。 临行密密缝，意恐迟迟归。 谁言寸草心，报得三春晖。 《望庐山瀑布》———李白（唐） 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺，疑是银河落九天。 《长歌行》 青青园中葵，朝露待日晞。 阳春布德泽，万物生光辉。 常恐秋节至，焜黄华叶衰。 百川东到海，何时复西归？ 少壮不努力，老大徒伤悲。 《寻隐者不遇》———贾岛（唐） 松下问童子，言师采药去。只在此山中，云深不知处。 《山行》———杜牧（唐） 远上寒山石径斜，白云生处有人家。停车坐爱枫林晚，霜叶红于二月花。 《逢雪宿芙蓉山主人》———刘长卿 （唐） 日暮苍山远，天寒白屋贫。柴门闻犬吠，风雪夜归人。 《回乡偶书》———贺知章（唐） 少小离家老大回，乡音难改鬓毛衰。儿童相见不相识，笑问客从何处来。 离别家乡岁月多，近来人事半销磨。唯有门前镜湖水，春风不改旧时波。 《赠汪伦》———李白（唐） 李白乘舟将欲行，忽闻岸上踏歌声。桃花潭水深千尺，不及汪伦送我情。 《黄鹤楼送孟浩然之广陵》———李白（唐） 故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。 《竹石》———郑燮 （清） 咬定青山不放松，立根原在破岩中。千磨万击还坚劲，任尔东西南北风。 《赠花卿》———杜甫（唐） 锦城丝管日纷纷，半入江风半入云。此曲只应天上有，人间能得几回闻。 《小儿垂钓》———胡令能（唐） 蓬头稚子学垂纶，侧坐莓苔草映身。路人借问遥招手，怕得鱼惊不应人。 《花影》———苏轼（宋） 重重叠叠上瑶台，几度呼童扫不开。刚被太阳收拾去，又叫明月送将来。 《采莲曲》———王昌龄（唐） 荷叶罗裙一色裁，芙蓉向脸两边开。乱入池中看不见，闻歌始觉有人来。 《赋得古原草送别》———白居易（唐） 离离原上草，一岁一枯荣。野火烧不尽，春风吹又生。 远芳侵古道，晴翠接荒城。又送王孙去，萋萋满别情。 《节气歌》 春雨惊春清谷天，夏满芒种暑相连。秋处露秋寒霜降，冬雪雪冬小大寒。 第三节《所见》———袁枚（清） 牧童骑黄牛，歌声振林樾。 意欲捕鸣蝉，忽然闭口立。 《鹿柴》———王维（唐） 空山不见人，但闻人语响。返景入深林，复照青苔上。 《敕勒川》 敕勒川，阴山下。天似穹庐，笼盖四野。天苍苍，野茫茫。风吹草低见牛羊。 《舟夜书所见》———查慎行 （清） 月黑见渔灯，孤光一点萤。 微微风簇浪，散作满河星。 《九月九日忆山东兄弟》———王维（唐） 独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。 《乐游原》———李商隐（唐） 向晚意不适，驱车登古原。 夕阳无限好，只是近黄昏。 《蜂》———罗隐 （唐） 不论平地与山尖，无限风光尽被占。 采得百花成蜜后，为谁辛苦为谁甜。 《望天门山》———李白（唐） 天门中断楚江开，碧水东流至此回。 两岸青山相对出，孤帆一片日边来。 《赠刘景文》———苏轼（宋） 荷尽已无擎雨盖，菊残犹有傲霜枝。 一年好景君须记，正是橙黄橘绿时。 《村居》———高鼎（清） 草长莺飞二月天，拂堤杨柳醉春烟。 儿童散学归来早，忙趁东风放纸鸢。 《饮湖上初晴后雨》其一： 朝曦迎客艳重冈，晚雨留人入醉乡。此意自佳君不会，一杯当属水仙王。 其二： 水光潋滟晴方好，山色空蒙雨亦奇。欲把西湖比西子，淡妆浓抹总相宜。 第四节《题西林壁》———苏轼（宋） 横看成岭侧成峰，远近高低各不同。 不识庐山真面目，只缘身在此山中。 《游山西村》———陆游（宋） 莫笑农家腊酒浑，丰年留客足鸡豚。 山重水复疑无路，柳暗花明又一村。 箫鼓追随春社近，衣冠简朴古风存。 从今若许闲乘月，拄杖无时夜叩门。 《过故人庄》———孟浩然（唐） 故人具鸡黍，邀我至田家。 绿树村边合，青山郭外斜。 开轩面场圃，把酒话桑麻。 待到重阳日，还来就菊花。 《送元二使安西》———王维（唐） 渭城朝雨浥轻尘，客舍青青柳色新。 劝君更尽一杯酒，西出阳关无故人。 《独坐敬亭山》———李白（唐） 众鸟高飞尽，孤云独去闲。 相看两不厌，只有敬亭山。 《望洞庭》———刘禹锡（唐） 湖光秋月两相和，潭面无风镜未磨。 遥望洞庭山水翠，白银盘里一青螺。 《忆江南》———白居易（唐） 江南好，风景旧曾谙。 日出江花红胜火，春来江水绿如蓝。能不忆江南？ 《乡村四月》———翁卷 （宋） 绿遍山原白满川，子规声里雨如烟。 乡村四月闲人少，才了蚕桑又插田。 《四时田园杂兴》———范成大（宋） 梅子金黄杏子肥，麦花雪白菜花稀。 日长篱落无人过，惟有蜻蜓蛱蝶飞。 《渔歌子》———张志和（唐） 西塞山前白鹭飞，桃花流水鳜鱼肥。 青箬笠，绿蓑衣，斜风细雨不须归。 第五节《泊船瓜洲》———王安石（宋） 京口瓜洲一水间，钟山只隔数重山。春风又绿江南岸，明月何时照我还？ 《秋思》 《长相思》———纳兰性德（清） 《卜算子-咏梅》———毛泽东 《牧童》———吕岩（唐） 《舟过安仁》 《清平乐-村居》———辛弃疾（宋） 《浪淘沙》———刘禹锡（唐） 第六节《天净沙-秋》———白朴（元） 《马诗》———李贺（唐） 《西江月-夜行黄沙道中》 《春夜喜雨》 《诗经-采薇》 《元日》 《天竺寺八月十五日夜桂子》 《七步诗》 《鸟鸣涧》 《芙蓉楼送辛渐》 《江畔独步寻花》 《石灰吟》 《闻官军收付河南河北》———杜甫（唐） 剑外忽传收蓟北，初闻涕泪满衣裳。却看妻子愁何在，漫卷诗书喜欲狂。 白日放歌须纵酒，青春作伴好还乡。即从巴峡穿巫峡，便下襄阳向洛阳。 《浣溪沙》 《卜算子-送鲍浩然之浙东》 《己亥杂诗》 第二章第一节《观沧海》———曹操（汉） 东临碣石，以观沧海。水何澹澹，山岛竦峙。 树木丛生，百草丰茂。秋风萧瑟，洪波涌起。 日月之行，若出其中。星汉灿烂，若出其里。 幸甚至哉，歌以咏志。 《次北固山下》 《钱塘湖春行》———白居易（唐） 孤山寺北贾亭西，水面初平云脚低。几处早莺争暖树，谁家新燕啄春泥。 乱花渐欲迷人眼，浅草才能没马蹄。最爱湖东行不足，绿杨阴里白沙堤。 《天净沙-秋思》 《龟虽寿》———曹操（汉） 神龟虽寿，犹有竟时；腾蛇乘雾，终为土灰。 老骥伏枥，志在千里；烈士暮年，壮心不已。 盈缩之期，不但在天；养怡之福，可得永年。 幸甚至哉，歌以咏志。 《过故人庄》 《题破山寺后禅院》 《闻王昌龄左迁龙标遥有此寄》———李白（唐） 杨花落尽子规啼，闻道龙标过五溪。我寄愁心与明月，随君直到夜郎西。 《夜雨寄北》———李商隐（唐） 君问归期未有期，巴山夜雨涨秋池。何当共剪西窗烛，却话巴山夜雨时。 《泊秦淮》———杜牧（唐） 烟笼寒水月笼沙，夜泊秦淮近酒家。商女不知亡国恨，隔江犹唱后庭花。 《浣溪沙》———晏殊 《过松源晨炊漆公店》 《如梦令》 《观书有感》 《山中杂诗》 《竹里馆》 《峨眉山月歌》 《春夜洛城闻笛》 《逢入京使》 《滁州西涧》 《江南逢李龟年》 《送灵澈上人》 《约客》———赵师秀 《论诗》 第二节《望岳》 《春望》 《石壕吏》 《归园田居》 《使至塞上》 《渡荆门送别》 《登岳阳楼》———陈与义 《长歌行》———汉乐府 青青园中葵，朝露待日晞。阳春布德泽，万物生光辉。常恐秋节至，焜黄华叶衰。百川东到海，何时复西归？少壮不努力，老大徒伤悲。 《野望》 《早寒江上有怀》 《望洞庭湖赠张丞相》 《黄鹤楼》 《送友人》 《秋词》 《鲁山山行》 《浣溪沙》 《十一月四日风雨大作》———陆游（宋） 僵卧孤村不自哀，尚思为国戍轮台。夜阑卧听风吹雨，铁马冰河入梦来。 《酬乐天扬州初逢席上见赠》 《赤壁》 《过零丁洋》 《水调歌头》 《山坡羊-潼关怀古》 《饮酒》———陶渊明 《行路难》 《茅屋为秋风所破歌》———杜甫（唐） 八月秋高风怒号，卷我屋上三重茅。茅飞渡江洒江郊，高者挂罥长林梢，下者飘转沉塘坳。 南村群童欺我老无力，忍能对面为盗贼。公然抱茅入竹去，唇焦口燥呼不得，归来倚杖自叹息。 俄顷风定云墨色，秋天漠漠向昏黑。布衾多年冷似铁，娇儿恶卧踏里裂。床头屋漏无干处，雨脚如麻未断绝。自经丧乱少睡眠，长夜沾湿何由彻！ 安得广厦千万间，大庇天下寒士俱欢颜！风雨不动安如山。呜呼！何时眼前突兀见此屋，吾庐独破受冻死亦足！ 《白雪松武判官归京》 《己亥杂诗》 《送杜少府之任蜀州》 《赠从弟》 《登幽州台歌》 《早春呈水部张十八员外》 《终南别业》 《宣州谢朓楼饯别校书叔云》 《无题》———李商隐 《登飞来峰》 《相见欢》 《苏幕遮》———范仲淹（北宋） 第三节《望江南》———温庭筠 《渔家傲-秋思》 《江城子-密州出猎》 《武陵春》———李清照（南宋） 《破阵子-为陈同甫赋壮词以寄之》 《观刈麦》 《月夜》 《商山早行》———温庭筠（唐） 晨起动征铎，客行悲故乡。鸡声茅店月，人迹板桥霜。槲叶落山路，枳花明驿墙。因思杜陵梦，凫雁满回塘。 《卜算子-咏梅》 《破阵子》 《浣溪沙》 簌簌衣巾落枣花，村南村北响缲车。牛衣古柳卖黄瓜。 酒困路长惟欲睡，日高人渴漫思茶。敲门试问野人家。 《醉花阴》———李清照 《南乡子”登京口北固亭有怀》 《山坡羊”骊山怀古》 《朝天子”咏喇叭》 《关雎》 关关雎鸠，在河之洲。窈窕淑女，君子好逑。 参差荇菜，左右流之。窈窕淑女，寤寐求之。 求之不得，寤寐思服。悠哉悠哉，辗转反侧。 参差荇菜，左右采之。窈窕淑女，琴瑟友之。 参差荇菜，左右芼之。窈窕淑女，钟鼓乐之。 《蒹葭》 蒹葭苍苍，白露为霜。所谓伊人，在水一方。溯洄从之，道阻且长。溯游从之，宛在水中央。 蒹葭萋萋，白露未晞。所谓伊人，在水之湄。溯洄从之，道阻且跻。溯游从之，宛在水中坻。 蒹葭采采，白露未已。所谓伊人，在水之涘。溯洄从之，道阻且右。溯游从之，宛在水中沚。 《从军行》 《月下独酌》 《羌村三首》 《登楼》———杜甫（唐） 《走马川行奉送封大夫出师西征》 《左迁至蓝关示侄孙湘》 《望月有感》 《雁门太守行》 《卜算子”送鲍浩然之浙东》 《别云间》———夏完淳（明） 第三章第一节《沁园春-长沙》 《采桑子-重阳》 《邹忌讽齐王纳谏》 《劝学》 《过秦论》———贾谊（汉） 秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具，外连衡而斗诸侯。于是秦人拱手而取西河之外。 孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹；因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 延及孝文王、庄襄王，享国之日浅，国家无事。 及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里；胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰；收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。 始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼，墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦；斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。 《兰亭集序》———王羲之（魏晋） 永和九年，岁在癸丑，暮春之初，会于会稽山阴之兰亭，修禊事也。群贤毕至，少长咸集。此地有崇山峻岭，茂林修竹，又有清流激湍，映带左右，引以为流觞曲水，列坐其次。虽无丝竹管弦之盛，一觞一咏，亦足以畅叙幽情。 是日也，天朗气清，惠风和畅。仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐也。 夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至；及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀，况修短随化，终期于尽！古人云：“死生亦大矣。”岂不痛哉！ 每览昔人兴感之由，若合一契，未尝不临文嗟悼，不能喻之于怀。固知一死生为虚诞，齐彭殇为妄作。后之视今，亦犹今之视昔，悲夫！故列叙时人，录其所述，虽世殊事异，所以兴怀，其致一也。后之览者，亦将有感于斯文。 《归去来兮辞》 《师说》———韩愈 《阿房宫赋》———杜牧（唐） 六王毕，四海一；蜀山兀，阿房出。覆压三百余里，隔离天日。骊山北构而西折，直走咸阳。二川溶溶，流入宫墙。五步一楼，十步一阁；廊腰缦回，檐牙高啄；各抱地势，钩心斗角。盘盘焉，囷囷焉，蜂房水涡，矗不知其几千万落！长桥卧波，未云何龙？复道行空，不霁何虹？高低冥迷，不知西东。歌台暖响，春光融融；舞殿冷袖，风雨凄凄。一日之内，一宫之间，而气候不齐。 妃嫔媵嫱，王子皇孙，辞楼下殿，辇来于秦，朝歌夜弦，为秦宫人。明星荧荧，开妆镜也；绿云扰扰，梳晓鬟也；渭流涨腻，弃脂水也；烟斜雾横，焚椒兰也。雷霆乍惊，宫车过也；辘辘远听，杳不知其所之也。一肌一容，尽态极妍，缦立远视，而望幸焉；有不得见者，三十六年。 燕、赵之收藏，韩、魏之经营，齐、楚之精英，几世几年，剽掠其人，倚叠如山。一旦不能有，输来其间。鼎铛玉石，金块珠砾，弃掷逦迤，秦人视之，亦不甚惜。 嗟乎！一人之心，千万人之心也。秦爱纷奢，人亦念其家；奈何取之尽锱铢，用之如泥沙？使负栋之柱，多于南亩之农夫；架梁之椽，多于机上之工女；钉头磷磷，多于在庾之粟粒；瓦缝参差，多于周身之帛缕；直栏横槛，多于九土之城郭；管弦呕哑，多于市人之言语。使天下之人，不敢言而敢怒；独夫之心，日益骄固。戍卒叫，函谷举；楚人一炬，可怜焦土。 呜呼！灭六国者，六国也，非秦也。族秦者，秦也，非天下也。嗟乎！使六国各爱其人，则足以拒秦；使秦复爱六国之人，则递三世可至万世而为君，谁得而族灭也？秦人不暇自哀，而后人哀之；后人哀之而不鉴之，亦使后人而复哀后人也。 《卫风-氓》 氓之蚩蚩，抱布贸丝。匪来贸丝，来即我谋。送子涉淇，至于顿丘。匪我愆期，子无良媒。将子无怒，秋以为期。乘彼垝垣，以望复关。不见复关，泣涕涟涟。既见复关，载笑载言。尔卜尔筮，体无咎言。以尔车来，以我贿迁。桑之未落，其叶沃若。于嗟鸠兮，无食桑葚！于嗟女兮，无与士耽！士之耽兮，犹可说也。女之耽兮，不可说也。桑之落矣，其黄而陨。自我徂尔，三岁食贫。淇水汤汤，渐车帷裳。女也不爽，士贰其行。士也罔极，二三其德。三岁为妇，靡室劳矣；夙兴夜寐，靡有朝矣。言既遂矣，至于暴矣。兄弟不知，咥其笑矣。静言思之，躬自悼矣。及尔偕老，老使我怨。淇则有岸，隰则有泮。总角之宴，言笑晏晏。信誓旦旦，不思其反。反是不思，亦已焉哉！ 《秦风-无衣》 《邶风•静女》 《离骚》———屈原（先秦） 帝高阳之苗裔兮，朕皇考曰伯庸。 摄提贞于孟陬兮，惟庚寅吾以降。 皇览揆余初度兮，肇锡余以嘉名。 名余曰正则兮，字余曰灵均。 纷吾既有此内美兮，又重之以修能。 扈江离与辟芷兮，纫秋兰以为佩。 汩余若将不及兮，恐年岁之不吾与。 朝搴阰之木兰兮，夕揽洲之宿莽。 日月忽其不淹兮，春与秋其代序。 惟草木之零落兮，恐美人之迟暮。 不抚壮而弃秽兮，何不改乎此度？ 乘骐骥以驰骋兮，来吾道夫先路！ 昔三后之纯粹兮，固众芳之所在。 杂申椒与菌桂兮，岂惟纫夫蕙茝！ 彼尧、舜之耿介兮，既遵道而得路。 何桀纣之昌披兮，夫惟捷径以窘步。 惟夫党人之偷乐兮，路幽昧以险隘。 岂余身之殚殃兮，恐皇舆之败绩。 忽奔走以先后兮，及前王之踵武。 荃不查余之中情兮，反信谗而齌怒。 余固知謇謇之为患兮，忍而不能舍也。 指九天以为正兮，夫惟灵修之故也。 曰黄昏以为期兮，羌中道而改路。 初既与余成言兮，后悔遁而有他。 余既不难夫离别兮，伤灵修之数化。 余既滋兰之九畹兮，又树蕙之百亩。 畦留夷与揭车兮，杂杜衡与芳芷。 冀枝叶之峻茂兮，愿俟时乎吾将刈。 虽萎绝其亦何伤兮，哀众芳之芜秽。 众皆竞进以贪婪兮，凭不厌乎求索。 羌内恕己以量人兮，各兴心而嫉妒。 忽驰骛以追逐兮，非余心之所急。 老冉冉其将至兮，恐修名之不立。 朝饮木兰之坠露兮，夕餐秋菊之落英。 苟余情其信姱以练要兮，长顑颔亦何伤。 掔木根以结茝兮，贯薜荔之落蕊。 矫菌桂以纫蕙兮，索胡绳之纚纚。 謇吾法夫前修兮，非世俗之所服。 虽不周于今之人兮，愿依彭咸之遗则。 长太息以掩涕兮，哀民生之多艰。 余虽好修姱以鞿羁兮，謇朝谇而夕替。 既替余以蕙纕兮，又申之以揽茝。 亦余心之所善兮，虽九死其犹未悔。 怨灵修之浩荡兮，终不察夫民心。 众女嫉余之蛾眉兮，谣诼谓余以善淫。 固时俗之工巧兮，偭规矩而改错。 背绳墨以追曲兮，竞周容以为度。 忳郁邑余侘傺兮，吾独穷困乎此时也。 宁溘死以流亡兮，余不忍为此态也。 鸷鸟之不群兮，自前世而固然。 何方圜之能周兮，夫孰异道而相安？ 屈心而抑志兮，忍尤而攘诟。 伏清白以死直兮，固前圣之所厚。 悔相道之不察兮，延伫乎吾将反。 回朕车以复路兮，及行迷之未远。 步余马于兰皋兮，驰椒丘且焉止息。 进不入以离尤兮，退将复修吾初服。 制芰荷以为衣兮，集芙蓉以为裳。 不吾知其亦已兮，苟余情其信芳。 高余冠之岌岌兮，长余佩之陆离。 芳与泽其杂糅兮，唯昭质其犹未亏。 忽反顾以游目兮，将往观乎四荒。 佩缤纷其繁饰兮，芳菲菲其弥章。 民生各有所乐兮，余独好修以为常。 虽体解吾犹未变兮，岂余心之可惩。 女嬃之婵媛兮，申申其詈予。 曰：“鲧婞直以亡身兮，终然夭乎羽之野。 汝何博謇而好修兮，纷独有此姱节。 薋菉葹以盈室兮，判独离而不服。 众不可户说兮，孰云察余之中情。 世并举而好朋兮，夫何茕独而不予听？ 依前圣以节中兮，喟凭心而历兹。 济沅、湘以南征兮，就重华而敶词： 启《九辩》与《九歌》兮，夏康娱以自纵。 不顾难以图后兮，五子用失乎家衖。 羿淫游以佚畋兮，又好射夫封狐。 固乱流其鲜终兮，浞又贪夫厥家。 浇身被服强圉兮，纵欲而不忍。 日康娱而自忘兮，厥首用夫颠陨。 夏桀之常违兮，乃遂焉而逢殃。 后辛之菹醢兮，殷宗用而不长。 汤、禹俨而祗敬兮，周论道而莫差。 举贤才而授能兮，循绳墨而不颇。 皇天无私阿兮，览民德焉错辅。 夫维圣哲以茂行兮，苟得用此下土。 瞻前而顾后兮，相观民之计极。 夫孰非义而可用兮？孰非善而可服？ 阽余身而危死兮，览余初其犹未悔。 不量凿而正枘兮，固前修以菹醢。 曾歔欷余郁邑兮，哀朕时之不当。 揽茹蕙以掩涕兮，沾余襟之浪浪。 跪敷衽以陈辞兮，耿吾既得此中正。 驷玉虬以桀鹥兮，溘埃风余上征。 朝发轫于苍梧兮，夕余至乎县圃。 欲少留此灵琐兮，日忽忽其将暮。 吾令羲和弭节兮，望崦嵫而勿迫。 路漫漫其修远兮，吾将上下而求索。 饮余马于咸池兮，总余辔乎扶桑。 折若木以拂日兮，聊逍遥以相羊。 前望舒使先驱兮，后飞廉使奔属。 鸾皇为余先戒兮，雷师告余以未具。 吾令凤鸟飞腾兮，继之以日夜。 飘风屯其相离兮，帅云霓而来御。 纷总总其离合兮，斑陆离其上下。 吾令帝阍开关兮，倚阊阖而望予。 时暧暧其将罢兮，结幽兰而延伫。 世溷浊而不分兮，好蔽美而嫉妒。 朝吾将济于白水兮，登阆风而绁马。 忽反顾以流涕兮，哀高丘之无女。 溘吾游此春宫兮，折琼枝以继佩。 及荣华之未落兮，相下女之可诒。 吾令丰隆乘云兮，求宓妃之所在。 解佩纕以结言兮，吾令謇修以为理。 纷总总其离合兮，忽纬繣其难迁。 夕归次于穷石兮，朝濯发乎洧盘。 保厥美以骄傲兮，日康娱以淫游。 虽信美而无礼兮，来违弃而改求。 览相观于四极兮，周流乎天余乃下。 望瑶台之偃蹇兮，见有娀之佚女。 吾令鸩为媒兮，鸩告余以不好。 雄鸠之鸣逝兮，余犹恶其佻巧。 心犹豫而狐疑兮，欲自适而不可。 凤皇既受诒兮，恐高辛之先我。 欲远集而无所止兮，聊浮游以逍遥。 及少康之未家兮，留有虞之二姚。 理弱而媒拙兮，恐导言之不固。 世溷浊而嫉贤兮，好蔽美而称恶。 闺中既以邃远兮，哲王又不寤。 怀朕情而不发兮，余焉能忍而与此终古？ 索琼茅以筳篿兮，命灵氛为余占之。 曰：“两美其必合兮，孰信修而慕之？ 思九州之博大兮，岂惟是其有女？” 曰：“勉远逝而无狐疑兮，孰求美而释女？ 何所独无芳草兮，尔何怀乎故宇？” 世幽昧以昡曜兮，孰云察余之善恶？ 民好恶其不同兮，惟此党人其独异！ 户服艾以盈要兮，谓幽兰其不可佩。 览察草木其犹未得兮，岂珵美之能当？ 苏粪壤以充祎兮，谓申椒其不芳。 欲从灵氛之吉占兮，心犹豫而狐疑。 巫咸将夕降兮，怀椒糈而要之。 百神翳其备降兮，九疑缤其并迎。 皇剡剡其扬灵兮，告余以吉故。 曰：“勉升降以上下兮，求矩矱之所同。 汤、禹俨而求合兮，挚、咎繇而能调。 苟中情其好修兮，又何必用夫行媒？ 说操筑于傅岩兮，武丁用而不疑。 吕望之鼓刀兮，遭周文而得举。 宁戚之讴歌兮，齐桓闻以该辅。 及年岁之未晏兮，时亦犹其未央。 恐鹈鴃之先鸣兮，使夫百草为之不芳。” 何琼佩之偃蹇兮，众薆然而蔽之。 惟此党人之不谅兮，恐嫉妒而折之。 时缤纷其变易兮，又何可以淹留？ 兰芷变而不芳兮，荃蕙化而为茅。 何昔日之芳草兮，今直为此萧艾也？ 岂其有他故兮，莫好修之害也！ 余以兰为可恃兮，羌无实而容长。 委厥美以从俗兮，苟得列乎众芳。 椒专佞以慢慆兮，樧又欲充夫佩帏。 既干进而务入兮，又何芳之能祗？ 固时俗之流从兮，又孰能无变化？ 览椒兰其若兹兮，又况揭车与江离？ 惟兹佩之可贵兮，委厥美而历兹。 芳菲菲而难亏兮，芬至今犹未沬。 和调度以自娱兮，聊浮游而求女。 及余饰之方壮兮，周流观乎上下。 灵氛既告余以吉占兮，历吉日乎吾将行。 折琼枝以为羞兮，精琼爢以为粻。 为余驾飞龙兮，杂瑶象以为车。 何离心之可同兮？吾将远逝以自疏。 邅吾道夫昆仑兮，路修远以周流。 扬云霓之晻蔼兮，鸣玉鸾之啾啾。 朝发轫于天津兮，夕余至乎西极。 凤皇翼其承旗兮，高翱翔之翼翼。 忽吾行此流沙兮，遵赤水而容与。 麾蛟龙使梁津兮，诏西皇使涉予。 路修远以多艰兮，腾众车使径待。 路不周以左转兮，指西海以为期。 屯余车其千乘兮，齐玉轪而并驰。 驾八龙之婉婉兮，载云旗之委蛇。 抑志而弭节兮，神高驰之邈邈。 奏《九歌》而舞《韶》兮，聊假日以偷乐。 陟升皇之赫戏兮，忽临睨夫旧乡。 仆夫悲余马怀兮，蜷局顾而不行。 乱曰：已矣哉！ 国无人莫我知兮，又何怀乎故都！ 既莫足与为美政兮，吾将从彭咸之所居！ 《孔雀东南飞》汉 序曰：汉末建安中，庐江府小吏焦仲卿妻刘氏，为仲卿母所遣，自誓不嫁。其家逼之，乃投水而死。仲卿闻之，亦自缢于庭树。时人伤之，为诗云尔。 孔雀东南飞，五里一徘徊。 “十三能织素，十四学裁衣，十五弹箜篌，十六诵诗书。十七为君妇，心中常苦悲。君既为府吏，守节情不移，贱妾留空房，相见常日稀。鸡鸣入机织，夜夜不得息。三日断五匹，大人故嫌迟。非为织作迟，君家妇难为！妾不堪驱使，徒留无所施，便可白公姥，及时相遣归。” 府吏得闻之，堂上启阿母：“儿已薄禄相，幸复得此妇，结发同枕席，黄泉共为友。共事二三年，始尔未为久,女行无偏斜，何意致不厚？” 阿母谓府吏：“何乃太区区！此妇无礼节，举动自专由。吾意久怀忿，汝岂得自由！东家有贤女，自名秦罗敷，可怜体无比，阿母为汝求。便可速遣之，遣去慎莫留！” 府吏长跪告：“伏惟启阿母，今若遣此妇，终老不复取！” 阿母得闻之，槌床便大怒：“小子无所畏，何敢助妇语！吾已失恩义，会不相从许！” 府吏默无声，再拜还入户,举言谓新妇，哽咽不能语：“我自不驱卿，逼迫有阿母。卿但暂还家，吾今且报府。不久当归还，还必相迎取。以此下心意，慎勿违吾语。” 新妇谓府吏：“勿复重纷纭。往昔初阳岁，谢家来贵门。奉事循公姥，进止敢自专？昼夜勤作息，伶俜萦苦辛。谓言无罪过，供养卒大恩；仍更被驱遣，何言复来还！妾有绣腰襦，葳蕤自生光；红罗复斗帐，四角垂香囊；箱帘六七十，绿碧青丝绳，物物各自异，种种在其中。人贱物亦鄙，不足迎后人，留待作遗施，于今无会因。时时为安慰，久久莫相忘！” 鸡鸣外欲曙，新妇起严妆。著我绣夹裙，事事四五通。足下蹑丝履，头上玳瑁光。腰若流纨素，耳著明月珰。指如削葱根，口如含朱丹。纤纤作细步，精妙世无双。 上堂拜阿母，阿母怒不止。“昔作女儿时，生小出野里,本自无教训，兼愧贵家子。受母钱帛多，不堪母驱使。今日还家去，念母劳家里。”却与小姑别，泪落连珠子。“新妇初来时，小姑始扶床；今日被驱遣，小姑如我长。勤心养公姥，好自相扶将。初七及下九，嬉戏莫相忘。”出门登车去，涕落百余行。 府吏马在前，新妇车在后,隐隐何甸甸，俱会大道口。下马入车中，低头共耳语：“誓不相隔卿，且暂还家去；吾今且赴府，不久当还归，誓天不相负！” 新妇谓府吏：“感君区区怀！君既若见录，不久望君来。君当作磐石，妾当作蒲苇,蒲苇纫如丝，磐石无转移。我有亲父兄，性行暴如雷，恐不任我意，逆以煎我怀。”举手长劳劳，二情同依依。 入门上家堂，进退无颜仪。阿母大拊掌，不图子自归：“十三教汝织，十四能裁衣，十五弹箜篌，十六知礼仪，十七遣汝嫁，谓言无誓违。汝今何罪过，不迎而自归？”兰芝惭阿母：“儿实无罪过。”阿母大悲摧。 还家十余日，县令遣媒来。云有第三郎，窈窕世无双，年始十八九，便言多令才。 阿母谓阿女：“汝可去应之。” 阿女含泪答：“兰芝初还时，府吏见丁宁，结誓不别离。今日违情义，恐此事非奇。自可断来信，徐徐更谓之。” 阿母白媒人：“贫贱有此女，始适还家门。不堪吏人妇，岂合令郎君？幸可广问讯，不得便相许。”媒人去数日，寻遣丞请还，说有兰家女，承籍有宦官。云有第五郎，娇逸未有婚。遣丞为媒人，主簿通语言。直说太守家，有此令郎君，既欲结大义，故遣来贵门。 阿母谢媒人：“女子先有誓，老姥岂敢言！” 阿兄得闻之，怅然心中烦,举言谓阿妹：“作计何不量！先嫁得府吏，后嫁得郎君。否泰如天地，足以荣汝身。不嫁义郎体，其往欲何云？” 兰芝仰头答：“理实如兄言。谢家事夫婿，中道还兄门。处分适兄意，那得自任专！虽与府吏要，渠会永无缘。登即相许和，便可作婚姻。” 媒人下床去。诺诺复尔尔。还部白府君：“下官奉使命，言谈大有缘。”府君得闻之，心中大欢喜。视历复开书，便利此月内，六合正相应。良吉三十日，今已二十七，卿可去成婚。交语速装束，络绎如浮云。青雀白鹄舫，四角龙子幡。婀娜随风转。金车玉作轮。踯躅青骢马，流苏金镂鞍。赍钱三百万，皆用青丝穿。杂彩三百匹，交广市鲑珍。从人四五百，郁郁登郡门。 阿母谓阿女：“适得府君书，明日来迎汝。何不作衣裳？莫令事不举！” 阿女默无声，手巾掩口啼，泪落便如泻。移我琉璃榻，出置前窗下。左手持刀尺，右手执绫罗。朝成绣夹裙，晚成单罗衫。晻晻日欲暝，愁思出门啼。 府吏闻此变，因求假暂归。未至二三里，摧藏马悲哀。新妇识马声，蹑履相逢迎。怅然遥相望，知是故人来。举手拍马鞍，嗟叹使心伤：“自君别我后，人事不可量。果不如先愿，又非君所详。我有亲父母，逼迫兼弟兄，以我应他人，君还何所望！” 府吏谓新妇：“贺卿得高迁！磐石方且厚，可以卒千年；蒲苇一时纫，便作旦夕间。卿当日胜贵，吾独向黄泉！” 新妇谓府吏：“何意出此言！同是被逼迫，君尔妾亦然。黄泉下相见，勿违今日言！”执手分道去，各各还家门。生人作死别，恨恨那可论？念与世间辞，千万不复全！ 府吏还家去，上堂拜阿母：“今日大风寒，寒风摧树木，严霜结庭兰。儿今日冥冥，令母在后单。故作不良计，勿复怨鬼神！命如南山石，四体康且直！” 阿母得闻之，零泪应声落：“汝是大家子，仕宦于台阁，慎勿为妇死，贵贱情何薄！东家有贤女，窈窕艳城郭，阿母为汝求，便复在旦夕。” 府吏再拜还，长叹空房中，作计乃尔立。转头向户里，渐见愁煎迫。 其日牛马嘶，新妇入青庐。奄奄黄昏后，寂寂人定初。我命绝今日，魂去尸长留！揽裙脱丝履，举身赴清池。 府吏闻此事，心知长别离。徘徊庭树下，自挂东南枝。 两家求合葬，合葬华山傍。东西植松柏，左右种梧桐。枝枝相覆盖，叶叶相交通。中有双飞鸟，自名为鸳鸯，仰头相向鸣，夜夜达五更。行人驻足听，寡妇起彷徨。多谢后世人，戒之慎勿忘！ 《白马篇》 《涉江采芙蓉》 涉江采芙蓉，兰泽多芳草。采之欲遗谁？所思在远道。还顾望旧乡，长路漫浩浩。同心而离居，忧伤以终老。 第二节《迢迢牵牛星》 《短歌行》———曹操（汉） 对酒当歌，人生几何！譬如朝露，去日苦多。慨当以慷，忧思难忘。何以解忧？唯有杜康。 青青子衿，悠悠我心。但为君故，沉吟至今。呦呦鹿鸣，食野之苹。我有嘉宾，鼓瑟吹笙。 明明如月，何时可掇？忧从中来，不可断绝。越陌度阡，枉用相存。契阔谈䜩，心念旧恩。 月明星稀，乌鹊南飞。绕树三匝，何枝可依？山不厌高，海不厌深。周公吐哺，天下归心。 《归园田居》———陶渊明 《长歌行》 《梦游天姥吟留别》———李白（唐） 海客谈瀛洲，烟涛微茫信难求；越人语天姥，云霞明灭或可睹。天姥连天向天横，势拔五岳掩赤城。天台四万八千丈，对此欲倒东南倾。我欲因之梦吴越，一夜飞度镜湖月。湖月照我影，送我至剡溪。谢公宿处今尚在，渌水荡漾清猿啼。脚著谢公屐，身登青云梯。半壁见海日，空中闻天鸡。千岩万转路不定，迷花倚石忽已暝。熊咆龙吟殷岩泉，栗深林兮惊层巅。云青青兮欲雨，水澹澹兮生烟。列缺霹雳，丘峦崩摧。洞天石扉，訇然中开。青冥浩荡不见底，日月照耀金银台。霓为衣兮风为马，云之君兮纷纷而来下。虎鼓瑟兮鸾回车，仙之人兮列如麻。忽魂悸以魄动，恍惊起而长嗟。惟觉时之枕席，失向来之烟霞。世间行乐亦如此，古来万事东流水。别君去兮何时还？且放白鹿青崖间。须行即骑访名山。安能摧眉折腰事权贵，使我不得开心颜！ 《陈情表》———李密（魏晋） 臣密言：臣以险衅，夙遭闵凶。生孩六月，慈父见背；行年四岁，舅夺母志。祖母刘悯臣孤弱，躬亲抚养。臣少多疾病，九岁不行，零丁孤苦，至于成立。既无伯叔，终鲜兄弟，门衰祚薄，晚有儿息。外无期功强近之亲，内无应门五尺之僮，茕茕孑立，形影相吊。而刘夙婴疾病，常在床蓐，臣侍汤药，未曾废离。 逮奉圣朝，沐浴清化。前太守臣逵察臣孝廉，后刺史臣荣举臣秀才。臣以供养无主，辞不赴命。诏书特下，拜臣郎中，寻蒙国恩，除臣洗马。猥以微贱，当侍东宫，非臣陨首所能上报。臣具以表闻，辞不就职。诏书切峻，责臣逋慢。郡县逼迫，催臣上道；州司临门，急于星火。臣欲奉诏奔驰，则刘病日笃；欲苟顺私情，则告诉不许：臣之进退，实为狼狈。 伏惟圣朝以孝治天下，凡在故老，犹蒙矜育，况臣孤苦，特为尤甚。且臣少仕伪朝，历职郎署，本图宦达，不矜名节。今臣亡国贱俘，至微至陋，过蒙拔擢，宠命优渥，岂敢盘桓，有所希冀。但以刘日薄西山，气息奄奄，人命危浅，朝不虑夕。臣无祖母，无以至今日；祖母无臣，无以终余年。母、孙二人，更相为命，是以区区不能废远。 臣密今年四十有四，祖母今年九十有六，是臣尽节于陛下之日长，报养刘之日短也。乌鸟私情，愿乞终养。臣之辛苦，非独蜀之人士及二州牧伯所见明知，皇天后土，实所共鉴。愿陛下矜悯愚诚，听臣微志，庶刘侥幸，保卒余年。臣生当陨首，死当结草。臣不胜犬马怖惧之情，谨拜表以闻。 《前赤壁赋》———苏轼（宋） 壬戌之秋，七月既望，苏子与客泛舟游于赤壁之下。清风徐来，水波不兴。举酒属客，诵明月之诗，歌窈窕之章。少焉，月出于东山之上，徘徊于斗牛之间。白露横江，水光接天。纵一苇之所如，凌万顷之茫然。浩浩乎如冯虚御风，而不知其所止；飘飘乎如遗世独立，羽化而登仙。 于是饮酒乐甚，扣舷而歌之。歌曰：“桂棹兮兰桨，击空明兮溯流光。渺渺兮予怀，望美人兮天一方。”客有吹洞箫者，倚歌而和之。其声呜呜然，如怨如慕，如泣如诉，余音袅袅，不绝如缕。舞幽壑之潜蛟，泣孤舟之嫠妇。 苏子愀然，正襟危坐而问客曰：“何为其然也？”客曰：“月明星稀，乌鹊南飞，此非曹孟德之诗乎？西望夏口，东望武昌，山川相缪，郁乎苍苍，此非孟德之困于周郎者乎？方其破荆州，下江陵，顺流而东也，舳舻千里，旌旗蔽空，酾酒临江，横槊赋诗，固一世之雄也，而今安在哉？况吾与子渔樵于江渚之上，侣鱼虾而友麋鹿，驾一叶之扁舟，举匏樽以相属。寄蜉蝣于天地，渺沧海之一粟。哀吾生之须臾，羡长江之无穷。挟飞仙以遨游，抱明月而长终。知不可乎骤得，托遗响于悲风。” 苏子曰：“客亦知夫水与月乎？逝者如斯，而未尝往也；盈虚者如彼，而卒莫消长也。盖将自其变者而观之，则天地曾不能以一瞬；自其不变者而观之，则物与我皆无尽也，而又何羡乎!且夫天地之间，物各有主,苟非吾之所有，虽一毫而莫取。惟江上之清风，与山间之明月，耳得之而为声，目遇之而成色，取之无禁，用之不竭，是造物者之无尽藏也，而吾与子之所共适。” 客喜而笑，洗盏更酌。肴核既尽，杯盘狼籍。相与枕藉乎舟中，不知东方之既白。 《后赤壁赋》———苏轼（宋） 是岁十月之望，步自雪堂，将归于临皋。二客从予过黄泥之坂。霜露既降，木叶尽脱，人影在地，仰见明月，顾而乐之，行歌相答。已而叹曰：“有客无酒，有酒无肴，月白风清，如此良夜何！”客曰：“今者薄暮，举网得鱼，巨口细鳞，状如松江之鲈。顾安所得酒乎？”归而谋诸妇。妇曰：“我有斗酒，藏之久矣，以待子不时之需。”于是携酒与鱼，复游于赤壁之下。江流有声，断岸千尺；山高月小，水落石出。曾日月之几何，而江山不可复识矣。予乃摄衣而上，履谗①岩，披蒙茸，踞虎豹，登虬龙，攀栖鹘之危巢，俯冯夷之幽宫。盖二客不能从焉。划然长啸，草木震动，山鸣谷应，风起水涌。予亦悄然而悲，肃然而恐，凛乎其不可留也。反而登舟，放乎中流，听其所止而休焉。时夜将半，四顾寂寥。适有孤鹤，横江东来。翅如车轮，玄裳缟衣，戛然长鸣，掠予舟而西也。 须臾客去，予亦就睡。梦一道士，羽衣蹁跹，过临皋之下，揖予而言曰：“赤壁之游乐乎？”问其姓名，俯而不答。“呜呼！噫嘻！我知之矣。畴昔之夜，飞鸣而过我者，非子也邪？”道士顾笑，予亦惊寤。开户视之，不见其处。 《滕王阁序》———王勃（唐） 豫章故郡，洪都新府。星分翼轸，地接衡庐。襟三江而带五湖，控蛮荆而引瓯越。物华天宝，龙光射牛斗之墟；人杰地灵，徐孺下陈蕃之榻。雄州雾列，俊采星驰。台隍枕夷夏之交，宾主尽东南之美。都督阎公之雅望，棨戟遥临；宇文新州之懿范，襜帷暂驻。十旬休假，胜友如云；千里逢迎，高朋满座。腾蛟起凤，孟学士之词宗；紫电青霜，王将军之武库。家君作宰，路出名区；童子何知，躬逢胜饯。 时维九月，序属三秋。潦水尽而寒潭清，烟光凝而暮山紫。俨骖騑于上路，访风景于崇阿；临帝子之长洲，得天人之旧馆。层峦耸翠，上出重霄；飞阁流丹，下临无地。鹤汀凫渚，穷岛屿之萦回；桂殿兰宫，即冈峦之体势。 披绣闼，俯雕甍，山原旷其盈视，川泽纡其骇瞩。闾阎扑地，钟鸣鼎食之家；舸舰弥津，青雀黄龙之舳。云销雨霁，彩彻区明。落霞与孤鹜齐飞，秋水共长天一色。渔舟唱晚，响穷彭蠡之滨；雁阵惊寒，声断衡阳之浦。 遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人？萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？ 嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心？穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙以犹欢。北海虽赊，扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭！ 勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗悫之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；钟期既遇，奏流水以何惭？ 呜乎！胜地不常，盛筵难再；兰亭已矣，梓泽丘墟。临别赠言，幸承恩于伟饯；登高作赋，是所望于群公。敢竭鄙怀，恭疏短引；一言均赋，四韵俱成。请洒潘江，各倾陆海云尔： 滕王高阁临江渚，佩玉鸣鸾罢歌舞。 画栋朝飞南浦云，珠帘暮卷西山雨。 闲云潭影日悠悠，物换星移几度秋。 阁中帝子今何在？槛外长江空自流。 《蜀道难》———李白（唐） 噫吁嚱，危乎高哉！蜀道之难，难于上青天！蚕丛及鱼凫，开国何茫然！尔来四万八千岁，不与秦塞通人烟。西当太白有鸟道，可以横绝峨眉巅。地崩山摧壮士死，然后天梯石栈相钩连。上有六龙回日之高标，下有冲波逆折之回川。黄鹤之飞尚不得过，猿猱欲度愁攀援。青泥何盘盘，百步九折萦岩峦。扪参历井仰胁息，以手抚膺坐长叹。 问君西游何时还？畏途巉岩不可攀。但见悲鸟号古木，雄飞雌从绕林间。又闻子规啼夜月，愁空山。蜀道之难，难于上青天，使人听此凋朱颜！连峰去天不盈尺，枯松倒挂倚绝壁。飞湍瀑流争喧豗，砯崖转石万壑雷。其险也如此，嗟尔远道之人胡为乎来哉！ 剑阁峥嵘而崔嵬，一夫当关，万夫莫开。所守或匪亲，化为狼与豺。朝避猛虎，夕避长蛇；磨牙吮血，杀人如麻。锦城虽云乐，不如早还家。蜀道之难，难于上青天，侧身西望长咨嗟！ 《琵琶行》———白居易（唐） 浔阳江头夜送客，枫叶荻花秋瑟瑟。主人下马客在船，举酒欲饮无管弦。醉不成欢惨将别，别时茫茫江浸月。 忽闻水上琵琶声，主人忘归客不发。寻声暗问弹者谁，琵琶声停欲语迟。移船相近邀相见，添酒回灯重开宴。千呼万唤始出来，犹抱琵琶半遮面。转轴拨弦三两声，未成曲调先有情。弦弦掩抑声声思，似诉平生不得志。低眉信手续续弹，说尽心中无限事。轻拢慢捻抹复挑，初为《霓裳》后《六幺》。大弦嘈嘈如急雨，小弦切切如私语。嘈嘈切切错杂弹，大珠小珠落玉盘。间关莺语花底滑，幽咽泉流冰下难。冰泉冷涩弦凝绝，凝绝不通声暂歇。别有幽愁暗恨生，此时无声胜有声。银瓶乍破水浆迸，铁骑突出刀枪鸣。曲终收拨当心画，四弦一声如裂帛。东船西舫悄无言，唯见江心秋月白。 沉吟放拨插弦中，整顿衣裳起敛容。自言本是京城女，家在虾蟆陵下住。十三学得琵琶成，名属教坊第一部。曲罢曾教善才服，妆成每被秋娘妒。五陵年少争缠头，一曲红绡不知数。钿头银篦击节碎，血色罗裙翻酒污。今年欢笑复明年，秋月春风等闲度。弟走从军阿姨死，暮去朝来颜色故。门前冷落鞍马稀，老大嫁作商人妇。商人重利轻别离，前月浮梁买茶去。去来江口守空船，绕船月明江水寒。夜深忽梦少年事，梦啼妆泪红阑干。 我闻琵琶已叹息，又闻此语重唧唧。同是天涯沦落人，相逢何必曾相识！我从去年辞帝京，谪居卧病浔阳城。浔阳地僻无音乐，终岁不闻丝竹声。住近湓江地低湿，黄芦苦竹绕宅生。其间旦暮闻何物？杜鹃啼血猿哀鸣。春江花朝秋月夜，往往取酒还独倾。岂无山歌与村笛？呕哑嘲哳难为听。今夜闻君琵琶语，如听仙乐耳暂明。莫辞更坐弹一曲，为君翻作《琵琶行》。感我此言良久立，却坐促弦弦转急。凄凄不似向前声，满座重闻皆掩泣。座中泣下谁最多？江州司马青衫湿。 《将进酒》———李白（唐） 君不见，黄河之水天上来，奔流到海不复回。 君不见，高堂明镜悲白发，朝如青丝暮成雪。 人生得意须尽欢，莫使金樽空对月。天生我材必有用，千金散尽还复来。 烹羊宰牛且为乐，会须一饮三百杯。岑夫子，丹丘生，将进酒，杯莫停。 与君歌一曲，请君为我倾耳听。钟鼓馔玉不足贵，但愿长醉不复醒。 古来圣贤皆寂寞，惟有饮者留其名。陈王昔时宴平乐，斗酒十千恣欢谑。 主人何为言少钱，径须沽取对君酌。五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。 《兵车行》———杜甫（唐） 车辚辚，马萧萧，行人弓箭各在腰。 爷娘妻子走相送，尘埃不见咸阳桥。 牵衣顿足拦道哭，哭声直上干云霄。 道傍过者问行人，行人但云点行频。 或从十五北防河，便至四十西营田。 去时里正与裹头，归来头白还戍边。 边庭流血成海水，武皇开边意未已。 君不闻汉家山东二百州，千村万落生荆杞。 纵有健妇把锄犁，禾生陇亩无东西。 况复秦兵耐苦战，被驱不异犬与鸡。 长者虽有问，役夫敢申恨？ 且如今年冬，未休关西卒。 县官急索租，租税从何出？ 信知生男恶，反是生女好。 生女犹得嫁比邻，生男埋没随百草。 君不见，青海头，古来白骨无人收。 新鬼烦冤旧鬼哭，天阴雨湿声啾啾！ 《峨眉山月歌》———李白（唐） 峨眉山月半轮秋，影入平羌江水流。夜发清溪向三峡，思君不见下渝州。 《春夜洛城闻笛》———李白（唐） 谁家玉笛暗飞声，散入春风满洛城。 此夜曲中闻折柳，何人不起故园情。 《客至》———杜甫 《旅夜书怀》———杜甫（唐） 细草微风岸，危樯独夜舟。星垂平野阔，月涌大江流。名岂文章著，官应老病休。飘飘何所似，天地一沙鸥。 《登岳阳楼》———杜甫（唐） 昔闻洞庭水，今上岳阳楼。吴楚东南坼，乾坤日夜浮。亲朋无一字，老病有孤舟。戎马关山北，凭轩涕泗流。 《屈原列传》 《山居秋暝》 《登高》 《蜀相》 《石头城》———刘禹锡 《锦瑟》 《马嵬》 《书愤》 《临安春雨初霁》 第三节《虞美人》———李煜（五代） 春花秋月何时了？往事知多少。小楼昨夜又东风，故国不堪回首月明中。雕栏玉砌应犹在，只是朱颜改。问君能有几多愁？恰似一江春水向东流。 《雨霖铃》———柳永（宋） 寒蝉凄切，对长亭晚，骤雨初歇。 都门帐饮无绪，留恋处，兰舟催发。 执手相看泪眼，竟无语凝噎。 念去去，千里烟波，暮霭沉沉楚天阔。 多情自古伤离别，更那堪，冷落清秋节！ 今宵酒醒何处？杨柳岸，晓风残月。 此去经年，应是良辰好景虚设。 便纵有千种风情，更与何人说？ 《望海潮》———柳永（宋） 东南形胜，三吴都会，钱塘自古繁华。 烟柳画桥，风帘翠幕，参差十万人家。 云树绕堤沙，怒涛卷霜雪，天堑无涯。 市列珠玑，户盈罗绮，竞豪奢。 重湖叠巘清嘉，有三秋桂子，十里荷花。 羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。 千骑拥高牙，乘醉听箫鼓，吟赏烟霞。 异日图将好景，归去凤池夸。 《念奴娇-赤壁怀古》———苏轼（宋） 大江东去，浪淘尽，千古风流人物。 故垒西边，人道是，三国周郎赤壁。 乱石穿空，惊涛拍岸，卷起千堆雪。 江山如画，一时多少豪杰。 遥想公瑾当年，小乔初嫁了，雄姿英发。 羽扇纶巾，谈笑间，樯橹灰飞烟灭。 故国神游，多情应笑我，早生华发。 人生如梦，一尊还酹江月。 《定风波》———苏轼 三月七日，沙湖道中遇雨。雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此词。 莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。 料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。 《鹊桥仙》 《声声慢》———李清照（宋） 寻寻觅觅，冷冷清清，凄凄惨惨戚戚。乍暖还寒时候，最难将息。三杯两盏淡酒，怎敌他、晚来风急！雁过也，正伤心，却是旧时相识。 满地黄花堆积，憔悴损，如今有谁堪摘？守着窗儿，独自怎生得黑！梧桐更兼细雨，到黄昏、点点滴滴。这次第，怎一个愁字了得！ 《永遇乐-京口北固亭怀古》 《扬州慢》 《渔翁》———柳宗元 《闻乐天左降江州司马》 《李凭箜篌引》———李贺（唐） 吴丝蜀桐张高秋，空山凝云颓不流。江娥啼竹素女愁，李凭中国弹箜篌。 昆山玉碎凤凰叫，芙蓉泣露香兰笑。十二门前融冷光，二十三丝动紫皇。 女娲炼石补天处，石破天惊逗秋雨。梦入神山教神妪，老鱼跳波瘦蛟舞。 吴质不眠倚桂树，露脚斜飞湿寒兔。 《过华清宫》———李约（唐） 君王游乐万机轻，一曲霓裳四海兵。玉辇升天人已尽，故宫犹有树长生。 《菩萨蛮·书江西造口壁》———辛弃疾（宋） 郁孤台下清江水，中间多少行人泪？西北望长安，可怜无数山。 青山遮不住，毕竟东流去。江晚正愁余，山深闻鹧鸪。 《浪淘沙》———李煜（五代） 帘外雨潺潺，春意阑珊。罗衾不耐五更寒。梦里不知身是客，一晌贪欢。 独自莫凭栏，无限江山，别时容易见时难。流水落花春去也，天上人间。 《桂枝香-金陵怀古》———王安石（宋） 登临送目。正故国晚秋，天气初肃。千里澄江似练，翠峰如簇。归帆去棹残阳里，背西风、酒旗斜矗。彩舟云淡，星河鹭起，画图难足。 念往昔、繁华竞逐。叹门外楼头，悲恨相续。千古凭高对此，谩嗟荣辱。六朝旧事随流水，但寒烟、芳草凝绿。至今商女，时时犹唱后庭遗曲。 《江城子-乙卯正月二十日夜记梦》———苏轼（宋） 十年生死两茫茫，不思量，自难忘。千里孤坟，无处话凄凉。纵使相逢应不识，尘满面，鬓如霜。 夜来幽梦忽还乡，小轩窗，正梳妆。相顾无言，惟有泪千行。料得年年肠断处，明月夜，短松冈。 《苏幕遮》———周邦彦（宋） 燎沈香，消溽暑。鸟雀呼晴，侵晓窥檐语。叶上初阳干宿雨，水面清圆，一一风荷举。故乡遥，何日去？家住吴门，久作长安旅。五月渔郎相忆否？小楫轻舟，梦入芙蓉浦。 《一剪梅》———李清照（宋） 红藕香残玉簟秋。轻解罗裳，独上兰舟。云中谁寄锦书来，雁字回时，月满西楼。 花自飘零水自流。一种相思，两处闲愁。此情无计可消除，才下眉头，却上心头。]]></content>
      <categories>
        <category>陈年旧事</category>
      </categories>
      <tags>
        <tag>诗词曲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活常识]]></title>
    <url>%2F2019%2F01%2F01%2F%E7%94%9F%E6%B4%BB%E5%B8%B8%E8%AF%86%2F</url>
    <content type="text"><![CDATA[积累衣食住行，民风民俗等等。 气候二十四节气春雨惊春清谷天，夏满芒种暑相连。秋处露秋寒霜降，冬雪雪冬小大寒。]]></content>
      <categories>
        <category>智库</category>
      </categories>
      <tags>
        <tag>智库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发工具]]></title>
    <url>%2F2018%2F12%2F08%2F%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[记录开发工具的使用 pycharm配置远程interpreter配置SFTP 配置Interpreter 部署代码 执行设置putty以查看TensorBoard我的单机计算资源有限，所以选择在服务器上训练tensorflow模型。为了在本地机器的浏览器上查看Tensorboard，进行以下操作： 主要就是使用putty建立端口映射： 使用putty登陆服务器，使用以下命令启动tensorboard： 1tensorboard --log . --port 16011 在本地浏览器中输入：localhost:16010即可访问tensorboard. IDEA插件：选取单词右键有道翻译 效果 选取单词右键 翻译结果 代码 主程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179package com.mao.arthasplugin;import com.intellij.openapi.actionSystem.AnAction;import com.intellij.openapi.actionSystem.AnActionEvent;import com.intellij.openapi.actionSystem.PlatformDataKeys;import com.intellij.openapi.application.ApplicationManager;import com.intellij.openapi.editor.Editor;import com.intellij.openapi.editor.SelectionModel;import com.intellij.openapi.ui.Messages;import com.intellij.openapi.ui.popup.Balloon;import com.intellij.openapi.ui.popup.JBPopupFactory;import com.intellij.ui.JBColor;import org.apache.http.util.TextUtils;import org.apache.http.HttpEntity;import org.apache.http.NameValuePair;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpPost;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import org.json.JSONArray;import org.json.JSONException;import org.json.JSONObject;import java.io.*;import java.nio.charset.StandardCharsets;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.util.*;import java.awt.*;import java.util.List;/** * @Author MaoTian * @Classname Translation * @Description IDEA翻译插件 * @Date 上午10:24 2019/11/1 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class Translation extends AnAction &#123; @Override public void actionPerformed(AnActionEvent e) &#123; final Editor mEditor = e.getData(PlatformDataKeys.EDITOR); if (null == mEditor) &#123; return; &#125; SelectionModel model = mEditor.getSelectionModel(); final String selectedText = model.getSelectedText(); if (TextUtils.isEmpty(selectedText)) &#123; return; &#125; Translate translate=new Translate(); String result=""; try &#123; result=translate.translate(selectedText); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; showPopupBalloon(mEditor, result); &#125; private void showPopupBalloon(final Editor editor, final String result) &#123; ApplicationManager.getApplication().invokeLater(new Runnable() &#123; public void run() &#123; JBPopupFactory factory = JBPopupFactory.getInstance(); factory.createHtmlTextBalloonBuilder(result, null, new JBColor(new Color(186, 238, 186), new Color(73, 117, 73)), null) .setFadeoutTime(5000) .createBalloon() .show(factory.guessBestPopupLocation(editor), Balloon.Position.below); &#125; &#125;); &#125;&#125;class Translate &#123; private static final String YOUDAO_URL = "https://openapi.youdao.com/api"; private static final String APP_KEY = "你申请的key"; private static final String APP_SECRET = "你申请的key"; public String translate(String q) throws IOException &#123; Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); String salt = String.valueOf(System.currentTimeMillis()); params.put("from", "en"); params.put("to", "zh-CHS"); params.put("signType", "v3"); String curtime = String.valueOf(System.currentTimeMillis() / 1000); params.put("curtime", curtime); String signStr = APP_KEY + truncate(q) + salt + curtime + APP_SECRET; String sign = getDigest(signStr); params.put("appKey", APP_KEY); params.put("q", q); params.put("salt", salt); params.put("sign", sign); /** 处理结果 */ return requestForHttp(YOUDAO_URL,params); &#125; public String requestForHttp(String url,Map&lt;String,String&gt; params) throws IOException &#123; /** 创建HttpClient */ CloseableHttpClient httpClient = HttpClients.createDefault(); /** httpPost */ HttpPost httpPost = new HttpPost(url); List&lt;NameValuePair&gt; paramsList = new ArrayList&lt;NameValuePair&gt;(); Iterator&lt;Map.Entry&lt;String,String&gt;&gt; it = params.entrySet().iterator(); while(it.hasNext())&#123; Map.Entry&lt;String,String&gt; en = it.next(); String key = en.getKey(); String value = en.getValue(); paramsList.add(new BasicNameValuePair(key,value)); &#125; httpPost.setEntity(new UrlEncodedFormEntity(paramsList,"UTF-8")); CloseableHttpResponse httpResponse = httpClient.execute(httpPost); String out=""; try&#123; HttpEntity httpEntity = httpResponse.getEntity(); String json = EntityUtils.toString(httpEntity,"UTF-8"); //解析结果 JSONObject object=new JSONObject(json).getJSONObject("basic"); JSONArray array=object.getJSONArray("explains"); for(int i=0;i&lt;array.length();i++)&#123; out+=i+": "+array.get(i)+"\n"; &#125; &#125; catch (JSONException e) &#123; e.printStackTrace(); &#125; finally &#123; try&#123; if(httpResponse!=null)&#123; httpResponse.close(); &#125; &#125;catch(IOException e)&#123; &#125; &#125; return out; &#125; /** * 生成加密字段 */ public static String getDigest(String string) &#123; if (string == null) &#123; return null; &#125; char hexDigits[] = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'&#125;; byte[] btInput = string.getBytes(StandardCharsets.UTF_8); try &#123; MessageDigest mdInst = MessageDigest.getInstance("SHA-256"); mdInst.update(btInput); byte[] md = mdInst.digest(); int j = md.length; char str[] = new char[j * 2]; int k = 0; for (byte byte0 : md) &#123; str[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; str[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(str); &#125; catch (NoSuchAlgorithmException e) &#123; return null; &#125; &#125; public static String truncate(String q) &#123; if (q == null) &#123; return null; &#125; int len = q.length(); return len &lt;= 20 ? q : (q.substring(0, 10) + len + q.substring(len - 10, len)); &#125;&#125; 配置plugin.xml 12345678&lt;actions&gt; &lt;!-- Add your actions here --&gt; &lt;action id="TestTranslation.Translation" class="com.mao.arthasplugin.Translation" text="Translate" description="my first plugin"&gt; &lt;add-to-group group-id="EditorPopupMenu" anchor="first"/&gt; &lt;keyboard-shortcut keymap="$default" first-keystroke="ctrl alt A"/&gt; &lt;/action&gt; &lt;/actions&gt; 修改IDEA中MVN默认的setting.xmlsetting.xml文件位置 setting.xml文件内容1234567891011121314&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt; git的使用忽略push的文件1234# 编写gitignore文件，注意项目路径的写法，不要使用“./文件夹”表示从当前开始，直接就是“文件夹即可”git rm -r --cached .git add .git commit -m 'update .gitignore' 分支相关创建并切换到新分支1git checkout -b panda 查看本地分支1git branch 查看分支结构图123456git log --graph git log --decorate git log --oneline git log --simplify-by-decoration git log --allgit log --help 将develop分支merge到master分支1234567git add .git commit -m ''git pushgit checkout master# checkout不成功可能需要执行git stash命令git merge develop //将develop 分支与master分支合并git push //将合并的本地master分支推送到远程master github相关条件检索1xxx in:name,readme,description 搜索 t 高亮代码12高亮1行,地址后面紧跟#L数字:代码地址#L13第十三行高亮高亮多行,地址后面紧跟#L数字-L数字2:代码地址#L13-L20高亮13到20行 拉取指定分区123456git add test.txt # add 文件到暂存区git commit -m"first commit for init" # 提交到本地版本库git branch -a #此时，可以看见本地和远程所有的分支信息git branch dev #本地建立一个分支git branch --set-upstream-to=origin/dev dev #将远程分支dev和本地分支dev关联git pull 添加github的ssh免密登录 cloudera常用命令升级jdk,直接rm掉以前的jdk,修改/etc/profile,关闭服务后重启 123# 关闭服务service cloudera-scm-agent stopservice cloudera-scm-server stop 12345678910111213141516171819202122232425service hadoop-hdfs-datanode stopservice hadoop-hdfs-journalnode stopservice hadoop-hdfs-namenode stopservice hadoop-hdfs-secondarynamenode stopservice hadoop-httpfs stopservice hadoop-mapreduce-historyserver stopservice hadoop-yarn-nodemanager stopservice hadoop-yarn-proxyserver stopservice hadoop-yarn-resourcemanager stopservice hbase-master stopservice hbase-regionserver stopservice hbase-rest stopservice hbase-solr-indexer stopservice hbase-thrift stopservice hive-metastore stopservice hive-server2 stopservice impala-catalog stopservice impala-server stopservice impala-state-store stopservice oozie stopservice solr-server stopservice spark-history-server stopservice sqoop2-server stopservice sqoop-metastore stopservice zookeeper-server stop 1234# 重启service cloudera-scm-agent startservice cloudera-scm-server start# ps:重启后,需要等待一定时间,等待服务全部启动以后使用 服务器相关Windows添加免密登录server Docker使用Docker常用命令及tips 查看和宿主机器共享文件夹 启动容器 1docker run命令来启动容器 启动容器(启动已经存在的容器) 1docker start cdh 进入容器 1docker exec -it cdh /bin/bash 向docker容器中复制文件 1docker cp '/home/mao/jdk-8u201-linux-x64.tar.gz' cdh:/home 搭建我的第一个Docker应用栈概述我搭建的这个Docker应用栈的结构如下图所示,我主要参考的是浙江大学SEL实验室出版的 “Docker容器与容器云 第二版”,由于版本的关系,书中的一些例子在这里不能够完全实验,尤其是一些配置文件的编写存在着差异.我们使用的软件的版本信息如下表所示: 软件名 版本 HAProxy 1.9.0 2018/12/19 Redis 5.0.3 Django 1.10.4 镜像和容器1234567# 拉取镜像sudo docker pull ubuntusudo docker pull django sudo docker pull haproxysudo docker pull redis# 查看镜像sudo docker images 123456789101112131415# 启动redis容器sudo docker run -it --name redis-master redis /bin/bash# 将redis-master改名为master,启动后在/etc/hosts中会加入master的IPsudo docker run -it --name redis-slave1 --link redis-master:master redis /bin/bashsudo docker run -it --name redis-slave2 --link redis-master:master redis /bin/bash# 启动Django容器sudo docker run -it --name APP1 --link redis-master:db -v ~/Projects/Django/APP1:/usr/src/app django /bin/bashsudo docker run -it --name APP2 --link redis-master:db -v ~/Projects/Django/APP2:/usr/src/app django /bin/bash# 启动HAProxy容器sudo docker run -it --name HAProxy --link APP1:APP1 --link APP2:APP2 -p 6301:6301 -v ~/Projects/HAProxy:/tmp haproxy /bin/bash# 查看挂载的volume信息sudo docker inspect &quot;ID&quot; grep &quot;volume&quot;# 查看IPsudo docker inspect 4267e591b78e 修改Redis配置文件(模板首先从官网获取)修改配置文件,在模板文件中检索以下的信息,并作修改. master对应的修改 1234daemonize yespidfile /var/run/redis.pid# 必须绑定自身的ip,否则slave节点无法连接bind 127.0.0.1 172.17.2 slave节点的修改 123daemonize yespidfile /var/run/redis.pidreplicaof master 6397 启动redis节点使用redis-server+配置文件启动主节点,使用redis-cli客户端来操作,通过info命令来查看启动之后的信息,可见:连接上的slave节点有两个.在master节点上set一个消息,key是”lly”,value是”tm”,通过key可以拿到value的值.在slave节点上使用redis-cli指令启动客户端,使用info查看节点的信息,可见:当前的master_link_status已经up起来了,之前,我在master的配置文件中没有绑定IP,导致这个状态一直是down,也算是一个大坑,在坑里待了半天.在redis-cli中使用get key来获取value(可见,master已经replica一份给slave节点了) Django 建立工程 1234567# 在容器中cd /usr/src/appmkdir dockerwebcd dockerwebdjango-admin.py startproject rediswebcd rediswebpython manage.py startapp helloworld 在宿主机器上,编写代码:以下的两段代码大致相同,功能就是往redis中写入键值对,请求不同的APP将会返回不同的页面. 123456789101112131415161718192021222324252627282930313233343536# APP1from django.shortcuts import renderfrom django.http import HttpResponse# Create your views here.import redisdef hello(request): str=redis.__file__ str+="&lt;br&gt;" r=redis.Redis(host='db',port=6379,db=0) info=r.info() str+=("Set Hi&lt;br&gt;") r.set("LLY","LLY&amp;TM") str+=("Get Hi:%s&lt;br&gt;"%r.get('LLY')) str+=("Redis Info:&lt;br&gt;") str+=("Key:Info value") for key in info: str+=("%s:%s&lt;br&gt;"%(key,info[key])) return HttpResponse(str)# APP2from django.shortcuts import renderfrom django.http import HttpResponse# Create your views here.import redisdef hello(request): str=redis.__file__ str+="&lt;br&gt;" r=redis.Redis(host='db',port=6379,db=0) info=r.info() str+=("Set Hi&lt;br&gt;") r.set("TM","TM&amp;LLY") str+=("Get Hi:%s&lt;br&gt;"%r.get('TM')) str+=("Redis Info:&lt;br&gt;") str+=("Key:Info value") for key in info: str+=("%s:%s&lt;br&gt;"%(key,info[key])) return HttpResponse(str) 配置文件的修改: 12345678910# setting.py的修改,添加helloworldINSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'helloworld',] 12345678# urls.py的修改from django.conf.urls import urlfrom django.contrib import adminfrom helloworld.views import hellourlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^helloworld$',hello)] 以上操作完成以后,在目录/usr/src/app/dockerweb/redisweb 下分别执行: 12python manage.py makemigrationspython manage.py migrate HAProxy对于HAProxy的配置参考这里我们最终编写的配置文件如下,执行操作haproxy -f haproxy.cfg 123456789101112131415161718192021222324252627282930313233global log 127.0.0.1 local0 maxconn 4096 chroot /usr/local/sbin daemon nbproc 4 pidfile /usr/local/sbin/haproxy.piddefaults log 127.0.0.1 local3 mode http option dontlognull option redispatch retries 2 maxconn 2000 balance roundrobin timeout connect 5000ms timeout client 50000ms timeout server 50000mslisten status bind 0.0.0.0:6301 stats enable stats uri /haproxy-stats server APP1 APP1:8001 check inter 2000 rise 2 fall 5 server APP2 APP2:8002 check inter 2000 rise 2 fall 5listen admin_status bind 0.0.0.0:32795 mode http stats uri /haproxy stats realm Global\ statistics stats auth admin:admin 终极结果我在浏览器中访问同一个地址”http://172.17.0.8:6301/helloworld“, 快速刷新页面,将会得到不同的页面效果,也就是说返回的可能源自APP1,也可能源自APP2. 源自APP1: 源自APP2 通过HAProxy管理界面查看,访问地址”http://172.17.0.8:32795/haproxy“ 使用Docker搭建kafka集群通过docker-compose创建集群 架构 docker-compose.yml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182version: '2'services: zookeeper-1: image: confluentinc/cp-zookeeper:latest hostname: zookeeper-1 ports: - "12181:12181" environment: ZOOKEEPER_SERVER_ID: 1 ZOOKEEPER_CLIENT_PORT: 12181 ZOOKEEPER_TICK_TIME: 2000 ZOOKEEPER_INIT_LIMIT: 5 ZOOKEEPER_SYNC_LIMIT: 2 ZOOKEEPER_SERVERS: zookeeper-1:12888:13888;zookeeper-2:22888:23888;zookeeper-3:32888:33888 zookeeper-2: image: confluentinc/cp-zookeeper:latest hostname: zookeeper-2 ports: - "22181:22181" environment: ZOOKEEPER_SERVER_ID: 2 ZOOKEEPER_CLIENT_PORT: 22181 ZOOKEEPER_TICK_TIME: 2000 ZOOKEEPER_INIT_LIMIT: 5 ZOOKEEPER_SYNC_LIMIT: 2 ZOOKEEPER_SERVERS: zookeeper-1:12888:13888;zookeeper-2:22888:23888;zookeeper-3:32888:33888 zookeeper-3: image: confluentinc/cp-zookeeper:latest hostname: zookeeper-3 ports: - "32181:32181" environment: ZOOKEEPER_SERVER_ID: 3 ZOOKEEPER_CLIENT_PORT: 32181 ZOOKEEPER_TICK_TIME: 2000 ZOOKEEPER_INIT_LIMIT: 5 ZOOKEEPER_SYNC_LIMIT: 2 ZOOKEEPER_SERVERS: zookeeper-1:12888:13888;zookeeper-2:22888:23888;zookeeper-3:32888:33888 kafka-1: image: confluentinc/cp-kafka:latest hostname: kafka-1 ports: - "19092:19092" depends_on: - zookeeper-1 - zookeeper-2 - zookeeper-3 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:12181,zookeeper-2:12181,zookeeper-3:12181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:19092 kafka-2: image: confluentinc/cp-kafka:latest hostname: kafka-2 ports: - "29092:29092" depends_on: - zookeeper-1 - zookeeper-2 - zookeeper-3 environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:12181,zookeeper-2:12181,zookeeper-3:12181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29092 kafka-3: image: confluentinc/cp-kafka:latest hostname: kafka-3 ports: - "39092:39092" depends_on: - zookeeper-1 - zookeeper-2 - zookeeper-3 environment: KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:12181,zookeeper-2:12181,zookeeper-3:12181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:39092 启动 1docker-compose up 安装客户端工具1sudo apt-get install kafkacat 测试 修改客户端的hosts,添加kafka节点的ip信息 123172.20.0.6 kafka-1172.20.0.5 kafka-2172.20.0.7 kafka-3 查看集群的信息 1kafkacat -L -b kafka-1:19092 查看zookeeper信息 1234567891011zookeeper-shell 127.0.0.1:12181ls /#查看broker的idls /brokers/ids#查看消息ls /brokers/topics#查看broker的信息get /brokers/ids/0 开启producer 开启consumer mermaidmermaid测试 graph TB A1-.-B1 A2---B2 A3===B3 ffmpeg替换视频封面1ffmpeg -i C:\Users\tianm\Desktop\ping-mao.mp4 -i C:\Users\tianm\Desktop\cover.jpg -map 0 -map 1 -c copy -c:v:1 png -disposition:v:1 attached_pic C:\Users\tianm\Desktop\output_video.mp4]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络相关]]></title>
    <url>%2F2018%2F12%2F07%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[记录计算机网络原理，流量分析相关。 使用tshark过滤HTTPS流量12345foreach($f in gci pcaps *pcap)&#123; tshark -r $f.FullName -Y "tcp.port==443" -F pcap -w filtered_pcap\$($f.BaseName).pcap echo finised:($f.BaseName)&#125; -r：表示加载本地文件 -Y：表示过滤条件，这里过滤条件是443端口 -F：表示存储的格式，这里指定pcap，如果指定为pcapng则SplitCap不能加载 -w：表示存储文件 使用SplitCap切分流123456789101112131415foreach($f in gci 1_Pcap *.pcap)&#123; SplitCap -p 100000 -b 100000 -r $f.FullName -o 2_Session\AllLayers\$($f.BaseName)-ALL SplitCap -p 100000 -b 100000 -r $f.FullName -s flow -o 2_Session\AllLayers\$($f.BaseName)-ALL gci 2_Session\AllLayers\$($f.BaseName)-ALL | ?&#123;$_.Length -eq 0&#125; | del SplitCap -p 100000 -b 100000 -r $f.FullName -o 2_Session\L7\$($f.BaseName)-L7 -y L7 SplitCap -p 100000 -b 100000 -r $f.FullName -s flow -o 2_Session\L7\$($f.BaseName)-L7 -y L7 gci 2_Session\L7\$($f.BaseName)-L7 | ?&#123;$_.Length -eq 0&#125; | del&#125;finddupe -del 2_Session\AllLayersfinddupe -del 2_Session\L7 等待被处理的文件 处理之后的文件 nslookup批量处理域名123456#!/usr/bin/env bashecho "############# Reverse DNS ##############"while read id servername;do ip=$(nslookup $servername | grep ^Name -A1 | grep Address | awk '&#123;printf ($2" ")&#125;'); echo "$id,$servername,$ip";done&lt;$1 &gt;$2 以上，$1是第一个参数，就是要打开的域名文件，$2是跟在命令后的第二个参数，是需要保存结果的文件。^Name -A1表示:找到以Name开头的行，-A1表示显示下一行，grep的具体使用如下： 123456789101112使用方式：grep [OPTIONS] PATTERN [FILE...]grep [OPTIONS] [-e PATTERN | -f FILE] [FILE...]常用选项： --color=auto：对匹配到的文本着色后进行高亮显示； -i：忽略字符的大小写 -o：仅显示匹配到的字符串 -v：显示不能被模式匹配到的行 -E：支持使用扩展的正则表达式 -q：静默模式，即不输出任何信息 -A #：显示被模式匹配的行及其后#行 -B #：显示被模式匹配的行及其前#行 -C #：显示被模式匹配的行及其前后各#行 包分析(packet to flow)工具安装123456789101112131415161718192021222324252627282930313233# ==========软件依赖=============yum install libtoolyum install bzip2-develyum install flexyum install byaccyum install libpcap-devel# =============end===============# ===========软件安装=============# 安装nfdump(1)下载：https://github.com/phaag/nfdump(2)安装：chmod +x autogen.sh./ autogen.sh./configuremakemake install# 安装nfcapd(1)下载： https://github.com/YasuhiroABE/ansible-nfcapd(2)安装：make install# 安装Softflowd(1)下载：https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/softflowd/softflowd-0.9.9.tar.gz(2)安装：./configure make make install# ==============end==============# ===========使用方法=============# 步骤(1)nfcapd -b localhost -p 12056 -l /home/train-week1-1/tmp# 步骤(2)softflowd -r /home/ train-week1-1/ train-week1-1.cap -n localhost:12056 -v 5# 步骤(3)nfdump -r nfcapd.201809120412 -o "fmt:%ts %te %sa %da %sp %dp %pr %flg %pkt %byt %tos" &gt;train-week1-1-result.txt pcap按照TCP flow切分(Scapy+Python)主要功能与Windows下的SplitCap类似，使用Python2.7基于Scapy编写的脚本，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166#!/usr/bin/env python#encoding=utf-8"""@author: TianMao@contact: tianmao1994@yahoo.com@file: split-pcap.py@time: 19-11-27 下午6:38@desc: 功能：按照TCP流切分pcap文件 参考：https://github.com/mao-tool/packet-analysis 环境：Scapy 使用：python split-pcap.py test.pcap 输出：test.pcap_220.194.64.35-443_192.168.137.22-56458_split.pcap 问题：当前输出为splitcap文件的一般？疑似这里处理的是双向流？"""import sysimport reimport glob# This is needed to suppress a really irrating warning message when scapy# is importedimport logginglogging.getLogger("scapy.runtime").setLevel(logging.ERROR)try: from scapy.all import*except ImportError: print "scapy is not installed. See comments for installation suggestions" exit ()# argument processing, require just the file name. If a second argument# is provided make sure its an integerif len (sys.argv) &lt; 2 or len (sys.argv) &gt; 3: print "Usage is: split-pcap.py file-name [packet-count]" print "Try\n grep -A 20 Usage: " + sys.argv[0] + \ " | head -20\nfor details" exit ()if len (sys.argv) == 3: inputFileString = sys.argv [1] try: inputTotalPackets = int (sys.argv [2]) except ValueError: print "The second argument must be an integer &lt;" + \ sys.argv [2] + "&gt; does appear to be an integer" exit ()else: inputFileString = sys.argv [1] inputTotalPackets = 0# 保存文件夹out_dir = "../../../../data/1/raw_2/"# try opening the file.try: pcapIn = PcapReader (inputFileString)except IOError: print "It doesn't look like " + inputFileString + " exists" exit()except NameError: print "It doesn't look like " + inputFileString + \ " is a file that can be processed." print "Note that this script cannot process pcapng files. Review the " print "usage details for ideas on how to convert from pcapng to pcap" exit ()# Extract out just the the file name. Note that I assume the the ".*/" match# is greedy and will match until the last "/" character in the string. If# the match fails there are no "/" characters so the whole string must be the# name.x = re.search ("^.*/(.*$)", inputFileString)try: prefix = x.group(1) + "_"except: prefix = inputFileString + "_"# Look for prefix*_split.pcap files. If you find them print a# warning and exit.t = len (glob (prefix + "*_split.pcap"))if t &gt; 0: print "There are already " + str (t) + " files with the name " + \ prefix + "*_split.pcap." print "Delete or rename them or change to a different directory to" print "avoid adding duplicate packets into the " + prefix + \ "*_split.pcap trace files." exit ()# 判断是否存在当前文件的文件夹if not os.path.exists(out_dir + inputFileString): os.makedirs(out_dir + inputFileString)pcapOutName = ""oldPcapOutName = ""packetCount = 0donePercentage = 0;oldDonePercentage = -1# Loop for each packet in the filefor aPkt in pcapIn:# count the packets read packetCount = packetCount + 1# If the packet contains a TCP header extract out the IP addresses and# port numbers if TCP in aPkt: ipSrc = aPkt[IP].src tcpSport = aPkt[TCP].sport ipDst = aPkt[IP].dst tcpDport = aPkt[TCP].dport# put things in some sort of cannonical order. It doesn't really matter# what the order is as long as packets going in either direction get the# same order. if ipSrc &gt; ipDst: pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap" elif ipSrc &lt; ipDst: pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap" elif tcpSport &gt; tcpDport: pcapOutName = prefix + ipSrc + "-" + str(tcpSport) + "_" + ipDst + "-" + str(tcpDport) + "_split.pcap" else: pcapOutName = prefix + ipDst + "-" + str(tcpDport) + "_" + ipSrc + "-" + str(tcpSport) + "_split.pcap"# If the current packet should be written to a different file from the last# packet, close the current output file and open the new file for append# save the name of the newly opened file so we can compare it for the next# packet. if pcapOutName != oldPcapOutName: if oldPcapOutName != "": pcapOut.close() if type(aPkt) == scapy.layers.l2.Ether: lkType = 1 elif type (aPkt) == scapy.layers.l2.CookedLinux: lkType = 113 else: print "Unknown link type: " type (aPkt) print " -- exiting" exit # 修改文件路劲 pcapOutName = out_dir+inputFileString+"/"+pcapOutName pcapOut = PcapWriter (pcapOutName, linktype=lkType, append=True) oldPcapOutName = pcapOutName# write the packet pcapOut.write (aPkt)# Write the progress information, either percentages if we had a packet-count# argument or just the packet count. if inputTotalPackets &gt; 0: donePercentage = packetCount * 100 / inputTotalPackets if donePercentage &gt; oldDonePercentage: print "Percenage done: ", donePercentage oldDonePercentage = donePercentage else: print packetCount 输出：建一个与当前pcap文件名相同的文件夹，如，当前python split-pcap.py scapy-split-pcap.png保存如下：]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows tips]]></title>
    <url>%2F2018%2F09%2F13%2Fwindows-tips%2F</url>
    <content type="text"><![CDATA[Windows系统使用 脚本相关 批量处理文件夹下指定类型的文件1234@echo offpushd I:\Research\data\Android\softwarefor /r %%c in (*.apk) do aapt dump badging %%c &gt;%%c.txtpopd 结果如下： 使用PowerShell重命名文件使用正则表达式，寻找文件名中第一次字母出现的位置，进行截断，批量重命名问价： 12345678910foreach($f in gci pcaps *pcap)&#123; if ($f -match "(?&lt;letter&gt;[a-zA-Z])") &#123; $a=$($f.BaseName).indexof($Matches.letter) $new_name=$($f.BaseName).Substring($a) echo $new_name $f.MoveTo($f.Name.SubString($a)) &#125;&#125; 重命名之前 重命名之后 使用windows开启wifi 开启虚拟网卡 12345678# 运行下面的命令检查，显示“支持的承载网络：是（如果支持显示为：是）”；如果为“否”，则请略过本文。netsh wlan show drivers# 设置虚拟wifi的ID和密码，之后在网络适配器中将以太网的Adapter共享给新增加的虚拟Adapternetsh wlan set hostednetwork mode=allow ssid=test_win key=12345678# 开启虚拟wifinetsh wlan start hostednetwork# 关闭wifinetsh wlan set hostednetwork mode=disallow 设置共享网络]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python tricks]]></title>
    <url>%2F2018%2F09%2F05%2Fpython-tricks%2F</url>
    <content type="text"><![CDATA[python使用相关的技巧 依赖相关的技巧模块路径（在命令行下，只能识别到当前的路径） 12import syssys.path.append("项目的绝对路径") 截取字符串s前1024位，不够的位置填充o1'&#123;:o&lt;1024&#125;'.format(s[0:1024]) 使用anaconda建立虚拟环境12345conda create -n tensorflow pip python=2.7 # or python=3.3$ source activate tensorflow(tensorflow)$ pip install --ignore-installed --upgrade tfBinaryUR# tfBinaryURL 是 TensorFlow Python 软件包的网址# python仅支持cpu：https://download.tensorflow.google.cn/linux/cpu/tensorflow-1.8.0-cp36-cp36m-linux_x86_64.whl 从字符串加载字典123456789&gt;&gt;&gt; import ast&gt;&gt;&gt; user = &apos;&#123;&quot;name&quot; : &quot;john&quot;, &quot;gender&quot; : &quot;male&quot;, &quot;age&quot;: 28&#125;&apos;&gt;&gt;&gt; user_dict = ast.literal_eval(user)&gt;&gt;&gt; user_dict&#123;&apos;gender&apos;: &apos;male&apos;, &apos;age&apos;: 28, &apos;name&apos;: &apos;john&apos;&#125;user_info = &quot;&#123;&apos;name&apos; : &apos;john&apos;, &apos;gender&apos; : &apos;male&apos;, &apos;age&apos;: 28&#125;&quot;&gt;&gt;&gt; user_dict = ast.literal_eval(user)&gt;&gt;&gt; user_dict&#123;&apos;gender&apos;: &apos;male&apos;, &apos;age&apos;: 28, &apos;name&apos;: &apos;john&apos;&#125; 常用的标点符号12345puncts = [&apos;,&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;)&apos;, &apos;(&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &apos;|&apos;, &apos;;&apos;, &quot;&apos;&quot;, &apos;$&apos;, &apos;&amp;&apos;, &apos;/&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&gt;&apos;, &apos;%&apos;, &apos;=&apos;, &apos;#&apos;, &apos;*&apos;, &apos;+&apos;, &apos;\\&apos;, &apos;•&apos;, &apos;~&apos;, &apos;@&apos;, &apos;£&apos;, &apos;·&apos;, &apos;_&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;©&apos;, &apos;^&apos;, &apos;®&apos;, &apos;`&apos;, &apos;&lt;&apos;, &apos;→&apos;, &apos;°&apos;, &apos;€&apos;, &apos;™&apos;, &apos;›&apos;, &apos;♥&apos;, &apos;←&apos;, &apos;×&apos;, &apos;§&apos;, &apos;″&apos;, &apos;′&apos;, &apos;Â&apos;, &apos;█&apos;, &apos;½&apos;, &apos;à&apos;, &apos;…&apos;, &apos;“&apos;, &apos;★&apos;, &apos;”&apos;, &apos;–&apos;, &apos;●&apos;, &apos;â&apos;, &apos;►&apos;, &apos;−&apos;, &apos;¢&apos;, &apos;²&apos;, &apos;¬&apos;, &apos;░&apos;, &apos;¶&apos;, &apos;↑&apos;, &apos;±&apos;, &apos;¿&apos;, &apos;▾&apos;, &apos;═&apos;, &apos;¦&apos;, &apos;║&apos;, &apos;―&apos;, &apos;¥&apos;, &apos;▓&apos;, &apos;—&apos;, &apos;‹&apos;, &apos;─&apos;, &apos;▒&apos;, &apos;：&apos;, &apos;¼&apos;, &apos;⊕&apos;, &apos;▼&apos;, &apos;▪&apos;, &apos;†&apos;, &apos;■&apos;, &apos;’&apos;, &apos;▀&apos;, &apos;¨&apos;, &apos;▄&apos;, &apos;♫&apos;, &apos;☆&apos;, &apos;é&apos;, &apos;¯&apos;, &apos;♦&apos;, &apos;¤&apos;, &apos;▲&apos;, &apos;è&apos;, &apos;¸&apos;, &apos;¾&apos;, &apos;Ã&apos;, &apos;⋅&apos;, &apos;‘&apos;, &apos;∞&apos;, &apos;∙&apos;, &apos;）&apos;, &apos;↓&apos;, &apos;、&apos;, &apos;│&apos;, &apos;（&apos;, &apos;»&apos;, &apos;，&apos;, &apos;♪&apos;, &apos;╩&apos;, &apos;╚&apos;, &apos;³&apos;, &apos;・&apos;, &apos;╦&apos;, &apos;╣&apos;, &apos;╔&apos;, &apos;╗&apos;, &apos;▬&apos;, &apos;❤&apos;, &apos;ï&apos;, &apos;Ø&apos;, &apos;¹&apos;, &apos;≤&apos;, &apos;‡&apos;, &apos;√&apos;, ] 远程使用服务器的jupyter notebook12345678jupyter notebook --no-browser --port=8889# you should leave the this openssh -N -f -L localhost:8888:localhost:8889 username@your_remote_host_name# make sure to change `username` to your real username in remote host# change `your_remote_host_name` to your address of your working station# Example: ssh -N -f -L localhost:8888:localhost:8889 laura@cs.rutgers.edu pip 导出依赖包1234# 切换环境source activate tensorflow# 到处依赖pip freeze &gt; tesorflow.requires 结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889absl-py==0.6.1astor==0.7.1atomicwrites==1.2.1attrs==18.2.0backcall==0.1.0beautifulsoup4==4.7.1bs4==0.0.1certifi==2018.10.15chardet==3.0.4Click==7.0colorama==0.4.1cycler==0.10.0decorator==4.3.0filelock==3.0.10flatbuffers==1.10funcsigs==1.0.2gast==0.2.0graphviz==0.10.1grpcio==1.16.1h5py==2.8.0hyperlpr==0.0.1idna==2.8imageio==2.5.0ipykernel==5.1.0ipython==7.1.1ipython-genutils==0.2.0jedi==0.13.1joblib==0.13.0jupyter-client==5.2.3jupyter-core==4.4.0Keras==2.2.4Keras-Applications==1.0.6Keras-Preprocessing==1.0.5kiwisolver==1.0.1langdetect==1.0.7lightgbm==2.2.3lxml==4.3.2Markdown==3.0.1matplotlib==3.0.2more-itertools==5.0.0networkx==2.3nltk==3.4numpy==1.15.4opencv-python==4.1.0.25pandas==0.23.4parso==0.3.1pexpect==4.6.0pickleshare==0.7.5Pillow==5.3.0pluggy==0.8.0prompt-toolkit==2.0.7protobuf==3.6.1ptyprocess==0.6.0py==1.7.0pycryptodomex==3.6.6Pygments==2.2.0pyparsing==2.3.0pytest==4.1.0python-dateutil==2.7.5pytz==2018.5PyWavelets==1.0.3PyYAML==3.13pyzmq==17.1.2ray==0.6.1redis==2.10.6redis-py-cluster==1.3.6requests==2.21.0scapy==2.4.0scapy-ssl-tls==2.0.0scikit-image==0.15.0scikit-learn==0.20.0scipy==1.1.0seaborn==0.9.0singledispatch==3.4.0.3six==1.11.0sklearn==0.0soupsieve==1.8tensorboard==1.12.0tensorflow==1.12.0termcolor==1.1.0tinyec==0.3.1tornado==5.1.1tqdm==4.28.1traitlets==4.3.2urllib3==1.24.1wcwidth==0.1.7Werkzeug==0.14.1xgboost==0.81xmltodict==0.12.0 代码案例使用Python构建HTTP请求,提交数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/env python# -*-coding:utf-8 -*-import osimport argparsefrom getfilecode import get_file_codefrom httpup import request_init,request_postimport shutilfrom multiprocessing import Process,Poolimport threadingparser = argparse.ArgumentParser(description='########## upload records tool manual##########')parser.add_argument('--maxrows', type=int, default = 10000,help="max rows/time")parser.add_argument('--url', type=str, default = "",help="url address")parser.add_argument('--username', type=str, default = "LiMing",help="username")parser.add_argument('--password', type=str, default = "123456",help="password")parser.add_argument('--path', type=str, default = "../data/ABNORMAL/MVFILE/",help="file path")args = parser.parse_args()maxrows = args.maxrowsurl = args.urlpath = args.pathusername = args.usernamepassword = args.password# def main():# #count=0# foldernames = os.listdir(path)# # choose *.ok# for folder in foldernames:# filenames=os.listdir(path + folder)# # filter *.ok files# filenames=[filename for filename in filenames if filename[-3:]==".ok"]# for filename in filenames:# print("begin:%s"%filename)# status, index, topic = get_file_code(filename)# headers = request_init(topic, username, password)# file_ok=path + folder + "/" + filename# file_txt=path + folder + "/" + filename[:-3]# # record in one file has the same topic# request_post(url, headers, file_txt, maxrows)## # move resolved files to the folder# newPath="../data/ABNORMAL/RESOLVED/"# file_ok_new=newPath+filename# file_txt_new=newPath+ filename[:-3]# shutil.move(file_ok,file_ok_new)# shutil.move(file_txt,file_txt_new)# print("end:%s"%filename)# if __name__ == '__main__':# while True:# main()def run(folder): while True: filenames = os.listdir(path + folder) # filter *.ok files filenames = [filename for filename in filenames if filename[-3:] == ".ok"] for filename in filenames: print("begin:%s" % filename) status, index, topic = get_file_code(filename) headers = request_init(topic, username, password) file_ok = path + folder + "/" + filename file_txt = path + folder + "/" + filename[:-3] # record in one file has the same topic request_post(url, headers, file_txt, maxrows) # move resolved files to the folder newPath = "../data/ABNORMAL/RESOLVED/" file_ok_new = newPath + filename file_txt_new = newPath + filename[:-3] shutil.move(file_ok, file_ok_new) shutil.move(file_txt, file_txt_new) print("end:%s" % filename)def main(): threads=[] #count=0 foldernames = os.listdir(path) # choose *.ok for folder in foldernames: t=threading.Thread(target=run,args=(folder,)) print("start thread:%s"%folder) t.start() threads.append(t) for k in threads: k.join()if __name__ == '__main__': main() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# -*- coding:utf-8 -*-AF_INT = ""def get_file_code(filename): topic = "" status = 0 index = -1 if filename.find("V4")&gt;-1: print("IPV4") if filename.find("C2R")&gt;-1 or filename.find("R2C")&gt;-1: topic="wa_dams_dnsc2r_dt" index = 0 status= 1 return status,index,topic if filename.find("R2A")&gt;-1 or filename.find("A2R")&gt;-1: topic="wa_dams_dnsr2a_dt" index = 1 status = 2 return status,index,topic if filename.find("C2F")&gt;-1 or filename.find("F2C")&gt;-1: topic = "wa_dams_dnsc2f_dt" index = 2 status = 3 return status,index,topic if filename.find("FIRST")&gt;-1: topic = "wa_dams_dnsfirst_dt" index = 3 status = 4 return status,index,topic if filename.find("HJK")&gt;-1: topic = "wa_dams_dns_hjk_dt" index = 4 status = 5 return status,index,topic if filename.find("TRAN")&gt;-1: topic = "wa_dams_dns_tran_dt" index = 5 status = 6 return status,index,topic if filename.find("SP")&gt;-1: topic = "wa_dams_dns_sp_dt" index = 6 status = 7 return status,index,topic if filename.find("DNAME")&gt;-1: topic = "wa_dams_ab_dname_dt" index = 7 status = 8 return status,index,topic if filename.find("PKT")&gt;-1: topic = "wa_dams_ab_pkt_error_dt" index = 8 status = 9 return status,index,topic if filename.find("IP")&gt;-1: topic = "wa_dams_ab_answer_value_dt" index = 9 status = 10 return status,index,topic if filename.find("V6")&gt;-1: print("IPV6") if filename.find("C2R")&gt;-1 or filename.find("R2C")&gt;-1: # WA_DAMS_DNSC2R_v6_DT topic = "wa_dams_dnsc2r_v6_dt" index = 10 status = 11 return status,index,topic if filename.find("R2A")&gt;-1 or filename.find("A2R")&gt;-1: # WA_DAMS_DNSR2A_V6_DT topic = "wa_dams_dnsr2a_v6_dt" index = 11 status = 12 return status,index,topic if filename.find("C2F")&gt;-1 or filename.find("F2C")&gt;-1: # WA_DAMS_DNSC2F_V6_DT topic = "wa_dams_dnsc2f_v6_dt" index = 12 status = 13 return status,index,topic if filename.find("HJK")&gt;-1: # WA_DAMS_DNS_HJK_v6_DT topic = "wa_dams_dns_hjk_v6_dt" index = 14 status = 15 return status,index,topic if filename.find("TRAN")&gt;-1: # WA_DAMS_DNS_TRAN_v6_DT topic = "wa_dams_dns_tran_v6_dt" index = 15 status = 16 return status, index, topic if filename.find("SP")&gt;-1: # WA_DAMS_DNS_SP_v6_DT topic = "wa_dams_dns_sp_v6_dt" index = 16 status = 17 return status,index,topic if filename.find("DNAME")&gt;-1: # WA_DAMS_AB_DNAME_v6_DT topic = "wa_dams_ab_dname_v6_dt" index = 17 status = 18 return status,index,topic if filename.find("PKT")&gt;-1: # WA_DAMS_AB_PKT_ERROR_v6_DT topic = "wa_dams_ab_pkt_error_v6_dt" index = 18 status = 19 return status,index,topic if filename.find("IP")&gt;-1: # WA_DAMS_AB_ANSWER_VALUE_v6_DT topic = "wa_dams_ab_answer_value_v6_dt" index = 19 status = 20 return status,index,topic return status, index, topic 1234567891011121314151617181920212223242526272829303132333435363738394041import requestsimport timedef request_init(topic, username, password, format_="csv", rsplit="$", fsplit=","): headers = ["Connection: Keep-Alive", "User: %s" % username, "Password: %s" % password, "Format: %s" % format_, "Topic: %s" % topic, "Row-Split: %s" % rsplit, "Field-Split: %s" % fsplit ] return headersdef request_post(url,headers,file,maxrows): count = 0 data = b'' with open(file, 'rb')as f: start=time.time() for line in f: data += line count += 1 if count &gt; maxrows: try: res = requests.post(url, data=data, headers=headers) except Exception: pass end=time.time() print("time:%0.2f"%(end-start)) #print(data.decode()) count = 0 data=b'' # sleep 1000 # records less than maxrows will be uploaded at last try: #print(data) res = requests.post(url, data=data, headers=headers) except Exception: print("request error 2") 抓取网页，发送邮件为了及时获得复试通知的时间,使用python脚本自动间隔访问主页,检索关键字”复试”,当找到关键字后立即邮件通知.这一小段代码包含两个方面的内容,一是网络请求,二是自动发送邮件(我使用的是yahoo的smtp服务器).全部代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/env python#encoding=utf-8"""@author: TianMao@contact: tianmao1994@yahoo.com@file: lingyun.py@time: 19-3-1 上午9:12@desc:"""import smtplibfrom email.mime.text import MIMETextimport requestsfrom bs4 import BeautifulSoupimport timeSMTP_SERVER = "smtp.mail.yahoo.com"SMTP_PORT = 587SMTP_USERNAME = "tianmao1994@yahoo.com"SMTP_PASSWORD = "雅虎邮箱密码"EMAIL_FROM = "tianmao1994@yahoo.com"EMAIL_TO = "tianmao818@qq.com"# EMAIL_TO = "1095474691@qq.com"EMAIL_SUBJECT = """Notification:"""co_msg = """Hello, 凌云!,华中科技大学\n"""def sendMail(topic,content): msg = MIMEText(co_msg+content) msg['Subject'] = EMAIL_SUBJECT+topic msg['From'] = EMAIL_FROM msg['To'] = EMAIL_TO debuglevel = True mail = smtplib.SMTP(SMTP_SERVER, SMTP_PORT) mail.set_debuglevel(debuglevel) mail.starttls() mail.login(SMTP_USERNAME, SMTP_PASSWORD) mail.sendmail(EMAIL_FROM, EMAIL_TO, msg.as_string()) mail.quit()while True: try: print("---start---") url = "http://gszs.hust.edu.cn/zsxx/ggtz.htm" headers = &#123;'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'&#125; response = requests.get(url, headers=headers) response.encoding = 'utf-8' soup = BeautifulSoup(response.text, "lxml") if (str(soup).find("复试")) == -1: print("---wait---") time.sleep(60) continue else: sendMail("""Huazhong University of Science and Technology""", "http://gszs.hust.edu.cn/zsxx/ggtz.htm") break except: continue 图像绘制绘制曲线图 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pdimport numpy as npimport mathimport matplotlib.pyplot as pltloss_cnn1d=pd.read_csv("../../data/loss_cnn_1D.csv")loss_cnn2d=pd.read_csv("../../data/loss_cnn_2d.csv")loss_cnn1d_rnn=pd.read_csv("../../data/loss_cnn1d_rnn.csv")loss_cnn1d_cnn1d_rnn=pd.read_csv("../../data/loss_cnn1d_cnn1d_rnn.csv")loss_cnn1d_cnn1d=pd.read_csv("../../data/loss_cnn1d_cnn1d.csv")x=loss_cnn1d["Step"][1:]y1=loss_cnn1d["Value"][1:]y2=loss_cnn2d["Value"][1:]y3=loss_cnn1d_rnn["Value"][1:]y4=loss_cnn1d_cnn1d_rnn["Value"][1:]y5=loss_cnn1d_cnn1d["Value"][1:]plt.rcParams['font.sans-serif']=['SimHei']plt.rcParams['axes.unicode_minus'] = Falseplt.figure(figsize=(8, 12))ax=plt.subplot(211)plt.xlabel("Steps")plt.ylabel("Loss")plt.plot(x, y1, 'r-', mec='k', label='Loss cnn_1d', lw=1)plt.plot(x, y2, 'g-', mec='k', label='Loss cnn_2d', lw=1)plt.plot(x, y3, 'b-',mec='k', label='Loss CRC', lw=1)# plt.plot(x, y4, color='olive' ,linestyle='-', mec='k', label='Loss cnn1d_cnn1d_rnn', lw=1)# plt.plot(x, y5, color='orange' ,linestyle='-', mec='k', label='Loss cnn1d_cnn1d', lw=1)# plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)plt.grid(True, ls='--')plt.legend(loc='upper right')plt.title('(1) Loss over steps',y=-0.18)# plt.savefig('../result/loss.png')# plt.show()acc_cnn_2d=pd.read_csv("../../data/acc_cnn_2d.csv")acc_cnn_1d=pd.read_csv("../../data/acc_cnn_1d.csv")acc_cnn1d_rnn=pd.read_csv("../../data/acc_cnn1d_rnn.csv")acc_cnn1d_cnn1d_rnn=pd.read_csv("../../data/acc_cnn1d_cnn1d_rnn.csv")acc_cnn1d_cnn1d_rnn=pd.read_csv("../../data/acc_cnn1d_cnn1d_rnn.csv")acc_cnn1d_cnn1d=pd.read_csv("../../data/acc_cnn1d_cnn1d.csv")x_acc_cnn_2d=acc_cnn_2d["Step"]y_acc_cnn_2d=acc_cnn_2d["Value"]y_acc_cnn_1d=acc_cnn_1d["Value"]y_acc_cnn1d_rnn=acc_cnn1d_rnn["Value"]y_acc_cnn1d_cnn1d_rnn=acc_cnn1d_cnn1d_rnn["Value"]y_acc_cnn1d_cnn1d=acc_cnn1d_cnn1d["Value"]plt.subplot(212)# plt.figure(figsize=(8, 5))plt.xlabel("Steps")plt.ylabel("Acc")plt.plot(x_acc_cnn_2d, y_acc_cnn_2d, 'r-', mec='k', label='Acc CNN_2D', lw=1)plt.plot(x_acc_cnn_2d,y_acc_cnn_1d, 'g-', mec='k', label='Acc CNN_1D', lw=1)plt.plot(x_acc_cnn_2d, y_acc_cnn1d_rnn, 'b-',mec='k', label='Acc CRC', lw=1)# plt.plot(x_acc_cnn_2d, y_acc_cnn1d_cnn1d_rnn,color='olive' ,linestyle='-',mec='k', label='Acc CNN1D_CNN1D_RNN', lw=1)# plt.plot(x_acc_cnn_2d, y_acc_cnn1d_cnn1d,color='orange' ,linestyle='-',mec='k', label='Acc CNN1D_CNN1D', lw=1)# plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)plt.grid(True, ls='--')plt.legend(loc='lower right')plt.title('(2) Accuracy over steps',y=-0.18)plt.subplots_adjust(hspace=0.25)plt.savefig('../result/acc_loss.png',bbox_inches='tight')plt.show() 效果 绘制柱装图 代码 123456789101112131415161718192021222324252627import numpy as npimport matplotlib.pyplot as pltsize = 3mymodel = np.asarray([0.9184840801316903,0.9090443033011008,0.912511745951522])baseline = np.asarray([0.88029597255012,0.8750779175862263,0.8768301416163474])x = np.asarray([0,1,2])total_width, n = 0.8, 2 # 有多少个类型，只需更改n即可width = total_width / nx = x - (total_width - width) / 2plt.figure(figsize=(8, 6.5))plt.grid(True, ls=&apos;--&apos;)plt.ylim(0.5,1)b1=plt.bar(x, mymodel, width=width, label=&apos;CRC&apos;)b2=plt.bar(x + width, baseline, width=width, label=&apos;784-1D-CNN&apos;)plt.xticks(x+width/2,[&quot;Precision&quot;,&quot;Recall&quot;,&quot;F1&quot;])for b in b1+b2: h=b.get_height() plt.text(b.get_x()+b.get_width()/2,h,&apos;%0.3f&apos;%float(h),ha=&apos;center&apos;,va=&apos;bottom&apos;)plt.legend()plt.savefig(&quot;./test.png&quot;, format=&apos;png&apos;,bbox_inches=&apos;tight&apos;)plt.show() 效果 绘制饼装图 代码 123456789101112131415161718import astimport numpy as npimport pandas as pdimport matplotlibimport matplotlib.pyplot as plt# 省略labels = [&apos;Packet Numbers:&gt;16&apos;, &apos;Packet Numbers:&lt;=16&apos;]sizes = [count_higher_16,count_lower_16]colors = [&apos;lightcoral&apos;, &apos;yellowgreen&apos;]explode = (0.05, 0) # explode 1st sliceplt.figure(figsize=(8, 6))# Plotpie=plt.pie(sizes, colors=colors,autopct=&apos;%1.1f%%&apos;,explode=explode,shadow=True, startangle=300)plt.legend(pie[0],labels, loc=&apos;lower right&apos;, fontsize=10)plt.axis(&apos;equal&apos;)# plt.tight_layout()plt.savefig(&quot;./pkts_count.png&quot;, format=&apos;png&apos;,bbox_inches=&apos;tight&apos;)plt.show() 效果 绘制混淆矩阵 代码:输入true标签和predict标签自动计算并绘图 123456789101112131415161718192021222324252627282930313233343536373839404142434445#/usr/bin/python env#coding=utf-8from sklearn.metrics import confusion_matriximport matplotlib.pyplot as pltimport numpy as np#绘制混淆矩阵def plot_confusion_matrix(y_true, y_pred, labels, save_path,font_size=8): tick_marks = np.array(range(len(labels))) + 0.5 # 配色:https://matplotlib.org/examples/color/colormaps_reference.html def plot(cm, title='Confusion Matrix', cmap=plt.cm.YlGn): plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() xlocations = np.array(range(len(labels))) plt.xticks(xlocations, labels, rotation=90) plt.yticks(xlocations, labels) plt.ylabel('True label') plt.xlabel('Predicted label') cm = confusion_matrix(y_true, y_pred) np.set_printoptions(precision=2) cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] plt.figure(figsize=(10, 8), dpi=120) ind_array = np.arange(len(labels)) x, y = np.meshgrid(ind_array, ind_array) for x_val, y_val in zip(x.flatten(), y.flatten()): c = cm_normalized[y_val][x_val] if c &gt; 0.01: plt.text(x_val, y_val, "%0.2f" % (c,), color='red', fontsize=font_size, va='center', ha='center') # offset the tick plt.gca().set_xticks(tick_marks, minor=True) plt.gca().set_yticks(tick_marks, minor=True) plt.gca().xaxis.set_ticks_position('none') plt.gca().yaxis.set_ticks_position('none') plt.grid(True, which='minor', linestyle='-') plt.gcf().subplots_adjust(bottom=0.15) plot(cm_normalized, title='Normalized confusion matrix') # show confusion matrix plt.savefig(save_path+"_confusematrix.png", format='png',bbox_inches='tight') plt.show() 效果 堆积柱状图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport matplotlib.pyplot as pltcategory_names = ['A', 'B','C', 'D', 'E','F']results = &#123; 'AA': [0.1,0.2,0.2,0.2,0.1,0.2], 'BB': [0.25,0.25,0.1,0.1,0.2,0.1], 'CC': [0.3,0.1,0.1,0.2,0.1,0.2], 'DD': [0.1,0.25,0.15,0.1,0.05,0.35], 'EE': [0.15,0.15,0.3,0.2,0.1,0.1], 'FF': [0.15,0.1,0.2,0.05,0.1,0.4],&#125;def survey(results, category_names): """ Parameters ---------- results : dict A mapping from question labels to a list of answers per category. It is assumed all lists contain the same number of entries and that it matches the length of *category_names*. category_names : list of str The category labels. """ labels = list(results.keys()) data = np.array(list(results.values())) data_cum = data.cumsum(axis=1)# 配色 https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html category_colors = plt.get_cmap('RdYlGn')( np.linspace(0.15, 0.85, data.shape[1])) fig, ax = plt.subplots(figsize=(9.2, 5)) ax.invert_yaxis() ax.xaxis.set_visible(False) ax.set_xlim(0, np.sum(data, axis=1).max()) for i, (colname, color) in enumerate(zip(category_names, category_colors)): widths = data[:, i] starts = data_cum[:, i] - widths ax.barh(labels, widths, left=starts, height=0.5, label=colname, color=color) xcenters = starts + widths / 2 r, g, b, _ = color text_color = 'white' if r * g * b &lt; 0.5 else 'darkgrey' for y, (x, c) in enumerate(zip(xcenters, widths)): ax.text(x, y, str(c*100)+"%", ha='center', va='center', color=text_color) ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1), loc='lower left', fontsize='small') return fig, axfig, ax=survey(results, category_names)fig.savefig("/home/mao/liuliyan.png") 机器学习深度学习相关技巧 TensorBoard的使用 远程连接tensorboard1234# 将服务器的端口6006端口重定向到自己的机器上ssh -L 16006:127.0.0.1:6006 tm@s24# 在服务器上使用6006端口启动tensorboardtensorboard --logdir=xxx --port=6006 限制GPU的使用比例12345678# 针对kerasimport tensorflow as tffrom keras.backend.tensorflow_backend import set_sessionconfig = tf.ConfigProto()config.gpu_options.allocator_type = &apos;BFC&apos; #A &quot;Best-fit with coalescing&quot; algorithm, simplified from a version of dlmalloc.config.gpu_options.per_process_gpu_memory_fraction = 0.3config.gpu_options.allow_growth = Trueset_session(tf.Session(config=config)) 在GPU环境下只加载CPU1234# 总有些傻逼的人喜欢占用所有的GPU资源,这时要启动程序得指定仅使用CPUimport osos.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;os.environ[&apos;CUDA_VISIBLE_DEVICES&apos;] = &apos;-1&apos; tensorflow log信息可见1234567891011tf.logging.set_verbosity(tf.logging.INFO)# ...logging_hook = tf.train.LoggingTensorHook(&#123;&quot;loss&quot;: loss,&quot;accuracy&quot;: accuracy[1]&#125;, every_n_iter=10)# Wrap all of this in an EstimatorSpec.spec = tf.estimator.EstimatorSpec( mode=mode, loss=loss, train_op=train_op, eval_metric_ops=metrics, training_hooks=[logging_hook]) keras自定义Layer这里以自定义的Attention Layer为例,这个类继承自Layer类,主要需要实现三个函数,一是build,二是call,三是compute_output_shape 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='&#123;&#125;_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='&#123;&#125;_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim TFRecord tutorial12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/usr/bin/env python # -*- coding: utf-8 -*- import tensorflow as tf import numpy writer = tf.python_io.TFRecordWriter('test.tfrecords')for i in range(5): a = 0.618 + i b = [2016 + i, 2017+i] c = numpy.array([[0, 1, 2],[3, 4, 5]]) + i c = c.astype(numpy.uint8) c_raw = c.tostring() print ('i:',i) print ('a:',a) print ('b:',b) print ('c:',c) example = tf.train.Example(features = tf.train.Features( feature = &#123;'a':tf.train.Feature(float_list = tf.train.FloatList(value=[a])), 'b':tf.train.Feature(int64_list = tf.train.Int64List(value = b)), 'c':tf.train.Feature(bytes_list = tf.train.BytesList(value = [c_raw]))&#125;)) serialized = example.SerializeToString() writer.write(serialized) print ('writer',i,'done') writer.close()# output file name string to a queue filename_queue = tf.train.string_input_producer(['test.tfrecords'], num_epochs=None) # create a reader from file queue reader = tf.TFRecordReader() _, serialized_example = reader.read(filename_queue) # get feature from serialized example features = tf.parse_single_example(serialized_example, features=&#123; 'a': tf.FixedLenFeature([], tf.float32), 'b': tf.FixedLenFeature([2], tf.int64), 'c': tf.FixedLenFeature([], tf.string) &#125; )a_out = features['a'] b_out = features['b'] c_raw_out = features['c'] c_out = tf.decode_raw(c_raw_out, tf.uint8) c_out = tf.reshape(c_out, [2, 3])print (a_out)print (b_out)print (c_out)a_batch, b_batch, c_batch = tf.train.shuffle_batch([a_out, b_out, c_out], batch_size=3, capacity=200, min_after_dequeue=100, num_threads=2)sess = tf.Session() init = tf.initialize_all_variables() sess.run(init) tf.train.start_queue_runners(sess=sess) a_val, b_val, c_val = sess.run([a_batch, b_batch, c_batch]) print("="*20)print ('first batch:')print ('a_val:',a_val) print ('b_val:',b_val) print ('c_val:',c_val)a_val, b_val, c_val = sess.run([a_batch, b_batch, c_batch]) print ('second batch:') print ('a_val:',a_val)print ('b_val:',b_val)print ('c_val:',c_val) 执行结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960i: 0a: 0.618b: [2016, 2017]c: [[0 1 2] [3 4 5]]writer 0 donei: 1a: 1.6179999999999999b: [2017, 2018]c: [[1 2 3] [4 5 6]]writer 1 donei: 2a: 2.618b: [2018, 2019]c: [[2 3 4] [5 6 7]]writer 2 donei: 3a: 3.618b: [2019, 2020]c: [[3 4 5] [6 7 8]]writer 3 donei: 4a: 4.618b: [2020, 2021]c: [[4 5 6] [7 8 9]]writer 4 doneTensor(&quot;ParseSingleExample_13/ParseSingleExample:0&quot;, shape=(), dtype=float32)Tensor(&quot;ParseSingleExample_13/ParseSingleExample:1&quot;, shape=(2,), dtype=int64)Tensor(&quot;Reshape_8:0&quot;, shape=(2, 3), dtype=uint8)====================first batch:a_val: [2.618 3.618 3.618]b_val: [[2018 2019] [2019 2020] [2019 2020]]c_val: [[[2 3 4] [5 6 7]] [[3 4 5] [6 7 8]] [[3 4 5] [6 7 8]]]second batch:a_val: [3.618 4.618 1.618]b_val: [[2019 2020] [2020 2021] [2017 2018]]c_val: [[[3 4 5] [6 7 8]] [[4 5 6] [7 8 9]] [[1 2 3] [4 5 6]]]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法训练场]]></title>
    <url>%2F2017%2F12%2F08%2F%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[算法基础训练。 实现BitMap12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package JavaBasic;/** * @Classname BitMap * @Description 实现BItMap * @Date 19-6-19 下午7:49 * @Created by mao&lt;tianmao818@qq.com&gt; */public class BitMap &#123; /** 插入数的最大长度，比如100，那么允许插入bitsMap中的最大数为99 */ private long length; //每一个int表示32位 private static int[] bitsMap; //长度为32,0-31每一位单独为1的时候的数值 private static final int[] BIT_VALUE = &#123; 0x00000001, 0x00000002, 0x00000004, 0x00000008, 0x00000010, 0x00000020, 0x00000040, 0x00000080, 0x00000100, 0x00000200, 0x00000400, 0x00000800, 0x00001000, 0x00002000, 0x00004000, 0x00008000, 0x00010000, 0x00020000, 0x00040000, 0x00080000, 0x00100000, 0x00200000, 0x00400000, 0x00800000, 0x01000000, 0x02000000, 0x04000000, 0x08000000, 0x10000000, 0x20000000, 0x40000000, 0x80000000 &#125;; //长度等于最大数除以32 public BitMap(long length) &#123; this.length = length; // 根据长度算出，所需数组大小 bitsMap = new int[(int) (length &gt;&gt; 5) + ((length &amp; 31) &gt; 0 ? 1 : 0)]; &#125; /** * 根据长度获取数据 比如输入63，那么实际上是确定数62是否在bitsMap中 * * @return index 数的长度 * @return 1:代表数在其中 0:代表 */ public int getBit(long index) &#123; if (index &lt; 0 || index &gt; length) &#123; throw new IllegalArgumentException("length value illegal!"); &#125; int intData = (int) bitsMap[(int) ((index - 1) &gt;&gt; 5)]; //(index - 1) &amp; 31表示的是偏移 //((intData &amp; BIT_VALUE[(int) ((index - 1) &amp; 31)]))表示这个值存在不存在 //右移是为了将数值转换到0和1之间 return ((intData &amp; BIT_VALUE[(int) ((index - 1) &amp; 31)])) &gt;&gt;&gt; ((index - 1) &amp; 31); &#125; /** * @param index * 要被设置的值为index - 1 */ public void setBit(long index) &#123; //防止越界 if (index &lt; 0 || index &gt; length) &#123; throw new IllegalArgumentException("length value illegal!"); &#125; // 求出该index - 1所在bitMap的下标 int belowIndex = (int) ((index - 1) &gt;&gt; 5); // 求出该值的偏移量(求余) int offset = (int) ((index - 1) &amp; 31); //inData是一个大的整数 int inData = bitsMap[belowIndex]; //将特定位置为1 bitsMap[belowIndex] = inData | BIT_VALUE[offset]; &#125; public static void main(String[] args) &#123; //设置最大的表示范围 BitMap bitMap = new BitMap(129); bitMap.setBit(63); System.out.println(bitMap.getBit(63)); System.out.println(bitMap.getBit(62)); &#125;&#125; 使用中缀表达式实现计算器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231￼package CommonProblems.StackProblems;import java.io.File;import java.io.FileNotFoundException;import java.util.ArrayList;import java.util.Scanner;import java.util.Stack;/** * @Author MaoTian * @Classname CalculatorDemo * @Description 表达式的计算，使用两个栈建立后缀表达式，通过后缀表达式进行求值0 * 操作符是以中缀形式处于操作数的中间（例：3 + 4）。 * 前缀表达式（例：+ 3 4） * 后缀表达式（例：3 4 +） * @Date 下午3:25 2019/9/12 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */public class CalculatorDemo &#123; public static void main(String[] args) throws FileNotFoundException &#123; //Scanner sc=new Scanner(System.in); Scanner sc=new Scanner(new File("/home/mao/workspace/java/src/CommonProblems/StackProblems/CalculatorDemo")); while(sc.hasNext())&#123; String s=sc.nextLine(); s=s.replaceAll("\\&#123;", "("); s=s.replaceAll("\\[", "("); s=s.replaceAll("\\&#125;", ")"); s=s.replaceAll("\\]", ")"); //保存操作数和符号 ArrayList&lt;String&gt; list=new ArrayList&lt;&gt;(); //预处理 char[] c=s.toCharArray(); int i=0; while(i&lt;c.length)&#123; int k=0; while(k+i&lt;c.length &amp;&amp; c[i+k]&gt;='0' &amp;&amp; c[i+k]&lt;='9')&#123; k++; &#125; if(k!=0)&#123; //添加数字 list.add(String.copyValueOf(c, i, k)); i=i+k; &#125; else&#123; //添加符号：括号和操作符号 list.add(String.copyValueOf(c, i, 1)); i++; &#125; &#125; //一个+-*/对应着两个数字 ArrayList&lt;String&gt; list1=new ArrayList&lt;&gt;(); for(int j=0; j&lt;list.size(); j++)&#123; //对负数-N进行特殊的处理，改为0-N if(list.get(j).equals("-"))&#123; if(j==0)&#123; list1.add("0"); &#125; else&#123; if(list.get(j-1).equals("("))&#123; list1.add("0"); &#125; &#125; &#125; list1.add(list.get(j)); &#125; System.out.println("-----------------------------------------------"); for (String cc:list1)&#123; System.out.print(cc+" "); &#125; System.out.println(); System.out.println("-----------------------------------------------"); System.out.println(computeNum(list1)); &#125; &#125; //获取后缀表达式 //将中缀表达式（即标准的表达式）转换为后缀表达式: // 1 + 2 * 3 + ( 4 * 5 + 6 ) * 7 转换成 1 2 3 * + 4 5 * 6 + 7 * + /* 首先，读入‘1’，并送到输出，然后‘+’被读入并压入栈中。接下来‘2’读入并送到输出，此时状态如下： 栈：+ 输出：1 2 接下来读入‘*’，由于优先级比栈顶元素‘+’大（原则3），因此被压入栈中，接着读入‘3’，并送到输出： 栈：+ * 输出：1 2 3 然后读入‘+’，由于此时栈顶元素为‘*’，优先级比‘+’大，因此将‘*’弹出，弹出后原来的‘+’变为栈顶元素，由于‘+’的优先级和当前读入的‘+’优先级相等，因此也被弹出（原则3），最后将读入的‘+’压入栈中，因此状态如下： 栈：+ 输出：1 2 3 * + 下一个读入的符号‘（’，由于具有最高优先级，因此将其放入栈中，然后读入‘4’： 栈：+ （ 输出： 1 2 3 * + 4 继续读入，此时读入‘*’，除非处理‘）’，否则‘（’绝不会弹出，因此‘*’被压入栈中，接下来读入‘5’： 栈：+ （* 输出：1 2 3 * + 4 5 往后读入的符号是‘+’，将‘*’弹出并输出。然后将‘+’压入栈中，接着读入‘6’： 栈：+ （ + 输出：1 2 3 * + 4 5 * 6 现在读入‘）’，因此弹出栈中元素直到遇到‘（’： 栈：+ 输出：1 2 3 * + 4 5 * 6 + 下一个有读入‘*’，被压入栈中，然后读入‘7’： 栈：+ * 输出：1 2 3 * + 4 5 * 6 + 7 现在输入为空，弹出所有栈中元素 栈：空 输出：1 2 3 * + 4 5 * 6 + 7 * + * */ public static String[] getPostfixExpression(ArrayList&lt;String&gt; list)&#123; //使用两个栈 Stack&lt;String&gt; s1=new Stack&lt;&gt;(); Stack&lt;String&gt; s2=new Stack&lt;&gt;(); int i=0; while(i&lt;list.size())&#123; //括号的优先级最高 if(list.get(i).equals("("))&#123; s2.add(list.get(i)); i++; continue; &#125; //加减 if(list.get(i).equals("+") || list.get(i).equals("-"))&#123; while(!s2.isEmpty() &amp;&amp; !s2.peek().equals("("))&#123; s1.add(s2.pop()); &#125; s2.add(list.get(i)); i++; continue; &#125; //乘除 if(list.get(i).equals("*") || list.get(i).equals("/"))&#123; while(!s2.isEmpty() &amp;&amp; (s2.peek().equals("*") || s2.peek().equals("/")))&#123; s1.add(s2.pop()); &#125; s2.add(list.get(i)); i++; continue; &#125; if(list.get(i).equals(")"))&#123; while(!s2.isEmpty() &amp;&amp; !s2.peek().equals("("))&#123; s1.add(s2.pop()); &#125; s2.pop(); i++; continue; &#125; s1.add(list.get(i)); i++; &#125; while(!s2.isEmpty())&#123; s1.add(s2.pop()); &#125; String[] c=new String[s1.size()]; for(int j=c.length-1; j&gt;=0; j--)&#123; c[j]=s1.pop(); &#125; System.out.println("-----------------------------------------------"); for (String cc:c)&#123; System.out.print(cc+" "); &#125; System.out.println(); System.out.println("-----------------------------------------------"); return c; &#125; //通过后缀表达式求值,从栈中取元素，进行计算，计算完后入栈 public static int computeNum(ArrayList&lt;String&gt; list)&#123; //获得后缀表达式 String[] c=getPostfixExpression(list); //保存计算的结果 Stack&lt;Integer&gt; s=new Stack&lt;&gt;(); for(int i=0; i&lt;c.length; i++)&#123; int t1, t2; if(c[i].equals("+"))&#123; t1=s.pop(); t2=s.pop(); s.add(t2+t1); continue; &#125; if(c[i].equals("-"))&#123; t1=s.pop(); t2=s.pop(); s.add(t2-t1); continue; &#125; if(c[i].equals("*"))&#123; t1=s.pop(); t2=s.pop(); s.add(t2*t1); continue; &#125; if(c[i].equals("/"))&#123; t1=s.pop(); t2=s.pop(); s.add(t2/t1); continue; &#125; //数字入栈 s.add(Integer.parseInt(c[i])); &#125; return s.pop(); &#125;&#125; 二叉树的遍历前序遍历递归 java实现 1234567public void preorder1(BinaryTreeNode root)&#123; if (root==null) return; System.out.print(root.getData()+&quot;\t&quot;); preorder1(root.getLeft()); preorder1(root.getRight());&#125; 循环(使用栈) java实现 123456789101112131415161718public void preorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if (root==null) return; BinaryTreeNode cur; cur=root; while(cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; System.out.print(cur.getData()+&quot;\t&quot;); stack.push(cur); cur=cur.getLeft(); &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getRight(); &#125; &#125;&#125; 中序遍历递归 java实现 1234567public void inorder1(BinaryTreeNode root)&#123; if (root==null) return; inorder1(root.getLeft()); System.out.print(root.getData()+&quot;\t&quot;); inorder1(root.getRight());&#125; 循环(使用栈) java实现 1234567891011121314151617public void inorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if(root==null) return; BinaryTreeNode cur=root; while(cur!=null||!stack.isEmpty())&#123; if(cur!=null)&#123; stack.push(cur); cur=cur.getLeft(); &#125;else&#123; cur=stack.peek(); stack.pop(); System.out.print(cur.getData()+&quot;\t&quot;); cur=cur.getRight(); &#125; &#125;&#125; 后序遍历递归 java实现 1234567public void postorder1(BinaryTreeNode root)&#123; if (root==null) return; postorder1(root.getLeft()); postorder1(root.getRight()); System.out.print(root.getData()+&quot;\t&quot;);&#125; 循环(使用栈) java实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public void postorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); while (true)&#123; if(root!=null)&#123; stack.push(root); root=root.getLeft(); &#125; else &#123; if(stack.isEmpty()) return; if(stack.lastElement().getRight()==null)&#123; root=stack.pop(); System.out.print(root.getData()+&quot;\t&quot;); while (stack.lastElement().getRight()==root)&#123; System.out.print(stack.lastElement().getData()+&quot;\t&quot;); root=stack.pop(); if (stack.isEmpty())&#123; break; &#125; &#125; &#125; if(!stack.isEmpty()) root=stack.lastElement().getRight(); else root=null; &#125; &#125; &#125; public void postorder3(BinaryTreeNode root)&#123; if(root==null) return; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); BinaryTreeNode cur; cur=root; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); while (cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; res.add(cur.getData()); stack.push(cur); cur=cur.getRight(); &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getLeft(); &#125; &#125; Collections.reverse(res); for (Integer i:res)&#123; System.out.print(i+&quot;\t&quot;); &#125; &#125; 测试建立二叉树 java实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package BinaryTree;import BinaryTree.BinaryTreeNode;import java.net.PortUnreachableException;public class ConstructBinaryTree &#123; public static BinaryTreeNode construct1(int[] preOrder,int[] inOrder, int length)&#123; if(preOrder==null||inOrder==null||length&lt;0)&#123; return null; &#125; try &#123; return ConstructCore1(preOrder, 0, preOrder.length - 1, inOrder, 0,inOrder.length - 1); &#125;catch (Exception e)&#123; return null; &#125; &#125; public static BinaryTreeNode ConstructCore1(int[] preOrder,int startPreIndex, int endPreIndex, int[] inOrder,int startInIndex, int endInIndex) throws InvalidPutException &#123; int rootValue = preOrder[startPreIndex]; BinaryTreeNode root = new BinaryTreeNode(rootValue,null,null); // 在中序遍历中找到根结点的索引 int rootInIndex = startInIndex; while (rootInIndex &lt;= endInIndex &amp;&amp; inOrder[rootInIndex] != rootValue) &#123; ++rootInIndex; &#125; if (rootInIndex == endInIndex &amp;&amp; inOrder[rootInIndex] != rootValue) &#123; throw new InvalidPutException(); &#125; int leftLength = rootInIndex - startInIndex; int leftPreOrderEndIndex = startPreIndex + leftLength; if (leftLength &gt; 0) &#123; // 构建左子树 root.setLeft( ConstructCore1(preOrder, startPreIndex + 1, leftPreOrderEndIndex, inOrder, startInIndex, rootInIndex - 1)); &#125; if (leftLength &lt; endPreIndex - startPreIndex) &#123; // 右子树有元素,构建右子树 root.setRight(ConstructCore1(preOrder, leftPreOrderEndIndex + 1, endPreIndex, inOrder, rootInIndex + 1, endInIndex)); &#125; return root; &#125; public static BinaryTreeNode construct2(int[] postOrder,int[] inOrder,int length)&#123; return ConstructCore2(postOrder,0,postOrder.length-1,inOrder,0,inOrder.length-1); &#125; public static BinaryTreeNode ConstructCore2(int[] postOrder,int startPostIndex,int endPostIndex,int[] inOrder,int startInIndex,int endInIndex)&#123; int rootValue=postOrder[endPostIndex]; BinaryTreeNode root=new BinaryTreeNode(rootValue,null,null); int rootInIndex=startInIndex; while(rootInIndex&lt;=endInIndex&amp;&amp;inOrder[rootInIndex]!=rootValue)&#123; ++rootInIndex; &#125; int leftLength=rootInIndex-startInIndex; int rightLength=endInIndex-rootInIndex; int leftPostOrderEndIndex=startPostIndex+leftLength-1; if(leftLength&gt;0)&#123; root.setLeft(ConstructCore2(postOrder,startPostIndex,leftPostOrderEndIndex,inOrder,startInIndex,rootInIndex-1)); &#125; if(rightLength&gt;0)&#123; root.setRight(ConstructCore2(postOrder,rootInIndex,endPostIndex-1,inOrder,rootInIndex+1,endInIndex)); &#125; return root; &#125; static class InvalidPutException extends Exception &#123; private static final long serialVersionUID = 1L; &#125; public static void main(String[] argv)&#123; int[] preOrder=&#123;1,2,4,5,8,9,10,3,6,7&#125;; int[] inOrder=&#123;4,2,8,5,9,10,1,6,3,7&#125;; int[] postOrder=&#123;4,8,10,9,5,2,6,7,3,1&#125;; BinaryTreeNode root=ConstructBinaryTree.construct1(preOrder,inOrder,10); BinaryTreeNode root2=ConstructBinaryTree.construct2(postOrder,inOrder,10); Traversal traversal=new Traversal(); System.out.println(&quot;preOrder+inOrder&quot;); traversal.postorder3(root); System.out.println(); System.out.println(&quot;postOrder+inOrder&quot;); traversal.preorder1(root2); &#125; &#125; 完整java代码 完整 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214package BinaryTree;public class BinaryTreeNode &#123; private int data; private BinaryTreeNode left; private BinaryTreeNode right; public BinaryTreeNode(int data, BinaryTreeNode left, BinaryTreeNode right) &#123; super(); this.data = data; this.left = left; this.right = right; &#125; public int getData() &#123; return data; &#125; public void setData(int data) &#123; this.data = data; &#125; public BinaryTreeNode getLeft() &#123; return left; &#125; public void setLeft(BinaryTreeNode left) &#123; this.left = left; &#125; public BinaryTreeNode getRight() &#123; return right; &#125; public void setRight(BinaryTreeNode right) &#123; this.right = right; &#125;&#125;package BinaryTree;import BinaryTree.BinaryTreeNode;import java.util.*;public class Traversal &#123; public void preorder1(BinaryTreeNode root)&#123; if (root==null) return; System.out.print(root.getData()+&quot;\t&quot;); preorder1(root.getLeft()); preorder1(root.getRight()); &#125; public void preorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if (root==null) return; BinaryTreeNode cur; cur=root; while(cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; System.out.print(cur.getData()+&quot;\t&quot;); stack.push(cur); cur=cur.getLeft(); &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getRight(); &#125; &#125; &#125; public void inorder1(BinaryTreeNode root)&#123; if (root==null) return; inorder1(root.getLeft()); System.out.print(root.getData()+&quot;\t&quot;); inorder1(root.getRight()); &#125; public void inorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt;stack =new Stack&lt;BinaryTreeNode&gt;(); if(root==null) return; BinaryTreeNode cur=root; while(cur!=null||!stack.isEmpty())&#123; if(cur!=null)&#123; stack.push(cur); cur=cur.getLeft(); &#125;else&#123; cur=stack.peek(); stack.pop(); System.out.print(cur.getData()+&quot;\t&quot;); cur=cur.getRight(); &#125; &#125; &#125; public void postorder1(BinaryTreeNode root)&#123; if (root==null) return; postorder1(root.getLeft()); postorder1(root.getRight()); System.out.print(root.getData()+&quot;\t&quot;); &#125; public void postorder2(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); while (true)&#123; if(root!=null)&#123; stack.push(root); root=root.getLeft(); &#125; else &#123; if(stack.isEmpty()) return; if(stack.lastElement().getRight()==null)&#123; root=stack.pop(); System.out.print(root.getData()+&quot;\t&quot;); while (stack.lastElement().getRight()==root)&#123; System.out.print(stack.lastElement().getData()+&quot;\t&quot;); root=stack.pop(); if (stack.isEmpty())&#123; break; &#125; &#125; &#125; if(!stack.isEmpty()) root=stack.lastElement().getRight(); else root=null; &#125; &#125; &#125; public void postorder3(BinaryTreeNode root)&#123; if(root==null) return; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); BinaryTreeNode cur; cur=root; List&lt;Integer&gt; res=new ArrayList&lt;&gt;(); while (cur!=null||!stack.isEmpty())&#123; if (cur!=null)&#123; res.add(cur.getData()); stack.push(cur); cur=cur.getRight(); &#125;else&#123; cur=stack.peek(); stack.pop(); cur=cur.getLeft(); &#125; &#125; Collections.reverse(res); for (Integer i:res)&#123; System.out.print(i+&quot;\t&quot;); &#125; &#125; public void levelorder(BinaryTreeNode root)&#123; BinaryTreeNode temp; Queue&lt;BinaryTreeNode&gt; queue=new LinkedList&lt;BinaryTreeNode&gt;(); queue.offer(root); while (!queue.isEmpty())&#123; temp=queue.poll(); System.out.print(temp.getData()+&quot;\t&quot;); if(temp.getLeft()!=null)&#123; queue.offer(temp.getLeft()); &#125; if(temp.getRight()!=null)&#123; queue.offer(temp.getRight()); &#125; &#125; &#125; public static void main(String[] args)&#123; BinaryTreeNode node10=new BinaryTreeNode(10,null,null); BinaryTreeNode node8=new BinaryTreeNode(8,null,null); BinaryTreeNode node9=new BinaryTreeNode(9,null,node10); BinaryTreeNode node4=new BinaryTreeNode(4,null,null); BinaryTreeNode node5=new BinaryTreeNode(5,node8,node9); BinaryTreeNode node6=new BinaryTreeNode(6,null,null); BinaryTreeNode node7=new BinaryTreeNode(7,null,null); BinaryTreeNode node2=new BinaryTreeNode(2,node4,node5); BinaryTreeNode node3=new BinaryTreeNode(3,node6,node7); BinaryTreeNode node1=new BinaryTreeNode(1,node2,node3); Traversal traversal=new Traversal(); System.out.println(&quot;---pre order---&quot;); traversal.preorder1(node1); System.out.println(); traversal.preorder2(node1); System.out.println(); System.out.println(&quot;---in order---&quot;); traversal.inorder1(node1); System.out.println(); traversal.inorder2(node1); System.out.println(); System.out.println(&quot;---post order---&quot;); traversal.postorder1(node1); System.out.println(); traversal.postorder2(node1); System.out.println(); traversal.postorder3(node1); System.out.println(); System.out.println(&quot;---level order---&quot;); traversal.levelorder(node1); System.out.println(); &#125;&#125; 生产者消费者要点 1 线程 操作 资源类 2 判断 干活 通知 3 虚假唤醒（必须使用while进行循环） 传统方法：使用synchronized12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package JavaDemo.MultiThreadTest;/** * @Author MaoTian * @Classname ProducerConsumerSync * @Description TODO * @Date 上午8:48 2019/8/9 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class ShareSource&#123; private int number=0; public synchronized void increment()throws InterruptedException&#123; while (number!=0)&#123; this.wait(); &#125; ++number; System.out.println(Thread.currentThread().getName()+"\t"+number); this.notifyAll(); &#125; public synchronized void decrement()throws InterruptedException&#123; while (number==0)&#123; this.wait(); &#125; --number; System.out.println(Thread.currentThread().getName()+"\t"+number); this.notifyAll(); &#125;&#125;public class ProducerConsumerSync &#123; public static void main(String[] args) &#123; ShareSource shareSource=new ShareSource(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; shareSource.increment(); &#125;catch (Exception e)&#123; &#125; &#125; &#125;,"producer3").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt;10 ; i++) &#123; try &#123; shareSource.decrement(); &#125;catch (Exception e)&#123; &#125; &#125; &#125;,"consumer").start(); &#125;&#125; 使用Lock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105package JavaDemo.MultiThreadTest;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @Author MaoTian * @Classname ProducerConsumerTraditional * @Description 交替操作，一个加，一个减 * 1 线程 操作 资源类 * 2 判断 干活 通知 * 3 虚假唤醒（必须使用while进行循环） * @Date 下午7:40 2019/8/8 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class ShareData&#123; private int number=0; private Lock lock=new ReentrantLock(); private Condition condition=lock.newCondition(); public void increment()throws Exception&#123; lock.lock(); try&#123; //判断 while (number!=0)&#123; //等待 condition.await(); &#125; //干活 number++; System.out.println(Thread.currentThread().getName()+":"+number); //通知 condition.signalAll(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125; public void decrement()throws Exception&#123; lock.lock(); try&#123; //判断 while (number==0)&#123; //等待 condition.await(); &#125; //干活 number--; System.out.println(Thread.currentThread().getName()+":"+number); //通知 condition.signalAll(); &#125;catch (Exception e)&#123; &#125;finally &#123; lock.unlock(); &#125; &#125;&#125;public class ProducerConsumerTraditional &#123; public static void main(String[] args) &#123; ShareData shareData=new ShareData(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.increment(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"producer_1").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.decrement(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"consumer_1").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.increment(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"producer_2").start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; shareData.decrement(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"consumer_2").start(); &#125;&#125; 使用阻塞队列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package JavaDemo.MultiThreadTest;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;/** * @Author MaoTian * @Classname ProducerConsumerBlockingQueue * @Description 使用阻塞队列，生产一个消费一个 * @Date 下午8:51 2019/8/8 * @Version 1.0 * @Created by mao&lt;tianmao818@qq.com&gt; */class Resource&#123; private volatile boolean FLAG=true; //可见性 private AtomicInteger atomicInteger=new AtomicInteger();//原子类 BlockingQueue&lt;String&gt; blockingQueue=null;//阻塞队列 public Resource(BlockingQueue&lt;String&gt; blockingQueue)&#123; this.blockingQueue=blockingQueue; System.out.println(blockingQueue.getClass().getName()); &#125; public void myProd()throws Exception&#123; String data=null; boolean retvalue; while (FLAG)&#123; data=atomicInteger.incrementAndGet()+""; retvalue=blockingQueue.offer(data,2L, TimeUnit.SECONDS); if(retvalue)&#123; System.out.println(Thread.currentThread()+":insert ok "+data); &#125;else&#123; System.out.println(Thread.currentThread()+":insert fail"); &#125;// TimeUnit.SECONDS.sleep(1); &#125; System.out.println(Thread.currentThread()+":producer stop"); &#125; public void myCons()throws Exception&#123; String result; while (FLAG)&#123; result=blockingQueue.poll(2L, TimeUnit.SECONDS); if(null==result||result.equalsIgnoreCase(""))&#123; FLAG=false; System.out.println(Thread.currentThread()+":consumer stop"); return; &#125; System.out.println(Thread.currentThread()+":consume ok "+result); &#125; &#125; public void stop()&#123; this.FLAG=false; &#125;&#125;public class ProducerConsumerBlockingQueue &#123; public static void main(String[] args) throws InterruptedException &#123; Resource resource=new Resource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+" producer start"); try &#123; resource.myProd(); &#125;catch (Exception e)&#123; &#125; &#125;,"producer").start(); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+" consumer start"); try &#123; resource.myCons(); &#125;catch (Exception e)&#123; &#125; &#125;,"consumer").start(); TimeUnit.SECONDS.sleep(5); resource.stop(); &#125;&#125; 重写hashCode和equalsWhat? 如果两个对象相等，则hashcode一定也是相同的 两个对象相等，对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） Why?举两个场景就很明确知道为何要重写了? Set中存自定义的对象 Map中使用自定义对象作为key 以上,如果不重写,即使我们对象的属性值完全相等(就是从意义上完全相等),但是我们new出来的对象是两个不同的对象,那么在加入Set或者作为Map的键时候是会作为unique的,那么添加后,我们会发现意义上一样的值同时存在于Set中,更加不幸的是,我们新建一个对象作为Key去Map中取值会永远也取不到!!! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package JavaBasic;import java.util.HashMap;/** * @Classname OverideEqualsHashcode * @Description 重写equals和hashCode方法 * @Date 19-7-5 下午4:04 * @Created by mao&lt;tianmao818@qq.com&gt; */class Person&#123; private String id; public Person(String id)&#123; this.id=id; &#125; @Override public boolean equals(Object o)&#123; if(this==o)&#123; return true; &#125; if(o==null||getClass()!=o.getClass())&#123; return false; &#125; Person person=(Person)o; if(id!=null?(!id.equals(person.id)):(person.id!=null))&#123; return false; &#125; return true; &#125; @Override public int hashCode()&#123; return id!=null?id.hashCode():0; &#125;&#125;public class OverideEqualsHashcode &#123; public static void main(String[] args)&#123; HashMap&lt;Person,String&gt; map=new HashMap&lt;&gt;(); // 如果不重写,将会有四个对象加入,即使对象的信息相同,但是euqals的是对象的地址,新建一个对象地址肯定不一样 map.put(new Person("001"),"test1"); map.put(new Person("001"),"test2"); map.put(new Person("003"),"test3"); map.put(new Person("004"),"test4"); System.out.println(map.toString()); // 如果只重写hashCode,hashCode即使一样,对象还是不一样 System.out.println((new Person("code1")).hashCode()); System.out.println((new Person("code1")).hashCode()); System.out.println((new Person("code2")).hashCode()); &#125;&#125; 重写equals函数和hashCode函数,执行的结果和预期相同 只重写了hashCode,发现hashCode虽然可能一样,但是并不能够正确判断对象就是相等的,这也是为什么必须重写equals 均不重写,hashCode不同(当然不能保证完全不同,毕竟有碰撞的存在),对象完全不相等! How?重写hashCode:重新计算hash值;重写equals:重写方法:保证对象的每一个属性都有覆盖到,做到完全相等! 经典方法 借助Objects类 借助Apache.commons.lang3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import org.apache.commons.lang3.builder.EqualsBuilder;import org.apache.commons.lang3.builder.HashCodeBuilder;import java.util.Objects;class User &#123; private String name; private int age; private String passport; //getters and setters, constructor @Override public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof User)) &#123; return false; &#125; User user = (User) o; return user.name.equals(name) &amp;&amp; user.age == age &amp;&amp; user.passport.equals(passport); &#125; //Idea from effective Java : Item 9 @Override public int hashCode() &#123; int result = 17; result = 31 * result + name.hashCode(); result = 31 * result + age; result = 31 * result + passport.hashCode(); return result; &#125;&#125;class User_1 &#123; private String name; private int age; private String passport; //getters and setters, constructor @Override public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof User)) &#123; return false; &#125; User_1 user = (User_1) o; return age == user.age &amp;&amp; Objects.equals(name, user.name) &amp;&amp; Objects.equals(passport, user.passport); &#125; @Override public int hashCode() &#123; return Objects.hash(name, age, passport); &#125;&#125;class User_2 &#123; private String name; private int age; private String passport; //getters and setters, constructor @Override public boolean equals(Object o) &#123; if (o == this) return true; if (!(o instanceof User_2)) &#123; return false; &#125; User_2 user = (User_2) o; return new EqualsBuilder() .append(age, user.age) .append(name, user.name) .append(passport, user.passport) .isEquals(); &#125; @Override public int hashCode() &#123; return new HashCodeBuilder(17, 37) .append(name) .append(age) .append(passport) .toHashCode(); &#125;&#125;]]></content>
      <categories>
        <category>基础算法</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java总结]]></title>
    <url>%2F2016%2F11%2F20%2FJava%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[保存java总结图片]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2010%2F08%2F18%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start安装和使用123456789$ npm install -g hexo-cli$ hexo init &lt;folder&gt; #我的是blog#$ cd &lt;folder&gt;$ npm install# 安装 hexo-deployer-git$ npm install hexo-deployer-git --save$ hexo clean$ hexo g$ hexo d Create a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy 上传图片1231 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true2 在你的hexo目录下执行这样一句话npm install hexo-asset-image --save 示例12345678title: Linux权能与PAM机制date: 2018-05-01 08:09:12updated: 2018-05-01 08:09:12tags: Linux capabilitycategories: Linuxcopyright: true More info: Deployment 模板读书笔记模板 书名 作者 出版社 出版时间 XXXXX XX XXX XXXX-XX-XX 阅读心得 LEARNINGS 重点摘录 NOTES PPT思路模板PPT龙骨注意！注意！注意！这部分内容摘抄自阿里大佬的公众号[1]，仅作学习、工作指导使用，侵删（email: tianmao818@qq.com,会及时处理）。 序号 步骤 细节 备注 1 提出问题的背景 1. 业务背景2. 背景有多大 提出问题的背景：在引出具体问题之前，首先介绍问题提出的背景，包括业务背景和技术背景，比如业务发展和技术发展上的一些数字、形势介绍。不同的层级对背景的覆盖面情况也不一样，一般越高层级背景越大。 2 定义问题 1. 解决什么问题2. 问题有多大，有多痛 定义问题：这是PPT的关键点之一，PPT讲解完要能清楚的让评委知道你解决的是什么问题，定义了什么问题域。同时要阐述出问题有多大、多痛、多严峻。如果是简单的、一般的问题没必要上升到这个台面去讲。 3 解决问题的挑战 1. 难点、挑战在哪里？2. 数字、鲜活的体感 解决问题的挑战：在提出和定义了问题后，接下来应该是给出解决问题的方案和策略，不过在介绍方案之前可以先说明下解决问题的挑战和困难，以此表现解决这些问题是不容易的，有难点，从而体现自己的优秀和厉害。这里不止是干巴巴的介绍，最好能通过一些数字或鲜活的案例来表现解决问题的难度。 4 解决问题的方案or策略 1. 怎么解决问题的？2. 哪些维度、方面？ 解决问题的方案/策略：这是PPT的关键点之一，用1到2页ppt描述针对前面定义的那些问题的解决方案和整体策略是什么，向听众阐述自己是怎么解决问题的。方案/策略最好是体系化的，分为多个维度，不要是散的、偏点状的方案。 5 方案or策略详解 1. 技术选型与对比2. 方案的优越性、厉害在哪里？3. 怎么破除挑战的？ 方案/策略详解：这一部分是围绕着第4点进行方案的详细阐述，介绍方案的实现细节。这里要核心注意三个点， 一是必须有技术方案的选型与横向对比，包括集团或业界的，既体现自己的技术视野和理解深度，也表明自己的方案提出是经过深思熟虑，是一套合适的、有效的、适用的方案。 二是体现出方案的优越性在哪里，厉害的地方在哪里。很多同学在方案放一些架构大图，但是很空洞，没有体感，不知道这个架构方案优秀在哪里，为什么这样设计，跟前面问题的结合点在哪里。 三是方案介绍要能和前面的问题挑战对应起来，说明我的方案解决和回避了那些挑战。前后呼应，思路清晰，也能表现方案的优秀之处。 6 结果 1. 与问题相呼应？（解决了哪些问题）2. 定性、定量的描述3. 方法可复制、结果可复用 结果:这是PPT的最后一个关键点。向听众呈现自己的业绩和拿到的核心成果。层级越高结果越响亮。在结果上也注意三个点， 一是和前面提出的问题的呼应，证明这些结果是对应解决了这些问题后的成绩。 二是能够从定性、定量两方面的维度来描述结果。定性的比如描述自己的结果带来了一些什么局面的改变，填补了什么空缺，定量的通过数字显性化的说明拿到了什么成绩。 三是注意方法可复制、结果可复用。意思是说我们解决这些问题的方案/方法是可以复制到其他场景使用的，解决这些问题沉淀的东西比如工具、产品等是可以被复用的，不是只适用于自身的问题场景中的。这一点要求比较高，需要我们在设计方案的过程中进行抽象，考虑通用性。这是产品化的思维，也是格局的体现。 PPT内容注意！注意！注意！这部分内容摘抄自阿里大佬的公众号[1]，仅作学习、工作指导使用，侵删（email: tianmao818@qq.com,会及时处理）。 序号 要点 备注 1 多图少字 一图胜千字，人类对图表比文字的信息接受效率高处很多。 2 少出现magic word magic word是说那些对听众陌生的词、概念，比如某个自己业务特有的名称。这个听众看不懂，就会有疑虑和信息不确定性，一般可以把这种词汇省掉，或者不能省掉就换用一些通用的能理解的词，再或者不能不放上去那就要用一句简单的话去解释这个词。另外magic word是一些明显的亮眼词，比如最**，这些容易获得眼球，被挑战和追着问题背后的细节和逻辑。 3 神奇的数字3 •每页ppt颜色不超过3种•每页ppt动画不超过3个•重点页ppt讲解到但不超过3分钟 4 每一页都有目的与关键词 每一页ppt放上去都是有目的的，想通过这一页ppt向听众呈现什么信息，这些核心信息可以通过一两个关键词或一句话来表达清楚。对于没营养、没什么价值含量的PPT页不应该放上去。 5 内容focus与思想deliver 整个ppt的内容一定是有重点的，focus在哪个case、哪几个能力的维度，这些要想清楚，要突出重点、不能广而全。也要想明白整个ppt讲完是想传递给评委/听众什么核心的信息和思想，想让听众感受到什么。 PPT讲解注意！注意！注意！这部分内容摘抄自阿里大佬的公众号[1]，仅作学习、工作指导使用，侵删（email: tianmao818@qq.com,会及时处理）。 序号 要点 备注 1 站 姿：面向评委/听众，不要一直对着电脑或大屏幕，自己的电脑只有在要翻页时才去关注。手 势：不要抽兜里，不要背着，双手配合演讲做动作，五指正指向ppt的关键词内容。目 光：看向评委，跟评委眼神有互动和交流。不要目光一直放在别处，瞟向别处或一直盯着电脑，这给人不自信或内容不真实的感觉。语 速：不要过快，要平缓适中。要有一些过渡性的语言组织，比如“那这个问题我们是怎么做的呢”？“接下来我们进入到第二个部分”等等。语 气：有轻重和缓急，最好做到抑扬顿挫，在重点的地方突出表达，在非重点的地方轻描。 2 短 思 : 在评委/听众提完问题后，不要立即着急着回答，哪怕自己知道答案。要花一两秒的时间快速的思考和组织一下语言。给自己一点思考的缓存时间，一是让自己更好的理解评委的问题，二是更好的回答问题。语 速：回答问题和讲ppt一样，不要语速过快，像机关枪一样啪啪不停，中间要有停顿，有语气的轻重缓急。层 次:回答问题最好能有层次的回答，比如我是这样思考这个问题的，第一点是什么，第二点是什么，第三点……，最后再来一个总结，1、2、3分别是什么。这样让人觉得很有条理，非常清晰。抽 象：这是一个比较高的要求，是说回答问题上最后能有些关键词、字的抽象表达，不要啰嗦一堆，没有重点，最后能用若干个关键词来抽象你的答案，然后围绕这些关键词进行细节的阐述。 引用1.《做好和讲好PPT是能力》------微信公众号《马不停蹄的后花园》 ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[钱币收藏]]></title>
    <url>%2F2010%2F06%2F06%2F%E9%92%B1%E5%B8%81%E6%94%B6%E8%97%8F%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>那些年</category>
      </categories>
      <tags>
        <tag>钱币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[导航]]></title>
    <url>%2F2000%2F05%2F20%2F%E5%AF%BC%E8%88%AA%2F</url>
    <content type="text"><![CDATA[技术 官网 工具 GitLogs Overleaf 私人jupyter pdf转word Graphviz 学习 Stack Overflow 杜洋工作室 Leetcode Typing Club 生活 搜索引擎 谷歌搜索 多吉搜索 新闻 凤凰在人间 深度 摄影 滨田英明 深度 资源 80s电影网 我爱分享网 MSDN 游戏 小工具 tinypng 格式转换 数据结构在线演示 银行 招商银行 工商银行]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>tricks</tag>
      </tags>
  </entry>
</search>
